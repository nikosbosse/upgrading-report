
@article{gneitingProbabilisticForecastsCalibration2007,
	title = {Probabilistic forecasts, calibration and sharpness},
	volume = {69},
	issn = {1467-9868},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2007.00587.x},
	doi = {10.1111/j.1467-9868.2007.00587.x},
	abstract = {Summary. Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
	language = {en},
	number = {2},
	urldate = {2020-02-17},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
	year = {2007},
	keywords = {Cross-validation, Density forecast, Ensemble prediction system, Ex post evaluation, Forecast verification, Model diagnostics, Posterior predictive assessment, Predictive distribution, Prequential principle, Probability integral transform, Proper scoring rule},
	pages = {243--268},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/BUWD6CGT/Gneiting et al. - 2007 - Probabilistic forecasts, calibration and sharpness.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/EUCMSBKN/j.1467-9868.2007.00587.html:text/html}
}

@article{gneitingStrictlyProperScoring2007,
	title = {Strictly {Proper} {Scoring} {Rules}, {Prediction}, and {Estimation}},
	volume = {102},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	language = {en},
	number = {477},
	urldate = {2020-03-22},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	month = mar,
	year = {2007},
	pages = {359--378},
	file = {Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:/mnt/data/Google Drive/Zotero/storage/P599P5ZY/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:application/pdf}
}

@article{bracherEvaluatingEpidemicForecasts2020a,
	title = {Evaluating epidemic forecasts in an interval format},
	url = {http://arxiv.org/abs/2005.12881},
	abstract = {For practical reasons, many forecasts of case, hospitalization and death counts in the context of the current COVID-19 pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID19 Forecast Hub run by the UMass-Amherst Inﬂuenza Forecasting Center of Excellence. Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This note provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts. Speciﬁcally, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a simple decomposition into a measure of sharpness and penalties for over- and underprediction.},
	language = {en},
	urldate = {2020-06-06},
	journal = {arXiv:2005.12881 [q-bio, stat]},
	author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.12881},
	keywords = {Statistics - Applications, Quantitative Biology - Populations and Evolution},
	file = {Bracher et al. - 2020 - Evaluating epidemic forecasts in an interval forma.pdf:/mnt/data/Google Drive/Zotero/storage/CUL2NHPC/Bracher et al. - 2020 - Evaluating epidemic forecasts in an interval forma.pdf:application/pdf}
}

@article{funkShorttermForecastsInform2020,
	title = {Short-term forecasts to inform the response to the {Covid}-19 epidemic in the {UK}},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2020.11.11.20220962v1},
	doi = {10.1101/2020.11.11.20220962},
	abstract = {Background: Short-term forecasts of infectious disease can create situational awareness and inform planning for outbreak response. Here, we report on multi-model forecasts of Covid-19 in the UK that were generated at regular intervals starting at the end of March 2020, in order to monitor expected healthcare utilisation and population impacts in real time. Methods: We evaluated the performance of individual model forecasts generated between 24 March and 14 July 2020, using a variety of metrics including the weighted interval score as well as metrics that assess the calibration, sharpness, bias and absolute error of forecasts separately. We further combined the predictions from individual models to ensemble forecasts using a simple mean as well as a quantile regression average that aimed to maximise performance. We further compared model performance to a null model of no change. Results: In most cases, individual models performed better than the null model, and ensembles models were well calibrated and performed comparatively to the best individual models. The quantile regression average did not noticeably outperform the mean ensemble. Conclusions: Ensembles of multi-model forecasts can inform the policy response to the Covid-19 pandemic by assessing future resource needs and expected population impact of morbidity and mortality.},
	language = {en},
	urldate = {2020-11-28},
	journal = {medRxiv},
	author = {Funk, S. and Abbott, S. and Atkins, B. D. and Baguelin, M. and Baillie, J. K. and Birrell, P. and Blake, J. and Bosse, N. I. and Burton, J. and Carruthers, J. and Davies, N. G. and Angelis, D. De and Dyson, L. and Edmunds, W. J. and Eggo, R. M. and Ferguson, N. M. and Gaythorpe, K. and Gorsich, E. and Guyver-Fletcher, G. and Hellewell, J. and Hill, E. M. and Holmes, A. and House, T. A. and Jewell, C. and Jit, M. and Jombart, T. and Joshi, I. and Keeling, M. J. and Kendall, E. and Knock, E. S. and Kucharski, A. J. and Lythgoe, K. A. and Meakin, S. R. and Munday, J. D. and Openshaw, P. J. M. and Overton, C. E. and Pagani, F. and Pearson, J. and Perez-Guzman, P. N. and Pellis, L. and Scarabel, F. and Semple, M. G. and Sherratt, K. and Tang, M. and Tildesley, M. J. and Leeuwen, E. Van and Whittles, L. K.},
	month = nov,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory Press},
	pages = {2020.11.11.20220962},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/9RK57885/Funk et al. - 2020 - Short-term forecasts to inform the response to the.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/AKDY6PAQ/2020.11.11.20220962v1.full.html:text/html}
}
@Article{funkAssessingPerformanceRealtime2019,
  title = {Assessing the Performance of Real-Time Epidemic Forecasts: {{A}} Case Study of {{Ebola}} in the {{Western Area}} Region of {{Sierra Leone}}, 2014-15},
  shorttitle = {Assessing the Performance of Real-Time Epidemic Forecasts},
  author = {Sebastian Funk and Anton Camacho and Adam J. Kucharski and Rachel Lowe and Rosalind M. Eggo and W. John Edmunds},
  year = {2019},
  month = {feb},
  volume = {15},
  pages = {e1006785},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006785},
  abstract = {Real-time forecasts based on mathematical models can inform critical decision-making during infectious disease outbreaks. Yet, epidemic forecasts are rarely evaluated during or after the event, and there is little guidance on the best metrics for assessment. Here, we propose an evaluation approach that disentangles different components of forecasting ability using metrics that separately assess the calibration, sharpness and bias of forecasts. This makes it possible to assess not just how close a forecast was to reality but also how well uncertainty has been quantified. We used this approach to analyse the performance of weekly forecasts we generated in real time for Western Area, Sierra Leone, during the 2013\textendash 16 Ebola epidemic in West Africa. We investigated a range of forecast model variants based on the model fits generated at the time with a semi-mechanistic model, and found that good probabilistic calibration was achievable at short time horizons of one or two weeks ahead but model predictions were increasingly unreliable at longer forecasting horizons. This suggests that forecasts may have been of good enough quality to inform decision making based on predictions a few weeks ahead of time but not longer, reflecting the high level of uncertainty in the processes driving the trajectory of the epidemic. Comparing forecasts based on the semi-mechanistic model to simpler null models showed that the best semi-mechanistic model variant performed better than the null models with respect to probabilistic calibration, and that this would have been identified from the earliest stages of the outbreak. As forecasts become a routine part of the toolkit in public health, standards for evaluation of performance will be important for assessing quality and improving credibility of mathematical models, and for elucidating difficulties and trade-offs when aiming to make the most useful and reliable forecasts.},
  file = {/mnt/data/Google Drive/Zotero/storage/X6Z9PIFT/Funk et al. - 2019 - Assessing the performance of real-time epidemic fo.pdf;/mnt/data/Google Drive/Zotero/storage/JN28VVKF/article.html},
  journal = {PLOS Computational Biology},
  keywords = {Epidemiology,Forecasting,Infectious disease epidemiology,Infectious diseases,Mathematical models,Probability distribution,Public and occupational health,Sierra Leone},
  language = {en},
  number = {2},
}
@Article{winklerScoringRulesEvaluation1996,
  title = {Scoring Rules and the Evaluation of Probabilities},
  author = {R. L. Winkler and Javier Mu{\~n}oz and Jos{\a'e} L. Cervera and Jos{\a'e} M. Bernardo and Gail Blattenberger and Joseph B. Kadane and Dennis V. Lindley and Allan H. Murphy and Robert M. Oliver and David R\'ios-Insua},
  year = {1996},
  month = {jun},
  volume = {5},
  pages = {1--60},
  issn = {1863-8260},
  doi = {10.1007/BF02562681},
  abstract = {In Bayesian inference and decision analysis, inferences and predictions are inherently probabilistic in nature. Scoring rules, which involve the computation of a score based on probability forecasts and what actually occurs, can be used to evaluate probabilities and to provide appropriate incentives for ``good'' probabilities. This paper review scoring rules and some related measures for evaluating probabilities, including decompositions of scoring rules and attributes of ``goodness'' of probabilites, comparability of scores, and the design of scoring rules for specific inferential and decision-making problems},
  file = {/mnt/data/Google Drive/Zotero/storage/VHTQR6BK/Winkler et al. - 1996 - Scoring rules and the evaluation of probabilities.pdf},
  journal = {Test},
  language = {en},
  number = {1},
}
@Article{dawidPresentPositionPotential1984,
  title = {Present {{Position}} and {{Potential Developments}}: {{Some Personal Views Statistical Theory}} the {{Prequential Approach}}},
  shorttitle = {Present {{Position}} and {{Potential Developments}}},
  author = {A. P. Dawid},
  year = {1984},
  volume = {147},
  pages = {278--290},
  issn = {2397-2327},
  doi = {10.2307/2981683},
  abstract = {The prequential approach is founded on the premiss that the purpose of statistical inference is to make sequential probability forecasts for future observations, rather than to express information about parameters. Many traditional parametric concepts, such as consistency and efficiency, prove to have natural counterparts in this formulation, which sheds new light on these and suggests fruitful extensions.},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.2307/2981683},
  copyright = {\textcopyright{} 1984 Royal Statistical Society},
  file = {/mnt/data/Google Drive/Zotero/storage/PX9RNJBW/Dawid - 1984 - Present Position and Potential Developments Some .pdf;/mnt/data/Google Drive/Zotero/storage/UXRWFPAE/2981683.html},
  journal = {Journal of the Royal Statistical Society: Series A (General)},
  keywords = {consistency,efficiency,likelihood,prequential principle,probability forecasting},
  language = {en},
  number = {2},
}
