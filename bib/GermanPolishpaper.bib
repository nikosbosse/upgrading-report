
@book{10GroupedTime,
  title = {10.2 {{Grouped}} Time Series | {{Forecasting}}: {{Principles}} and {{Practice}}},
  shorttitle = {10.2 {{Grouped}} Time Series | {{Forecasting}}},
  url = {https://Otexts.com/fpp2/},
  urldate = {2020-03-31},
  abstract = {2nd edition},
  file = {/mnt/data/Google Drive/Zotero/storage/6QI93F5W/gts.html}
}

@online{2016GasparriniEpidem,
  title = {2016\_gasparrini\_{{Epidem}}},
  url = {http://www.ag-myresearch.com/2016_gasparrini_epidem.html},
  urldate = {2019-10-31},
  abstract = {Gasparrini A Epidemiology . 2016; 27 (6):835-842},
  file = {/mnt/data/Google Drive/Zotero/storage/GFY3ANQU/2016_gasparrini_Epidem.pdf;/mnt/data/Google Drive/Zotero/storage/YRVFWKQT/2016_gasparrini_epidem.html},
  langid = {english},
  organization = {{Antonio Gasparrini: my research}}
}

@article{2020.03.27.20043752,
  title = {Forecasting {{COVID}}-19 Impact on Hospital Bed-Days, {{ICU}}-Days, Ventilator-Days and Deaths by {{US}} State in the next 4 Months},
  author = {{Murray} and JL, Christopher},
  date = {2020},
  journaltitle = {medRxiv},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2020.03.27.20043752},
  url = {https://www.medrxiv.org/content/early/2020/03/30/2020.03.27.20043752},
  abstract = {Importance This study presents the first set of estimates of predicted health service utilization and deaths due to COVID-19 by day for the next 4 months for each state in the US.Objective To determine the extent and timing of deaths and excess demand for hospital services due to COVID-19 in the US.Design, Setting, and Participants This study used data on confirmed COVID-19 deaths by day from WHO websites and local and national governments; data on hospital capacity and utilization for US states; and observed COVID-19 utilization data from select locations to develop a statistical model forecasting deaths and hospital utilization against capacity by state for the US over the next 4 months.Exposure(s) COVID-19.Main outcome(s) and measure(s) Deaths, bed and ICU occupancy, and ventilator use.Results Compared to licensed capacity and average annual occupancy rates, excess demand from COVID-19 at the peak of the pandemic in the second week of April is predicted to be 64,175 (95\% UI 7,977 to 251,059) total beds and 17,380 (95\% UI 2,432 to 57,955) ICU beds. At the peak of the pandemic, ventilator use is predicted to be 19,481 (95\% UI 9,767 to 39,674). The date of peak excess demand by state varies from the second week of April through May. We estimate that there will be a total of 81,114 (95\% UI 38,242 to 162,106) deaths from COVID-19 over the next 4 months in the US. Deaths from COVID-19 are estimated to drop below 10 deaths per day between May 31 and June 6.Conclusions and Relevance In addition to a large number of deaths from COVID-19, the epidemic in the US will place a load well beyond the current capacity of hospitals to manage, especially for ICU care. These estimates can help inform the development and implementation of strategies to mitigate this gap, including reducing non-COVID-19 demand for services and temporarily increasing system capacity. These are urgently needed given that peak volumes are estimated to be only three weeks away. The estimated excess demand on hospital systems is predicated on the enactment of social distancing measures in all states that have not done so already within the next week and maintenance of these measures throughout the epidemic, emphasizing the importance of implementing, enforcing, and maintaining these measures to mitigate hospital system overload and prevent deaths.Data availability statement A full list of data citations are available by contacting the corresponding author.Funding Statement Bill \&amp; Melinda Gates Foundation and the State of WashingtonQuestion Assuming social distancing measures are maintained, what are the forecasted gaps in available health service resources and number of deaths from the COVID-19 pandemic for each state in the United States?Findings Using a statistical model, we predict excess demand will be 64,175 (95\% UI 7,977 to 251,059) total beds and 17,380 (95\% UI 2,432 to 57,955) ICU beds at the peak of COVID-19. Peak ventilator use is predicted to be 19,481 (95\% UI 9,767 to 39,674) ventilators. Peak demand will be in the second week of April. We estimate 81,114 (95\% UI 38,242 to 162,106) deaths in the United States from COVID-19 over the next 4 months.Meaning Even with social distancing measures enacted and sustained, the peak demand for hospital services due to the COVID-19 pandemic is likely going to exceed capacity substantially. Alongside the implementation and enforcement of social distancing measures, there is an urgent need to develop and implement plans to reduce non-COVID-19 demand for and temporarily increase capacity of health facilities.Competing Interest StatementThe authors have declared no competing interest.Clinical TrialN/AFunding StatementFunding was provided by the Bill \&amp; Melinda Gates Found and the State of Washington.Author DeclarationsAll relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript.YesAll necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesA full list of data citations are available by contacting the corresponding author.},
  elocation-id = {2020.03.27.20043752},
  eprint = {https://www.medrxiv.org/content/early/2020/03/30/2020.03.27.20043752.full.pdf}
}

@article{abbottEstimatingTimevaryingReproduction2020,
  title = {Estimating the Time-Varying Reproduction Number of {{SARS}}-{{CoV}}-2 Using National and Subnational Case Counts},
  author = {Abbott, Sam and Hellewell, Joel and Thompson, Robin N. and Sherratt, Katharine and Gibbs, Hamish P. and Bosse, Nikos I. and Munday, James D. and Meakin, Sophie and Doughty, Emma L. and Chun, June Young and Chan, Yung-Wai Desmond and Finger, Flavio and Campbell, Paul and Endo, Akira and Pearson, Carl A. B. and Gimma, Amy and Russell, Tim and {CMMID COVID modelling group} and Flasche, Stefan and Kucharski, Adam J. and Eggo, Rosalind M. and Funk, Sebastian},
  date = {2020-06-01},
  journaltitle = {Wellcome Open Research},
  shortjournal = {Wellcome Open Res},
  volume = {5},
  pages = {112},
  issn = {2398-502X},
  doi = {10.12688/wellcomeopenres.16006.1},
  url = {https://wellcomeopenresearch.org/articles/5-112/v1},
  urldate = {2020-08-12},
  abstract = {Background:               Interventions are now in place worldwide to reduce transmission of the novel coronavirus. Assessing temporal variations in transmission in different countries is essential for evaluating the effectiveness of public health interventions and the impact of changes in policy.                                         Methods:               We use case notification data to generate daily estimates of the time-dependent reproduction number in different regions and countries. Our modelling framework, based on open source tooling, accounts for reporting delays, so that temporal variations in reproduction number estimates can be compared directly with the times at which interventions are implemented.                                         Results:               We provide three example uses of our framework. First, we demonstrate how the toolset displays temporal changes in the reproduction number. Second, we show how the framework can be used to reconstruct case counts by date of infection from case counts by date of notification, as well as to estimate the reproduction number. Third, we show how maps can be generated to clearly show if case numbers are likely to decrease or increase in different regions. Results are shown for regions and countries worldwide on our website (               https://epiforecasts.io/covid/               ) and are updated daily. Our tooling is provided as an open-source R package to allow replication by others.                                         Conclusions:               This decision-support tool can be used to assess changes in virus transmission in different regions and countries worldwide. This allows policymakers to assess the effectiveness of current interventions, and will be useful for inferring whether or not transmission will increase when interventions are lifted. As well as providing daily updates on our website, we also provide adaptable computing code so that our approach can be used directly by researchers and policymakers on confidential datasets. We hope that our tool will be used to support decisions in countries worldwide throughout the ongoing COVID-19 pandemic.},
  file = {/mnt/data/Google Drive/Zotero/storage/VABAHAUK/v1.html},
  langid = {english}
}

@online{aerzteblattSARSCoV2DiagnostikRKIPasst2020,
  title = {SARS-CoV-2-Diagnostik: RKI passt Testempfehlungen an},
  shorttitle = {SARS-CoV-2-Diagnostik},
  author = {Ärzteblatt, Redaktion Deutsches, Deutscher Ärzteverlag GmbH},
  date = {2020-11-03},
  url = {https://www.aerzteblatt.de/nachrichten/118001/SARS-CoV-2-Diagnostik-RKI-passt-Testempfehlungen-an},
  urldate = {2021-05-30},
  abstract = {Berlin – Mittels einer Anpassung der Testkriterien für SARS-CoV-2-Infektionen an die Herbst- und Wintersaison will das Robert-Koch-Institut (RKI) eine... \#COVID-19},
  langid = {german},
  organization = {{Deutsches Ärzteblatt}}
}

@article{alarconN6methyladenosineM6AMarks2015,
  title = {N6-Methyl-Adenosine ({{m6A}}) Marks Primary {{microRNAs}} for Processing},
  author = {Alarcón, Claudio R. and Lee, Hyeseung and Goodarzi, Hani and Halberg, Nils and Tavazoie, Sohail F.},
  date = {2015-03-26},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {519},
  pages = {482--485},
  issn = {0028-0836},
  doi = {10.1038/nature14281},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4475635/},
  urldate = {2018-12-30},
  abstract = {The first step in the biogenesis of microRNAs is the processing of primary microRNAs (pri-miRNAs) by the microprocessor complex, composed of the RNA binding protein DGCR8 and the ribonuclease type III DROSHA–. This initial event requires the recognition of the junction between the stem and the flanking single-stranded RNA of the pri-miRNA hairpin by DGCR8 followed by recruitment of DROSHA, which cleaves the RNA duplex to yield the pre-miRNA product. While the mechanisms underlying pri-miRNA processing have been elucidated, the mechanism by which DGCR8 recognizes and binds pri-miRNAs as opposed to other secondary structures present in transcripts is not understood. We find that methyltransferase like 3 (METTL3) methylates pri-miRNAs, marking them for recognition and processing by DGCR8. Consistent with this, METTL3 depletion reduced the binding of DGCR8 to pri-miRNAs and resulted in the global reduction of mature miRNAs and concomitant accumulation of unprocessed pri-miRNAs. In vitro processing reactions confirmed the sufficiency of the m6A mark in promoting pri-miRNA processing. Finally, gain-of-function experiments revealed that METTL3 is sufficient to enhance miRNA maturation in a global and non-cell-type specific manner. Our findings reveal that the m6A mark acts as a key post-transcriptional modification that promotes the initiation of miRNA biogenesis.},
  eprint = {25799998},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/P5WI7ULS/Alarcón et al. - 2015 - N6-methyl-adenosine (m6A) marks primary microRNAs .pdf},
  number = {7544},
  pmcid = {PMC4475635}
}

@online{alexiosDoesAnythingNOT2013,
  title = {Does Anything {{NOT}} Beat the {{GARCH}}(1,1)?},
  author = {{alexios}},
  date = {2013-01-07T23:57:59+00:00},
  url = {http://www.unstarched.net/2013/01/07/does-anything-not-beat-the-garch11/},
  urldate = {2019-09-13},
  abstract = {In their paper on GARCH model comparison, Hansen and Lunde (2005) present evidence that among 330 different models, and using daily data on the DM/\$ rate and IBM stock returns, no model does signif…},
  file = {/mnt/data/Google Drive/Zotero/storage/JBRPH7Y6/does-anything-not-beat-the-garch11.html},
  langid = {american},
  organization = {{unstarched}}
}

@online{alTranscriptomewideMappingMethyladenosine,
  title = {Transcriptome-Wide Mapping of {{N}}(6)-Methyladenosine by m(6){{A}}-Seq Based on Immunocapturing and Massively Parallel Sequencing. - {{PubMed}} - {{NCBI}}},
  author = {{al}, et, Dominissini D.},
  abstract = {Nat Protoc. 2013 Jan;8(1):176-89. doi: 10.1038/nprot.2012.148. Epub 2013 Jan 3. Research Support, Non-U.S. Gov't},
  eprint = {23288318},
  eprinttype = {pubmed},
  langid = {english}
}

@article{alvarez-castroBayesianAnalysisHighdimensional,
  title = {Bayesian Analysis of High-Dimensional Count Data},
  author = {Alvarez-Castro, Ignacio},
  pages = {150},
  file = {/mnt/data/Google Drive/Zotero/storage/AG4T7EPT/Alvarez-Castro - Bayesian analysis of high-dimensional count data.pdf},
  langid = {english}
}

@article{aminikhanghahiSurveyMethodsTime2017,
  title = {A {{Survey}} of {{Methods}} for {{Time Series Change Point Detection}}},
  author = {Aminikhanghahi, Samaneh and Cook, Diane J.},
  date = {2017-05},
  journaltitle = {Knowledge and information systems},
  shortjournal = {Knowl Inf Syst},
  volume = {51},
  pages = {339--367},
  issn = {0219-1377},
  doi = {10.1007/s10115-016-0987-z},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5464762/},
  urldate = {2020-04-03},
  abstract = {Change points are abrupt variations in time series data. Such abrupt changes may represent transitions that occur between states. Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring, climate change detection, speech and image analysis, and human activity analysis. This survey article enumerates, categorizes, and compares many of the methods that have been proposed to detect change points in time series. The methods examined include both supervised and unsupervised algorithms that have been introduced and evaluated. We introduce several criteria to compare the algorithms. Finally, we present some grand challenges for the community to consider.},
  eprint = {28603327},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/LWWTKKN3/Aminikhanghahi and Cook - 2017 - A Survey of Methods for Time Series Change Point D.pdf},
  number = {2},
  pmcid = {PMC5464762}
}

@online{amodeiConcreteProblemsAI2016,
  title = {Concrete {{Problems}} in {{AI Safety}}},
  author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
  date = {2016-07-25},
  url = {http://arxiv.org/abs/1606.06565},
  urldate = {2019-12-06},
  abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (“avoiding side effects” and “avoiding reward hacking”), an objective function that is too expensive to evaluate frequently (“scalable supervision”), or undesirable behavior during the learning process (“safe exploration” and “distributional shift”). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
  archiveprefix = {arXiv},
  eprint = {1606.06565},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/2FP42T6I/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  langid = {english},
  primaryclass = {cs}
}

@article{andersCountbasedDifferentialExpression2013,
  title = {Count-Based Differential Expression Analysis of {{RNA}} Sequencing Data Using {{R}} and {{Bioconductor}}},
  author = {Anders, Simon and McCarthy, Davis J and Chen, Yunshun and Okoniewski, Michal and Smyth, Gordon K and Huber, Wolfgang and Robinson, Mark D},
  date = {2013-09},
  journaltitle = {Nature Protocols},
  volume = {8},
  pages = {1765--1786},
  issn = {1754-2189, 1750-2799},
  doi = {10.1038/nprot.2013.099},
  url = {http://www.nature.com/articles/nprot.2013.099},
  urldate = {2018-12-01},
  file = {/mnt/data/Google Drive/Zotero/storage/2RQPP28C/Anders et al. - 2013 - Count-based differential expression analysis of RN.pdf;/mnt/data/Google Drive/Zotero/storage/K3D4K5TK/Anders et al. - 2013 - Count-based differential expression analysis of RN.pdf;/mnt/data/Google Drive/Zotero/storage/AMN9KUQI/nprot.2013.html},
  langid = {english},
  number = {9}
}

@article{andersDifferentialExpressionAnalysis2010,
  title = {Differential Expression Analysis for Sequence Count Data},
  author = {Anders, Simon and Huber, Wolfgang},
  date = {2010-10-27},
  journaltitle = {Genome Biology},
  shortjournal = {Genome Biology},
  volume = {11},
  pages = {R106},
  issn = {1474-760X},
  doi = {10.1186/gb-2010-11-10-r106},
  url = {https://doi.org/10.1186/gb-2010-11-10-r106},
  urldate = {2018-12-02},
  abstract = {High-throughput sequencing assays such as RNA-Seq, ChIP-Seq or barcode counting provide quantitative readouts in the form of count data. To infer differential signal in such data correctly and with good statistical power, estimation of data variability throughout the dynamic range and a suitable error model are required. We propose a method based on the negative binomial distribution, with variance and mean linked by local regression and present an implementation, DESeq, as an R/Bioconductor package.},
  file = {/mnt/data/Google Drive/Zotero/storage/IY4SX6PH/Anders and Huber - 2010 - Differential expression analysis for sequence coun.pdf;/mnt/data/Google Drive/Zotero/storage/W8YK39DN/gb-2010-11-10-r106.html},
  number = {10}
}

@article{andersonAsymptoticTheoryCertain1952,
  title = {Asymptotic {{Theory}} of {{Certain}} "{{Goodness}} of {{Fit}}" {{Criteria Based}} on {{Stochastic Processes}}},
  author = {Anderson, T. W. and Darling, D. A.},
  date = {1952},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {23},
  pages = {193--212},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  abstract = {The statistical problem treated is that of testing the hypothesis that n independent, identically distributed random variables have a specified continuous distribution function F(x). If Fn(x) is the empirical cumulative distribution function and ψ(t) is some nonnegative weight function (0 ≤ t ≤ 1), we consider \$n\^\{\textbackslash frac\{1\}\{2\}\} \textbackslash sup\_\{-\textbackslash infty\vphantom\}},
  eprint = {2236446},
  eprinttype = {jstor},
  number = {2}
}

@article{angusProbabilityIntegralTransform1994,
  title = {The {{Probability Integral Transform}} and {{Related Results}}},
  author = {Angus, John E.},
  date = {1994-12-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {36},
  pages = {652--654},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/1036146},
  url = {https://epubs.siam.org/doi/abs/10.1137/1036146},
  urldate = {2020-08-12},
  abstract = {A simple proof of the probability integral transform theorem in probability and statistics is given that depends only on probabilistic concepts and elementary properties of continuous functions. This proof yields the theorem in its fullest generality. A similar theorem that forms the basis for the inverse method of random number generation is also discussed and contrasted to the probability integral transform theorem. Typical applications are discussed. Despite their generality and far reaching consequences, these theorems are remarkable in their simplicity and ease of proof.},
  file = {/mnt/data/Google Drive/Zotero/storage/8K3YQL5Q/1036146.html},
  number = {4}
}

@article{antanaviciuteM6aViewerSoftwareDetection2017,
  title = {{{m6aViewer}}: Software for the Detection, Analysis, and Visualization of {{N6}}-Methyladenosine Peaks from {{m6A}}-Seq/{{ME}}-{{RIP}} Sequencing Data},
  shorttitle = {{{m6aViewer}}},
  author = {Antanaviciute, Agne and Baquero-Perez, Belinda and Watson, Christopher M. and Harrison, Sally M. and Lascelles, Carolina and Crinnion, Laura and Markham, Alexander F. and Bonthron, David T. and Whitehouse, Adrian and Carr, Ian M.},
  date = {2017-10},
  journaltitle = {RNA (New York, N.Y.)},
  shortjournal = {RNA},
  volume = {23},
  pages = {1493--1501},
  issn = {1469-9001},
  doi = {10.1261/rna.058206.116},
  abstract = {Recent methods for transcriptome-wide N6-methyladenosine (m6A) profiling have facilitated investigations into the RNA methylome and established m6A as a dynamic modification that has critical regulatory roles in gene expression and may play a role in human disease. However, bioinformatics resources available for the analysis of m6A sequencing data are still limited. Here, we describe m6aViewer-a cross-platform application for analysis and visualization of m6A peaks from sequencing data. m6aViewer implements a novel m6A peak-calling algorithm that identifies high-confidence methylated residues with more precision than previously described approaches. The application enables data analysis through a graphical user interface, and thus, in contrast to other currently available tools, does not require the user to be skilled in computer programming. m6aViewer and test data can be downloaded here: http://dna2.leeds.ac.uk/m6a.},
  eprint = {28724534},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/XLZRMJQT/Antanaviciute et al. - 2017 - m6aViewer software for the detection, analysis, a.pdf},
  keywords = {Adenosine,Computational Biology,m6A,m6A-seq,next-generation sequencing,peak-calling,RNA methylation,Sequence Analysis; RNA,Software,User-Computer Interface},
  langid = {english},
  number = {10},
  pmcid = {PMC5602108}
}

@online{arikTabNetAttentiveInterpretable2020,
  title = {{{TabNet}}: {{Attentive Interpretable Tabular Learning}}},
  shorttitle = {{{TabNet}}},
  author = {Arik, Sercan O. and Pfister, Tomas},
  date = {2020-12-09},
  url = {http://arxiv.org/abs/1908.07442},
  urldate = {2021-04-06},
  abstract = {We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.},
  archiveprefix = {arXiv},
  eprint = {1908.07442},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/Z9J7NDUW/Arik and Pfister - 2020 - TabNet Attentive Interpretable Tabular Learning.pdf;/mnt/data/Google Drive/Zotero/storage/DKZLYDLU/1908.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@book{arnoldStateSpaceModels,
  title = {State {{Space Models}} in {{Stan}}},
  author = {Arnold, Jeffrey B.},
  url = {https://jrnold.github.io/ssmodels-in-stan/index.html},
  urldate = {2019-11-05},
  abstract = {Documentation for State Space Models in Stan.},
  file = {/mnt/data/Google Drive/Zotero/storage/5VIFYEF5/index.html}
}

@book{arnoldStateSpaceModelsa,
  title = {State {{Space Models}} in {{Stan}}},
  author = {Arnold, Jeffrey B.},
  url = {https://jrnold.github.io/ssmodels-in-stan/stan-functions.html#simulation-smoothers-1},
  urldate = {2020-01-13},
  abstract = {Documentation for State Space Models in Stan.},
  file = {/mnt/data/Google Drive/Zotero/storage/QMZCUAWY/stan-functions.html}
}

@article{asselBrierScoreDoes2017,
  title = {The {{Brier}} Score Does Not Evaluate the Clinical Utility of Diagnostic Tests or Prediction Models},
  author = {Assel, Melissa and Sjoberg, Daniel D. and Vickers, Andrew J.},
  date = {2017-12-02},
  journaltitle = {Diagnostic and Prognostic Research},
  shortjournal = {Diagnostic and Prognostic Research},
  volume = {1},
  pages = {19},
  issn = {2397-7523},
  doi = {10.1186/s41512-017-0020-3},
  url = {https://doi.org/10.1186/s41512-017-0020-3},
  urldate = {2020-02-25},
  abstract = {A variety of statistics have been proposed as tools to help investigators assess the value of diagnostic tests or prediction models. The Brier score has been recommended on the grounds that it is a proper scoring rule that is affected by both discrimination and calibration. However, the Brier score is prevalence dependent in such a way that the rank ordering of tests or models may inappropriately vary by prevalence.},
  file = {/mnt/data/Google Drive/Zotero/storage/M3IGKA6P/Assel et al. - 2017 - The Brier score does not evaluate the clinical uti.pdf;/mnt/data/Google Drive/Zotero/storage/LL74JDGX/s41512-017-0020-3.html},
  number = {1}
}

@article{atanasovDistillingWisdomCrowds2016,
  title = {Distilling the {{Wisdom}} of {{Crowds}}: {{Prediction Markets}} vs. {{Prediction Polls}}},
  shorttitle = {Distilling the {{Wisdom}} of {{Crowds}}},
  author = {Atanasov, Pavel and Rescober, Phillip and Stone, Eric and Swift, Samuel A. and Servan-Schreiber, Emile and Tetlock, Philip and Ungar, Lyle and Mellers, Barbara},
  date = {2016-04-22},
  journaltitle = {Management Science},
  shortjournal = {Management Science},
  volume = {63},
  pages = {691--706},
  publisher = {{INFORMS}},
  issn = {0025-1909},
  doi = {10.1287/mnsc.2015.2374},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2015.2374},
  urldate = {2021-05-30},
  abstract = {We report the results of the first large-scale, long-term, experimental test between two crowdsourcing methods: prediction markets and prediction polls. More than 2,400 participants made forecasts on 261 events over two seasons of a geopolitical prediction tournament. Forecasters were randomly assigned to either prediction markets (continuous double auction markets) in which they were ranked based on earnings, or prediction polls in which they submitted probability judgments, independently or in teams, and were ranked based on Brier scores. In both seasons of the tournament, prices from the prediction market were more accurate than the simple mean of forecasts from prediction polls. However, team prediction polls outperformed prediction markets when forecasts were statistically aggregated using temporal decay, differential weighting based on past performance, and recalibration. The biggest advantage of prediction polls was at the beginning of long-duration questions. Results suggest that prediction polls with proper scoring feedback, collaboration features, and statistical aggregation are an attractive alternative to prediction markets for distilling the wisdom of crowds.This paper was accepted by Uri Gneezy, behavioral economics.},
  file = {/mnt/data/Google Drive/Zotero/storage/I4SLHN59/Atanasov et al. - 2016 - Distilling the Wisdom of Crowds Prediction Market.pdf;/mnt/data/Google Drive/Zotero/storage/KGDWE6LP/mnsc.2015.html},
  number = {3}
}

@article{bacherSCnormRobustNormalization2017,
  title = {{{SCnorm}}: Robust Normalization of Single-Cell {{RNA}}-Seq Data},
  shorttitle = {{{SCnorm}}},
  author = {Bacher, Rhonda and Chu, Li-Fang and Leng, Ning and Gasch, Audrey P. and Thomson, James A. and Stewart, Ron M. and Newton, Michael and Kendziorski, Christina},
  date = {2017-06},
  journaltitle = {Nature Methods},
  volume = {14},
  pages = {584--586},
  issn = {1548-7105},
  doi = {10.1038/nmeth.4263},
  url = {https://www.nature.com/articles/nmeth.4263},
  urldate = {2019-05-05},
  abstract = {The normalization of RNA-seq data is essential for accurate downstream inference, but the assumptions upon which most normalization methods are based are not applicable in the single-cell setting. Consequently, applying existing normalization methods to single-cell RNA-seq data introduces artifacts that bias downstream analyses. To address this, we introduce SCnorm for accurate and efficient normalization of single-cell RNA-seq data.},
  file = {/mnt/data/Google Drive/Zotero/storage/HBMJDWG2/Bacher et al. - 2017 - SCnorm robust normalization of single-cell RNA-se.pdf;/mnt/data/Google Drive/Zotero/storage/AJBKL4QM/nmeth.html},
  langid = {english},
  number = {6}
}

@online{badkundriForecasting20172018Yemen2019,
  title = {Forecasting the 2017-2018 {{Yemen Cholera Outbreak}} with {{Machine Learning}}},
  author = {Badkundri, Rohil and Valbuena, Victor and Pinnamareddy, Srikusmanjali and Cantrell, Brittney and Standeven, Janet},
  date = {2019-02-16},
  url = {http://arxiv.org/abs/1902.06739},
  urldate = {2019-10-29},
  abstract = {The ongoing Yemen cholera outbreak has been deemed one of the worst cholera outbreaks in history, with over a million people impacted and thousands dead. Triggered by a civil war, the outbreak has been shaped by various political, environmental, and epidemiological factors and continues to worsen. While cholera has several effective treatments, the untimely and inefficient distribution of existing medicines has been the primary cause of cholera mortality. With the hope of facilitating resource allocation, various mathematical models have been created to track the Yemeni outbreak and identify at-risk administrative divisions, called governorates. Existing models are not powerful enough to accurately and consistently forecast cholera cases per governorate over multiple timeframes. To address the need for a complex, reliable model, we offer the Cholera Artificial Learning Model (CALM); a system of 4 extreme-gradient-boosting (XGBoost) machine learning models that forecast the number of new cholera cases a Yemeni governorate will experience from a time range of 2 weeks to 2 months. CALM provides a novel machine learning approach that makes use of rainfall data, past cholera cases and deaths data, civil war fatalities, and inter-governorate interactions represented across multiple time frames. Additionally, the use of machine learning, along with extensive feature engineering, allows CALM to easily learn complex non-linear relations apparent in an epidemiological phenomenon. CALM is able to forecast cholera incidence 2 weeks to 2 months in advance within a margin of just 5 cholera cases per 10,000 people in real-world simulation.},
  archiveprefix = {arXiv},
  eprint = {1902.06739},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/CIJMPID3/Badkundri et al. - 2019 - Forecasting the 2017-2018 Yemen Cholera Outbreak w.pdf;/mnt/data/Google Drive/Zotero/storage/I8V88CEC/1902.html},
  keywords = {68T01,Computer Science - Computers and Society,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  primaryclass = {cs, q-bio}
}

@article{baileyPracticalGuidelinesComprehensive2013,
  title = {Practical {{Guidelines}} for the {{Comprehensive Analysis}} of {{ChIP}}-Seq {{Data}}},
  author = {Bailey, Timothy and Krajewski, Pawel and Ladunga, Istvan and Lefebvre, Celine and Li, Qunhua and Liu, Tao and Madrigal, Pedro and Taslim, Cenny and Zhang, Jie},
  date = {2013-11-14},
  journaltitle = {PLoS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {9},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1003326},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3828144/},
  urldate = {2018-12-03},
  abstract = {Mapping the chromosomal locations of transcription factors, nucleosomes, histone modifications, chromatin remodeling enzymes, chaperones, and polymerases is one of the key tasks of modern biology, as evidenced by the Encyclopedia of DNA Elements (ENCODE) Project. To this end, chromatin immunoprecipitation followed by high-throughput sequencing (ChIP-seq) is the standard methodology. Mapping such protein-DNA interactions in vivo using ChIP-seq presents multiple challenges not only in sample preparation and sequencing but also for computational analysis. Here, we present step-by-step guidelines for the computational analysis of ChIP-seq data. We address all the major steps in the analysis of ChIP-seq data: sequencing depth selection, quality checking, mapping, data normalization, assessment of reproducibility, peak calling, differential binding analysis, controlling the false discovery rate, peak annotation, visualization, and motif analysis. At each step in our guidelines we discuss some of the software tools most frequently used. We also highlight the challenges and problems associated with each step in ChIP-seq data analysis. We present a concise workflow for the analysis of ChIP-seq data in Figure 1 that complements and expands on the recommendations of the ENCODE and modENCODE projects. Each step in the workflow is described in detail in the following sections.},
  eprint = {24244136},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/UJATMVH8/Bailey et al. - 2013 - Practical Guidelines for the Comprehensive Analysi.pdf},
  number = {11},
  pmcid = {PMC3828144}
}

@article{balversMomentumMeanReversion2006,
  title = {Momentum and Mean Reversion across National Equity Markets},
  author = {Balvers, Ronald J. and Wu, Yangru},
  date = {2006-01-01},
  journaltitle = {Journal of Empirical Finance},
  shortjournal = {Journal of Empirical Finance},
  volume = {13},
  pages = {24--48},
  issn = {0927-5398},
  doi = {10.1016/j.jempfin.2005.05.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0927539805000708},
  urldate = {2019-08-26},
  abstract = {Numerous studies have separately identified mean reversion and momentum. This paper considers these effects jointly. Our empirical model assumes that only global equity price index shocks can have permanent components. This is motivated in a production-based asset pricing context, given that production levels converge across developed countries. Combination momentum-contrarian strategies, used to select from among 18 developed equity markets at a monthly frequency, outperform both pure momentum and pure contrarian strategies. The results continue to hold after corrections for factor sensitivities and transaction costs. They reveal the importance of controlling for mean reversion in exploiting momentum and vice versa.},
  file = {/mnt/data/Google Drive/Zotero/storage/LH3XYYQK/Balvers and Wu - 2006 - Momentum and mean reversion across national equity.pdf;/mnt/data/Google Drive/Zotero/storage/VVDQF252/S0927539805000708.html},
  keywords = {International asset pricing,Investment strategies,Mean Reversion,Momentum},
  number = {1}
}

@article{batistaRNAModificationN6methyladenosine2017,
  title = {The {{RNA Modification N6}}-Methyladenosine and {{Its Implications}} in {{Human Disease}}},
  author = {Batista, Pedro J.},
  date = {2017-06-01},
  journaltitle = {Genomics, Proteomics \& Bioinformatics},
  shortjournal = {Genomics, Proteomics \& Bioinformatics},
  volume = {15},
  pages = {154--163},
  issn = {1672-0229},
  doi = {10.1016/j.gpb.2017.03.002},
  url = {http://www.sciencedirect.com/science/article/pii/S1672022917300773},
  urldate = {2018-09-09},
  abstract = {Impaired gene regulation lies at the heart of many disorders, including developmental diseases and cancer. Furthermore, the molecular pathways that control gene expression are often the target of cellular parasites, such as viruses. Gene expression is controlled through multiple mechanisms that are coordinated to ensure the proper and timely expression of each gene. Many of these mechanisms target the life cycle of the RNA molecule, from transcription to translation. Recently, another layer of regulation at the RNA level involving RNA modifications has gained renewed interest of the scientific community. The discovery that N6-methyladenosine (m6A), a modification present in mRNAs and long noncoding RNAs, can be removed by the activity of RNA demethylases, launched the field of epitranscriptomics; the study of how RNA function is regulated through the addition or removal of post-transcriptional modifications, similar to strategies used to regulate gene expression at the DNA and protein level. The abundance of RNA post-transcriptional modifications is determined by the activity of writer complexes (methylase) and eraser (RNA demethylase) proteins. Subsequently, the effects of RNA modifications materialize as changes in RNA structure and/or modulation of interactions between the modified RNA and RNA binding proteins or regulatory RNAs. Disruption of these pathways impairs gene expression and cellular function. This review focuses on the links between the RNA modification m6A and its implications in human diseases.},
  file = {/mnt/data/Google Drive/Zotero/storage/UBCYT2I4/Batista - 2017 - The RNA Modification N 6 -methyladenosine and Its .pdf;/mnt/data/Google Drive/Zotero/storage/PEDHAD2Q/S1672022917300773.html},
  keywords = {-methyladenosine,Cancer,Epitranscriptomics,Metabolic disease,Viral replication},
  number = {3},
  series = {{{RNA Epigenetics}} ({{I}})}
}

@online{bellemareCramerDistanceSolution2017,
  title = {The {{Cramer Distance}} as a {{Solution}} to {{Biased Wasserstein Gradients}}},
  author = {Bellemare, Marc G. and Danihelka, Ivo and Dabney, Will and Mohamed, Shakir and Lakshminarayanan, Balaji and Hoyer, Stephan and Munos, Rémi},
  date = {2017-05-30},
  url = {http://arxiv.org/abs/1705.10743},
  urldate = {2021-03-31},
  abstract = {The Wasserstein probability metric has received much attention from the machine learning community. Unlike the Kullback-Leibler divergence, which strictly measures change in probability, the Wasserstein metric reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated, among others, in ordinal regression and generative modelling. In this paper we describe three natural properties of probability divergences that reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein metric possesses the first two properties but, unlike the Kullback-Leibler divergence, does not possess the third. We provide empirical evidence suggesting that this is a serious issue in practice. Leveraging insights from probabilistic forecasting we propose an alternative to the Wasserstein metric, the Cram\textbackslash 'er distance. We show that the Cram\textbackslash 'er distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. To illustrate the relevance of the Cram\textbackslash 'er distance in practice we design a new algorithm, the Cram\textbackslash 'er Generative Adversarial Network (GAN), and show that it performs significantly better than the related Wasserstein GAN.},
  archiveprefix = {arXiv},
  eprint = {1705.10743},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/NUK9GSCY/Bellemare et al. - 2017 - The Cramer Distance as a Solution to Biased Wasser.pdf;/mnt/data/Google Drive/Zotero/storage/6RWSZS7X/1705.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{berulavaN6adenosineMethylationMiRNAs2015,
  title = {N6-Adenosine Methylation in {{MiRNAs}}},
  author = {Berulava, Tea and Rahmann, Sven and Rademacher, Katrin and Klein-Hitpass, Ludgar and Horsthemke, Bernhard},
  date = {2015},
  journaltitle = {PloS One},
  shortjournal = {PLoS ONE},
  volume = {10},
  pages = {e0118438},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0118438},
  abstract = {Methylation of N6-adenosine (m6A) has been observed in many different classes of RNA, but its prevalence in microRNAs (miRNAs) has not yet been studied. Here we show that a knockdown of the m6A demethylase FTO affects the steady-state levels of several miRNAs. Moreover, RNA immunoprecipitation with an anti-m6A-antibody followed by RNA-seq revealed that a significant fraction of miRNAs contains m6A. By motif searches we have discovered consensus sequences discriminating between methylated and unmethylated miRNAs. The epigenetic modification of an epigenetic modifier as described here adds a new layer to the complexity of the posttranscriptional regulation of gene expression.},
  eprint = {25723394},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/HZBWDIG9/Berulava et al. - 2015 - N6-adenosine methylation in MiRNAs.pdf},
  keywords = {Adenosine,Alpha-Ketoglutarate-Dependent Dioxygenase FTO,Epigenesis; Genetic,Gene Knockdown Techniques,HEK293 Cells,Humans,Methylation,MicroRNAs,Proteins,RNA Stability,RNA; Messenger},
  langid = {english},
  number = {2},
  pmcid = {PMC4344304}
}

@online{betancourtConceptualIntroductionHamiltonian2017,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  date = {2017-01-09},
  url = {http://arxiv.org/abs/1701.02434},
  urldate = {2019-07-01},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archiveprefix = {arXiv},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/UMKJ7NJ7/Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Car.pdf;/mnt/data/Google Drive/Zotero/storage/UJKZW85Y/1701.html},
  keywords = {Statistics - Methodology},
  primaryclass = {stat}
}

@online{betancourtDiagnosingSuboptimalCotangent2016,
  title = {Diagnosing {{Suboptimal Cotangent Disintegrations}} in {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  date = {2016-04-03},
  url = {http://arxiv.org/abs/1604.00695},
  urldate = {2020-01-28},
  abstract = {When properly tuned, Hamiltonian Monte Carlo scales to some of the most challenging high-dimensional problems at the frontiers of applied statistics, but when that tuning is suboptimal the performance leaves much to be desired. In this paper I show how suboptimal choices of one critical degree of freedom, the cotangent disintegration, manifest in readily observed diagnostics that facilitate the robust application of the algorithm.},
  archiveprefix = {arXiv},
  eprint = {1604.00695},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/QC7GG6QV/Betancourt - 2016 - Diagnosing Suboptimal Cotangent Disintegrations in.pdf;/mnt/data/Google Drive/Zotero/storage/IHVMNDXE/1604.html},
  keywords = {Statistics - Methodology},
  primaryclass = {stat}
}

@article{biNPEBseqNonparametricEmpirical2013,
  title = {{{NPEBseq}}: Nonparametric Empirical Bayesian-Based Procedure for Differential Expression Analysis of {{RNA}}-Seq Data},
  shorttitle = {{{NPEBseq}}},
  author = {Bi, Yingtao and Davuluri, Ramana V.},
  date = {2013-08-27},
  journaltitle = {BMC bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {14},
  pages = {262},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-14-262},
  abstract = {BACKGROUND: RNA-seq, a massive parallel-sequencing-based transcriptome profiling method, provides digital data in the form of aligned sequence read counts. The comparative analyses of the data require appropriate statistical methods to estimate the differential expression of transcript variants across different cell/tissue types and disease conditions. RESULTS: We developed a novel nonparametric empirical Bayesian-based approach (NPEBseq) to model the RNA-seq data. The prior distribution of the Bayesian model is empirically estimated from the data without any parametric assumption, and hence the method is "nonparametric" in nature. Based on this model, we proposed a method for detecting differentially expressed genes across different conditions. We also extended this method to detect differential usage of exons from RNA-seq data. The evaluation of NPEBseq on both simulated and publicly available RNA-seq datasets and comparison with three popular methods showed improved results for experiments with or without biological replicates. CONCLUSIONS: NPEBseq can successfully detect differential expression between different conditions not only at gene level but also at exon level from RNA-seq datasets. In addition, NPEBSeq performs significantly better than current methods and can be applied to genome-wide RNA-seq datasets. Sample datasets and R package are available at http://bioinformatics.wistar.upenn.edu/NPEBseq.},
  eprint = {23981227},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/U9YSE5HP/Bi and Davuluri - 2013 - NPEBseq nonparametric empirical bayesian-based pr.pdf},
  keywords = {Bayes Theorem,Computational Biology,Gene Expression Profiling,High-Throughput Nucleotide Sequencing,RNA,Sequence Alignment,Sequence Analysis; RNA,Software,Statistics; Nonparametric},
  langid = {english},
  pmcid = {PMC3765716}
}

@article{bollerslevGeneralizedAutoregressiveConditional1986,
  title = {Generalized Autoregressive Conditional Heteroskedasticity},
  author = {Bollerslev, Tim},
  date = {1986-04-01},
  journaltitle = {Journal of Econometrics},
  shortjournal = {Journal of Econometrics},
  volume = {31},
  pages = {307--327},
  issn = {0304-4076},
  doi = {10.1016/0304-4076(86)90063-1},
  url = {http://www.sciencedirect.com/science/article/pii/0304407686900631},
  urldate = {2019-09-12},
  abstract = {A natural generalization of the ARCH (Autoregressive Conditional Heteroskedastic) process introduced in Engle (1982) to allow for past conditional variances in the current conditional variance equation is proposed. Stationarity conditions and autocorrelation structure for this new class of parametric models are derived. Maximum likelihood estimation and testing are also considered. Finally an empirical example relating to the uncertainty of the inflation rate is presented.},
  file = {/mnt/data/Google Drive/Zotero/storage/PKUCUB9C/Bollerslev - 1986 - Generalized autoregressive conditional heteroskeda.pdf;/mnt/data/Google Drive/Zotero/storage/HQAPZT2C/0304407686900631.html},
  number = {3}
}

@article{bondtDoesStockMarket1985,
  title = {Does the {{Stock Market Overreact}}?},
  author = {Bondt, WERNER F. M. De and Thaler, Richard},
  date = {1985},
  journaltitle = {The Journal of Finance},
  volume = {40},
  pages = {793--805},
  issn = {1540-6261},
  doi = {10.1111/j.1540-6261.1985.tb05004.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1985.tb05004.x},
  urldate = {2019-08-26},
  abstract = {Research in experimental psychology suggests that, in violation of Bayes' rule, most people tend to “overreact” to unexpected and dramatic news events. This study of market efficiency investigates whether such behavior affects stock prices. The empirical evidence, based on CRSP monthly return data, is consistent with the overreaction hypothesis. Substantial weak form market inefficiencies are discovered. The results also shed new light on the January returns earned by prior “winners” and “losers.” Portfolios of losers experience exceptionally large January returns as late as five years after portfolio formation.},
  file = {/mnt/data/Google Drive/Zotero/storage/P4WU9PUE/Bondt and Thaler - 1985 - Does the Stock Market Overreact.pdf;/mnt/data/Google Drive/Zotero/storage/37TBKMKU/j.1540-6261.1985.tb05004.html},
  langid = {english},
  number = {3}
}

@online{borgwardtBayesianTwosampleTests2009,
  title = {Bayesian Two-Sample Tests},
  author = {Borgwardt, Karsten M. and Ghahramani, Zoubin},
  date = {2009-06-22},
  url = {http://arxiv.org/abs/0906.4032},
  urldate = {2019-04-15},
  abstract = {In this paper, we present two classes of Bayesian approaches to the twosample problem. Our first class of methods extends the Bayesian t-test to include all parametric models in the exponential family and their conjugate priors. Our second class of methods uses Dirichlet process mixtures (DPM) of such conjugate-exponential distributions as flexible nonparametric priors over the unknown distributions.},
  archiveprefix = {arXiv},
  eprint = {0906.4032},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/ZSQ62H2A/Borgwardt and Ghahramani - 2009 - Bayesian two-sample tests.pdf},
  keywords = {Computer Science - Machine Learning},
  langid = {english},
  primaryclass = {cs}
}

@online{bowmanUncertaintyQuantificationEpidemiological2020,
  title = {Uncertainty Quantification for Epidemiological Forecasts of {{COVID}}-19 through Combinations of Model Predictions},
  author = {Bowman, V. E. and Silk, D. S. and Dalrymple, U. and Woods, D. C.},
  date = {2020-06-29},
  url = {http://arxiv.org/abs/2006.10714},
  urldate = {2020-09-15},
  abstract = {A common statistical problem is prediction, or forecasting, in the presence of an ensemble of multiple candidate models. For example, multiple candidate models may be available to predict case numbers in a disease epidemic, resulting from different modelling approaches (e.g. mechanistic or empirical) or differing assumptions about spatial or age mixing. Alternative models capture genuine uncertainty in scientific understanding of disease dynamics, and/or different simplifying assumptions underpinning each model derivation. While the analysis of multi-model ensembles can be computationally challenging, accounting for this 'structural uncertainty' can improve forecast accuracy and reduce the risk of over-estimated confidence. In this paper we look at combining epidemiological forecasts for COVID-19 daily deaths, hospital admissions, and hospital and ICU occupancy, in order to improve the predictive accuracy of the short term forecasts. We combining models via combinations of individual predictive densities with weights chosen via application of predictive scoring, as commonly applied in meteorological and economic forecasting.},
  archiveprefix = {arXiv},
  eprint = {2006.10714},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/BQP6JRXC/Bowman et al. - 2020 - Uncertainty quantification for epidemiological for.pdf},
  keywords = {62P10,Statistics - Applications},
  langid = {english},
  primaryclass = {stat}
}

@article{bracherComparisonCombinationRealtime2020,
  title = {Comparison and Combination of Real-Time {{COVID19}} Forecasts in {{Germany}} and {{Poland}}},
  author = {Bracher, Johannes},
  date = {2020-10-08},
  publisher = {{OSF}},
  url = {https://osf.io/k8d39},
  urldate = {2021-05-29},
  abstract = {Short-term forecasts of cases, deaths and hospitalizations can improve situational awareness and provide an additional element to inform public health decision making during the COVID19 pandemic. While early in the pandemic only few prediction models were available, there is now a growing number of forecasts based on diverse methods and data streams. This project consists in a systematic comparison of such forecasts made for Germany and Poland. The main goals are the following: - create a database of standardized and comparable short-term forecasts. - assess the degree of agreement or disagreement between different forecasts. - systematically and quantitatively evaluate forecasts in order to identify particularly reliable prediction methods. - assess whether combinations of different forecasts (ensemble forecasts) can lead to improved predictive performance. This pre-registration serves to ensure a transparent set of rules and criteria to guide this collaborative study. Many technical and methodological aspects of this project follow the US COVID19 Forecast Hub (https://covid19forecasthub.org/). We aim for compatibility with work from the US in terms of forecast targets, evaluation and various technical aspects.},
  file = {/mnt/data/Google Drive/Zotero/storage/2TL2XVR6/k8d39.html},
  langid = {american}
}

@article{bracherEvaluatingEpidemicForecasts2021,
  title = {Evaluating Epidemic Forecasts in an Interval Format},
  author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
  date = {2021-02},
  journaltitle = {PLoS computational biology},
  shortjournal = {PLoS Comput Biol},
  volume = {17},
  pages = {e1008618},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008618},
  abstract = {For practical reasons, many forecasts of case, hospitalization, and death counts in the context of the current Coronavirus Disease 2019 (COVID-19) pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction.},
  eprint = {33577550},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/EX37R6J8/Bracher et al. - 2021 - Evaluating epidemic forecasts in an interval forma.pdf},
  keywords = {Communicable Diseases,COVID-19,Forecasting,Humans,Pandemics,Probability,SARS-CoV-2},
  langid = {english},
  number = {2},
  pmcid = {PMC7880475}
}

@article{bracherEvaluationIncidentDeath,
  title = {Evaluation of Incident Death Forecasts Based on Pairwise Model Com- Parisons},
  author = {Bracher, Johannes},
  pages = {7},
  file = {/mnt/data/Google Drive/Zotero/storage/8KME4KK3/pairwise_wis-2.pdf;/mnt/data/Google Drive/Zotero/storage/EDVTA2K9/cleaned-scores.csv;/mnt/data/Google Drive/Zotero/storage/EFIHPLBE/Bracher - Evaluation of incident death forecasts based on pa - old.pdf;/mnt/data/Google Drive/Zotero/storage/GP7DBYGD/20201013-inc-scores.csv;/mnt/data/Google Drive/Zotero/storage/MZ5RU92T/pariwise-comparisons-Johannes.Rnw;/mnt/data/Google Drive/Zotero/storage/QETA4SZX/pairwise_wis-1.pdf},
  langid = {english}
}

@article{bracherShorttermForecastingCOVID192021,
  title = {Short-Term Forecasting of {{COVID}}-19 in {{Germany}} and {{Poland}} during the Second Wave – a Preregistered Study},
  author = {Bracher, Johannes and Wolffram, Daniel and Deuschel, J. and Görgen, K. and Ketterer, J. L. and Ullrich, A. and Abbott, S. and Barbarossa, M. V. and Bertsimas, D. and Bhatia, S. and Bodych, M. and Bosse, N. I. and Burgard, J. P. and Castro, L. and Fairchild, G. and Fuhrmann, J. and Funk, S. and Gogolewski, K. and Gu, Q. and Heyder, S. and Hotz, T. and Kheifetz, Y. and Kirsten, H. and Krueger, T. and Krymova, E. and Li, M. L. and Meinke, J. H. and Michaud, I. J. and Niedzielewski, K. and Ożański, T. and Rakowski, F. and Scholz, M. and Soni, S. and Srivastava, A. and Zieliński, J. and Zou, D. and Gneiting, T. and Schienle, M.},
  date = {2021-01-11},
  journaltitle = {medRxiv},
  pages = {2020.12.24.20248826},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2020.12.24.20248826},
  url = {https://www.medrxiv.org/content/10.1101/2020.12.24.20248826v2},
  urldate = {2021-04-01},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}We report insights from ten weeks of collaborative COVID-19 forecasting for Germany and Poland (12 October – 19 December 2020). The study period covers the onset of the second wave in both countries, with tightening non-pharmaceutical interventions (NPIs) and subsequently a decay (Poland) or plateau and renewed increase (Germany) in reported cases. Thirteen independent teams provided probabilistic real-time forecasts of COVID-19 cases and deaths. These were reported for lead times of one to four weeks, with evaluation focused on one- and two-week horizons, which are less affected by changing NPIs. Heterogeneity between forecasts was considerable both in terms of point predictions and forecast spread. Ensemble forecasts showed good relative performance, in particular in terms of coverage, but did not clearly dominate single-model predictions. The study was preregistered and will be followed up in future phases of the pandemic.{$<$}/p{$>$}},
  file = {/mnt/data/Google Drive/Zotero/storage/BHPBLCD9/Bracher et al. - 2021 - Short-term forecasting of COVID-19 in Germany and .pdf;/mnt/data/Google Drive/Zotero/storage/I3ULULUZ/2020.12.24.20248826v2.html},
  langid = {english}
}

@article{bracherToyExampleBehaviour,
  title = {Toy Example on the Behaviour of Log({{WIS}} + 1)},
  author = {Bracher, Johannes},
  pages = {3},
  file = {/mnt/data/Google Drive/Zotero/storage/CPGYKYSE/Bracher - Toy example on the behaviour of log(WIS + 1).pdf},
  langid = {english}
}

@incollection{brauerCompartmentalModelsEpidemiology2008,
  title = {Compartmental {{Models}} in {{Epidemiology}}},
  booktitle = {Mathematical {{Epidemiology}}},
  author = {Brauer, Fred},
  editor = {Brauer, Fred and van den Driessche, Pauline and Wu, Jianhong},
  date = {2008},
  pages = {19--79},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-78911-6_2},
  url = {https://doi.org/10.1007/978-3-540-78911-6_2},
  urldate = {2020-08-12},
  abstract = {We describe and analyze compartmental models for disease transmission. We begin with models for epidemics, showing how to calculate the basic reproduction number and the final size of the epidemic. We also study models with multiple compartments, including treatment or isolation of infectives. We then consider models including births and deaths in which there may be an endemic equilibrium and study the asymptotic stability of equilibria. We conclude by studying age of infection models which give a unifying framework for more complicated compartmental models.},
  file = {/mnt/data/Google Drive/Zotero/storage/28HGWCZ9/Brauer - 2008 - Compartmental Models in Epidemiology.pdf},
  isbn = {978-3-540-78911-6},
  keywords = {Compartmental Model,Contact Rate,Endemic Equilibrium,Epidemic Model,Reproduction Number},
  langid = {english},
  options = {useprefix=true},
  series = {Lecture {{Notes}} in {{Mathematics}}}
}

@article{brehmerProperizationConstructingProper2020,
  title = {Properization: Constructing Proper Scoring Rules via {{Bayes}} Acts},
  shorttitle = {Properization},
  author = {Brehmer, Jonas R. and Gneiting, Tilmann},
  date = {2020-06-01},
  journaltitle = {Annals of the Institute of Statistical Mathematics},
  shortjournal = {Ann Inst Stat Math},
  volume = {72},
  pages = {659--673},
  issn = {1572-9052},
  doi = {10.1007/s10463-019-00705-7},
  url = {https://doi.org/10.1007/s10463-019-00705-7},
  urldate = {2021-03-26},
  abstract = {Scoring rules serve to quantify predictive performance. A scoring rule is proper if truth telling is an optimal strategy in expectation. Subject to customary regularity conditions, every scoring rule can be made proper, by applying a special case of the Bayes act construction studied by Grünwald and Dawid (Ann Stat 32:1367–1433, 2004) and Dawid (Ann Inst Stat Math 59:77–93, 2007), to which we refer as properization. We discuss examples from the recent literature and apply the construction to create new types, and reinterpret existing forms, of proper scoring rules and consistent scoring functions. In an abstract setting, we formulate sufficient conditions under which Bayes acts exist and scoring rules can be made proper.},
  file = {/mnt/data/Google Drive/Zotero/storage/5RSYDJY5/Brehmer and Gneiting - 2020 - Properization constructing proper scoring rules v.pdf},
  langid = {english},
  number = {3}
}

@online{BriefGuideStan,
  title = {Brief {{Guide}} to {{Stan}}’s {{Warnings}}},
  url = {https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded},
  urldate = {2020-01-28},
  file = {/mnt/data/Google Drive/Zotero/storage/YDGRAKPL/warnings.html}
}

@article{brittonEstimationEmergingEpidemics2019,
  title = {Estimation in Emerging Epidemics: Biases and Remedies},
  shorttitle = {Estimation in Emerging Epidemics},
  author = {Britton, Tom and Scalia Tomba, Gianpaolo},
  date = {2019-01-31},
  journaltitle = {Journal of The Royal Society Interface},
  shortjournal = {Journal of The Royal Society Interface},
  volume = {16},
  pages = {20180670},
  publisher = {{Royal Society}},
  doi = {10.1098/rsif.2018.0670},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsif.2018.0670},
  urldate = {2020-06-05},
  abstract = {When analysing new emerging infectious disease outbreaks, one typically has observational data over a limited period of time and several parameters to estimate, such as growth rate, the basic reproduction number R0, the case fatality rate and distributions of serial intervals, generation times, latency and incubation times and times between onset of symptoms, notification, death and recovery/discharge. These parameters form the basis for predicting a future outbreak, planning preventive measures and monitoring the progress of the disease outbreak. We study inference problems during the emerging phase of an outbreak, and point out potential sources of bias, with emphasis on: contact tracing backwards in time, replacing generation times by serial intervals, multiple potential infectors and censoring effects amplified by exponential growth. These biases directly affect the estimation of, for example, the generation time distribution and the case fatality rate, but can then propagate to other estimates such as R0 and growth rate. We propose methods to remove or at least reduce bias using statistical modelling. We illustrate the theory by numerical examples and simulations.},
  file = {/mnt/data/Google Drive/Zotero/storage/UE77Z7A2/Britton and Scalia Tomba - 2019 - Estimation in emerging epidemics biases and remed.pdf;/mnt/data/Google Drive/Zotero/storage/DZF7TEZF/rsif.2018.html},
  number = {150}
}

@article{brockerEvaluatingRawEnsembles2012,
  title = {Evaluating Raw Ensembles with the Continuous Ranked Probability Score},
  author = {Bröcker, Jochen},
  date = {2012},
  journaltitle = {Quarterly Journal of the Royal Meteorological Society},
  volume = {138},
  pages = {1611--1617},
  issn = {1477-870X},
  doi = {10.1002/qj.1891},
  url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.1891},
  urldate = {2020-09-15},
  abstract = {The continuous ranked probability score (CRPS) is a frequently used scoring rule. In contrast with many other scoring rules, the CRPS evaluates cumulative distribution functions. An ensemble of forecasts can easily be converted into a piecewise constant cumulative distribution function with steps at the ensemble members. This renders the CRPS a convenient scoring rule for the evaluation of ‘raw’ ensembles, obviating the need for sophisticated ensemble model output statistics or dressing methods prior to evaluation. In this article, a relation between the CRPS score and the quantile score is established. The evaluation of ‘raw’ ensembles using the CRPS is discussed in this light. It is shown that latent in this evaluation is an interpretation of the ensemble as quantiles but with non-uniform levels. This needs to be taken into account if the ensemble is evaluated further, for example with rank histograms. Copyright © 2012 Royal Meteorological Society},
  annotation = {\_eprint: https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.1891},
  file = {/mnt/data/Google Drive/Zotero/storage/A7Q6CG9X/Bröcker - 2012 - Evaluating raw ensembles with the continuous ranke.pdf;/mnt/data/Google Drive/Zotero/storage/EDUPVWNY/qj.html},
  keywords = {ensemble forecasts,probability forecasts,quantile score,scoring rules},
  langid = {english},
  number = {667}
}

@article{brockerReliabilitySufficiencyDecomposition2009,
  title = {Reliability, Sufficiency, and the Decomposition of Proper Scores},
  author = {Bröcker, Jochen},
  date = {2009-07},
  journaltitle = {Quarterly Journal of the Royal Meteorological Society},
  shortjournal = {Q.J.R. Meteorol. Soc.},
  volume = {135},
  pages = {1512--1519},
  issn = {00359009, 1477870X},
  doi = {10.1002/qj.456},
  url = {http://doi.wiley.com/10.1002/qj.456},
  urldate = {2020-08-13},
  abstract = {Scoring rules are an important tool for evaluating the performance of probabilistic forecasting schemes. A scoring rule is called strictly proper if its expectation is optimal if and only if the forecast probability represents the true distribution of the target. In the binary case, strictly proper scoring rules allow for a decomposition into terms related to the resolution and to the reliability of the forecast. This fact is particularly well known for the Brier Score. In this paper, this result is extended to forecasts for finite–valued targets. Both resolution and reliability are shown to have a positive effect on the score. It is demonstrated that resolution and reliability are directly related to forecast attributes which are desirable on grounds independent of the notion of scores. This finding can be considered an epistemological justification of measuring forecast quality by proper scores. A link is provided to the original work of DeGroot and Fienberg (1982), extending their concepts of sufficiency and refinement. The relation to the conjectured sharpness principle of Gneiting et al. (2005a) is elucidated.},
  file = {/mnt/data/Google Drive/Zotero/storage/PPAUUYD5/Bröcker - 2009 - Reliability, sufficiency, and the decomposition of.pdf},
  langid = {english},
  number = {643}
}

@article{brockerReliabilitySufficiencyDecomposition2009a,
  title = {Reliability, sufficiency, and the decomposition of proper scores},
  author = {Bröcker, Jochen},
  date = {2009},
  journaltitle = {Quarterly Journal of the Royal Meteorological Society},
  volume = {135},
  pages = {1512--1519},
  issn = {1477-870X},
  doi = {10.1002/qj.456},
  url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.456},
  urldate = {2020-09-05},
  abstract = {Scoring rules are an important tool for evaluating the performance of probabilistic forecasting schemes. A scoring rule is called strictly proper if its expectation is optimal if and only if the forecast probability represents the true distribution of the target. In the binary case, strictly proper scoring rules allow for a decomposition into terms related to the resolution and the reliability of a forecast. This fact is particularly well known for the Brier Score. In this article, this result is extended to forecasts for finite-valued targets. Both resolution and reliability are shown to have a positive effect on the score. It is demonstrated that resolution and reliability are directly related to forecast attributes that are desirable on grounds independent of the notion of scores. This finding can be considered an epistemological justification of measuring forecast quality by proper scoring rules. A link is provided to the original work of DeGroot and Fienberg, extending their concepts of sufficiency and refinement. The relation to the conjectured sharpness principle of Gneiting, et al., is elucidated. Copyright © 2009 Royal Meteorological Society},
  annotation = {\_eprint: https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.456},
  file = {/mnt/data/Google Drive/Zotero/storage/MGX929X8/Bröcker - 2009 - Reliability, sufficiency, and the decomposition of.pdf;/mnt/data/Google Drive/Zotero/storage/52SCPZK3/qj.html},
  keywords = {probabilistic forecasts,reliability,resolution,scoring rules},
  langid = {french},
  number = {643}
}

@article{brooksNonmechanisticForecastsSeasonal2018,
  title = {Nonmechanistic Forecasts of Seasonal Influenza with Iterative One-Week-Ahead Distributions},
  author = {Brooks, Logan C. and Farrow, David C. and Hyun, Sangwon and Tibshirani, Ryan J. and Rosenfeld, Roni},
  editor = {Viboud, Cecile},
  date = {2018-06-15},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {14},
  pages = {e1006134},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006134},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1006134},
  urldate = {2020-03-29},
  abstract = {Accurate and reliable forecasts of seasonal epidemics of infectious disease can assist in the design of countermeasures and increase public awareness and preparedness. This article describes two main contributions we made recently toward this goal: a novel approach to probabilistic modeling of surveillance time series based on “delta densities”, and an optimization scheme for combining output from multiple forecasting methods into an adaptively weighted ensemble. Delta densities describe the probability distribution of the change between one observation and the next, conditioned on available data; chaining together nonparametric estimates of these distributions yields a model for an entire trajectory. Corresponding distributional forecasts cover more observed events than alternatives that treat the whole season as a unit, and improve upon multiple evaluation metrics when extracting key targets of interest to public health officials. Adaptively weighted ensembles integrate the results of multiple forecasting methods, such as delta density, using weights that can change from situation to situation. We treat selection of optimal weightings across forecasting methods as a separate estimation task, and describe an estimation procedure based on optimizing cross-validation performance. We consider some details of the data generation process, including data revisions and holiday effects, both in the construction of these forecasting methods and when performing retrospective evaluation. The delta density method and an adaptively weighted ensemble of other forecasting methods each improve significantly on the next best ensemble component when applied separately, and achieve even better cross-validated performance when used in conjunction. We submitted real-time forecasts based on these contributions as part of CDC’s 2015/2016 FluSight Collaborative Comparison. Among the fourteen submissions that season, this system was ranked by CDC as the most accurate.},
  file = {/mnt/data/Google Drive/Zotero/storage/KWJ8KSUS/Brooks et al. - 2018 - Nonmechanistic forecasts of seasonal influenza wit.pdf},
  langid = {english},
  number = {6}
}

@article{brooksNonmechanisticForecastsSeasonal2018a,
  title = {Nonmechanistic Forecasts of Seasonal Influenza with Iterative One-Week-Ahead Distributions},
  author = {Brooks, Logan C. and Farrow, David C. and Hyun, Sangwon and Tibshirani, Ryan J. and Rosenfeld, Roni},
  editor = {Viboud, Cecile},
  date = {2018-06-15},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {14},
  pages = {e1006134},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006134},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1006134},
  urldate = {2020-03-29},
  abstract = {Accurate and reliable forecasts of seasonal epidemics of infectious disease can assist in the design of countermeasures and increase public awareness and preparedness. This article describes two main contributions we made recently toward this goal: a novel approach to probabilistic modeling of surveillance time series based on “delta densities”, and an optimization scheme for combining output from multiple forecasting methods into an adaptively weighted ensemble. Delta densities describe the probability distribution of the change between one observation and the next, conditioned on available data; chaining together nonparametric estimates of these distributions yields a model for an entire trajectory. Corresponding distributional forecasts cover more observed events than alternatives that treat the whole season as a unit, and improve upon multiple evaluation metrics when extracting key targets of interest to public health officials. Adaptively weighted ensembles integrate the results of multiple forecasting methods, such as delta density, using weights that can change from situation to situation. We treat selection of optimal weightings across forecasting methods as a separate estimation task, and describe an estimation procedure based on optimizing cross-validation performance. We consider some details of the data generation process, including data revisions and holiday effects, both in the construction of these forecasting methods and when performing retrospective evaluation. The delta density method and an adaptively weighted ensemble of other forecasting methods each improve significantly on the next best ensemble component when applied separately, and achieve even better cross-validated performance when used in conjunction. We submitted real-time forecasts based on these contributions as part of CDC’s 2015/2016 FluSight Collaborative Comparison. Among the fourteen submissions that season, this system was ranked by CDC as the most accurate.},
  file = {/mnt/data/Google Drive/Zotero/storage/88JUM5DJ/Brooks et al. - 2018 - Nonmechanistic forecasts of seasonal influenza wit.pdf},
  langid = {english},
  number = {6}
}

@article{buhlmannStatisticalSignificanceHighdimensional2013,
  title = {Statistical Significance in High-Dimensional Linear Models},
  author = {Bühlmann, Peter},
  date = {2013-09},
  journaltitle = {Bernoulli},
  shortjournal = {Bernoulli},
  volume = {19},
  pages = {1212--1242},
  issn = {1350-7265},
  doi = {10.3150/12-BEJSP11},
  url = {http://arxiv.org/abs/1202.1377},
  urldate = {2019-11-19},
  abstract = {We propose a method for constructing p-values for general hypotheses in a high-dimensional linear model. The hypotheses can be local for testing a single regression parameter or they may be more global involving several up to all parameters. Furthermore, when considering many hypotheses, we show how to adjust for multiple testing taking dependence among the p-values into account. Our technique is based on Ridge estimation with an additional correction term due to a substantial projection bias in high dimensions. We prove strong error control for our p-values and provide sufficient conditions for detection: for the former, we do not make any assumption on the size of the true underlying regression coefficients while regarding the latter, our procedure might not be optimal in terms of power. We demonstrate the method in simulated examples and a real data application.},
  archiveprefix = {arXiv},
  eprint = {1202.1377},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/YTRN93FG/Bühlmann - 2013 - Statistical significance in high-dimensional linea.pdf},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  langid = {english},
  number = {4}
}

@article{bullardEvaluationStatisticalMethods2010,
  title = {Evaluation of Statistical Methods for Normalization and Differential Expression in {{mRNA}}-{{Seq}} Experiments},
  author = {Bullard, James H. and Purdom, Elizabeth and Hansen, Kasper D. and Dudoit, Sandrine},
  date = {2010-02-18},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {11},
  pages = {94},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-11-94},
  url = {https://doi.org/10.1186/1471-2105-11-94},
  urldate = {2018-12-02},
  abstract = {High-throughput sequencing technologies, such as the Illumina Genome Analyzer, are powerful new tools for investigating a wide range of biological and medical questions. Statistical and computational methods are key for drawing meaningful and accurate conclusions from the massive and complex datasets generated by the sequencers. We provide a detailed evaluation of statistical methods for normalization and differential expression (DE) analysis of Illumina transcriptome sequencing (mRNA-Seq) data.},
  file = {/mnt/data/Google Drive/Zotero/storage/T6NTKDDI/Bullard et al. - 2010 - Evaluation of statistical methods for normalizatio.pdf;/mnt/data/Google Drive/Zotero/storage/IR9GCWBF/1471-2105-11-94.html},
  number = {1}
}

@online{burknerApproximateLeavefutureoutCrossvalidation2019,
  title = {Approximate Leave-Future-out Cross-Validation for {{Bayesian}} Time Series Models},
  author = {Bürkner, Paul-Christian and Gabry, Jonah and Vehtari, Aki},
  date = {2019-10-02},
  url = {http://arxiv.org/abs/1902.06281},
  urldate = {2020-04-13},
  abstract = {One of the common goals of time series analysis is to use the observed series to inform predictions for future observations. In the absence of any actual new data to predict, cross-validation can be used to estimate a model's future predictive accuracy, for instance, for the purpose of model comparison or selection. Exact cross-validation for Bayesian models is often computationally expensive, but approximate cross-validation methods have been developed, most notably methods for leave-one-out cross-validation (LOO-CV). If the actual prediction task is to predict the future given the past, LOO-CV provides an overly optimistic estimate because the information from future observations is available to influence predictions of the past. To properly account for the time series structure, we can use leave-future-out cross-validation (LFO-CV). Like exact LOO-CV, exact LFO-CV requires refitting the model many times to different subsets of the data. Using Pareto smoothed importance sampling, we propose a method for approximating exact LFO-CV that drastically reduces the computational costs while also providing informative diagnostics about the quality of the approximation.},
  archiveprefix = {arXiv},
  eprint = {1902.06281},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/VYSN2ZSQ/Bürkner et al. - 2019 - Approximate leave-future-out cross-validation for .pdf;/mnt/data/Google Drive/Zotero/storage/62CNNBDG/1902.html},
  keywords = {Statistics - Methodology},
  primaryclass = {stat}
}

@article{burknerBrmsPackageBayesian2017,
  title = {\textbf{Brms} : {{An}} {{\emph{R}}} {{Package}} for {{Bayesian Multilevel Models Using}} {{\emph{Stan}}}},
  shorttitle = {\textbf{Brms}},
  author = {Bürkner, Paul-Christian},
  date = {2017},
  journaltitle = {Journal of Statistical Software},
  shortjournal = {J. Stat. Soft.},
  volume = {80},
  issn = {1548-7660},
  doi = {10.18637/jss.v080.i01},
  url = {http://www.jstatsoft.org/v80/i01/},
  urldate = {2020-05-15},
  abstract = {The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to fit – among others – linear, robust linear, binomial, Poisson, survival, response times, ordinal, quantile, zero-inflated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user defined covariance structures, censored data, as well as metaanalytic standard errors. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. In addition, model fit can easily be assessed and compared using posterior-predictive checks and leave-one-out crossvalidation. If you use brms, please cite this article as published in the Journal of Statistical Software (Bu¨rkner 2017).},
  file = {/mnt/data/Google Drive/Zotero/storage/LX9P9SKT/Bürkner - 2017 - bbrmsb  An iRi Package for Bayesian Mul.pdf},
  langid = {english},
  number = {1}
}

@online{BurtonMalkielRandom,
  title = {Burton {{Malkiel}}'s {{A Random Walk Down Wall Street}}},
  url = {https://www.crcpress.com/Burton-Malkiels-A-Random-Walk-Down-Wall-Street/Burton/p/book/9781912128822},
  urldate = {2019-09-14},
  abstract = {Burton Malkiel’s 1973 A Random Walk Down Wall Street was an explosive contribution to debates about how to reap a good return on investing in stocks and shares. Reissued and updated many times since, Malkiel’s text remains an indispensable contribution to the world of investment strategy – one},
  file = {/mnt/data/Google Drive/Zotero/storage/PN3SUKFL/9781912128822.html},
  langid = {english},
  organization = {{CRC Press}}
}

@article{buttonPowerFailureWhy2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munafò, Marcus R.},
  date = {2013-05},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {14},
  pages = {365--376},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3475},
  url = {http://www.nature.com/articles/nrn3475},
  urldate = {2019-09-16},
  abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
  file = {/mnt/data/Google Drive/Zotero/storage/BPDAKYEF/Button et al. - 2013 - Power failure why small sample size undermines th.pdf},
  langid = {english},
  number = {5}
}

@article{cahillImprovedIdentificationConcordant2018,
  title = {Improved Identification of Concordant and Discordant Gene Expression Signatures Using an Updated Rank-Rank Hypergeometric Overlap Approach},
  author = {Cahill, Kelly M. and Huo, Zhiguang and Tseng, George C. and Logan, Ryan W. and Seney, Marianne L.},
  date = {2018-06-25},
  journaltitle = {Scientific Reports},
  volume = {8},
  pages = {9588},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-27903-2},
  url = {https://www.nature.com/articles/s41598-018-27903-2},
  urldate = {2019-04-30},
  abstract = {Recent advances in large-scale gene expression profiling necessitate concurrent development of biostatistical approaches to reveal meaningful biological relationships. Most analyses rely on significance thresholds for identifying differentially expressed genes. We use an approach to compare gene expression datasets using ‘threshold-free’ comparisons. Significance cut-offs to identify genes shared between datasets may be too stringent and may miss concordant patterns of gene expression with potential biological relevance. A threshold-free approach gaining popularity in several research areas, including neuroscience, is Rank–Rank Hypergeometric Overlap (RRHO). Genes are ranked by their p-value and effect size direction, and ranked lists are compared to identify significantly overlapping genes across a continuous significance gradient rather than at a single arbitrary cut-off. We have updated the previous RRHO analysis by accurately detecting overlap of genes changed in the same and opposite directions between two datasets. Here, we use simulated and real data to show the drawbacks of the previous algorithm as well as the utility of our new algorithm. For example, we show the power of detecting discordant transcriptional patterns in the postmortem brain of subjects with psychiatric disorders. The new R package, RRHO2, offers a new, more intuitive visualization of concordant and discordant gene overlap.},
  file = {/mnt/data/Google Drive/Zotero/storage/EXQLU2YM/Cahill et al. - 2018 - Improved identification of concordant and discorda.pdf;/mnt/data/Google Drive/Zotero/storage/2TAF3FYG/s41598-018-27903-2.html},
  langid = {english},
  number = {1}
}

@article{cahillImprovedIdentificationConcordant2018a,
  title = {Improved Identification of Concordant and Discordant Gene Expression Signatures Using an Updated Rank-Rank Hypergeometric Overlap Approach},
  author = {Cahill, Kelly M. and Huo, Zhiguang and Tseng, George C. and Logan, Ryan W. and Seney, Marianne L.},
  date = {2018-06-25},
  journaltitle = {Scientific Reports},
  volume = {8},
  pages = {9588},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-27903-2},
  url = {https://www.nature.com/articles/s41598-018-27903-2},
  urldate = {2019-05-14},
  abstract = {Recent advances in large-scale gene expression profiling necessitate concurrent development of biostatistical approaches to reveal meaningful biological relationships. Most analyses rely on significance thresholds for identifying differentially expressed genes. We use an approach to compare gene expression datasets using ‘threshold-free’ comparisons. Significance cut-offs to identify genes shared between datasets may be too stringent and may miss concordant patterns of gene expression with potential biological relevance. A threshold-free approach gaining popularity in several research areas, including neuroscience, is Rank–Rank Hypergeometric Overlap (RRHO). Genes are ranked by their p-value and effect size direction, and ranked lists are compared to identify significantly overlapping genes across a continuous significance gradient rather than at a single arbitrary cut-off. We have updated the previous RRHO analysis by accurately detecting overlap of genes changed in the same and opposite directions between two datasets. Here, we use simulated and real data to show the drawbacks of the previous algorithm as well as the utility of our new algorithm. For example, we show the power of detecting discordant transcriptional patterns in the postmortem brain of subjects with psychiatric disorders. The new R package, RRHO2, offers a new, more intuitive visualization of concordant and discordant gene overlap.},
  file = {/mnt/data/Google Drive/Zotero/storage/R7GP5J3D/Cahill et al. - 2018 - Improved identification of concordant and discorda.pdf;/mnt/data/Google Drive/Zotero/storage/UIBAJM9R/s41598-018-27903-2.html},
  langid = {english},
  number = {1}
}

@article{caiBriefReviewMechanisms2009,
  title = {A {{Brief Review}} on the {{Mechanisms}} of {{miRNA Regulation}}},
  author = {Cai, Yimei and Yu, Xiaomin and Hu, Songnian and Yu, Jun},
  date = {2009-12-01},
  journaltitle = {Genomics, Proteomics \& Bioinformatics},
  shortjournal = {Genomics, Proteomics \& Bioinformatics},
  volume = {7},
  pages = {147--154},
  issn = {1672-0229},
  doi = {10.1016/S1672-0229(08)60044-3},
  url = {http://www.sciencedirect.com/science/article/pii/S1672022908600443},
  urldate = {2019-05-22},
  abstract = {MicroRNAs (miRNAs) are a class of short, endogenously-initiated non-coding RNAs that post-transcriptionally control gene expression via either translational repression or mRNA degradation. It is becoming evident that miRNAs are playing significant roles in regulatory mechanisms operating in various organisms, including developmental timing and host-pathogen interactions as well as cell differentiation, proliferation, apoptosis and tumorigenesis. Likewise, as a regulatory element, miRNA itself is coordinatively modulated by multifarious effectors when carrying out basic functions, such as SNP, miRNA editing, methylation and circadian clock. This mini-review summarized the current understanding of interactions between miRNAs and their targets, including recent advancements in deciphering the regulatory mechanisms that control the biogenesis and functionality of miRNAs in various cellular processes.},
  file = {/mnt/data/Google Drive/Zotero/storage/5SXG7AHS/jaskiewicz2012.pdf;/mnt/data/Google Drive/Zotero/storage/7HXU67AZ/Cai et al. - 2009 - A Brief Review on the Mechanisms of miRNA Regulati.pdf;/mnt/data/Google Drive/Zotero/storage/HP44VFNY/S1672022908600443.html},
  keywords = {miRNA,miRNA regulation,non-coding RNA,targets},
  number = {4}
}

@article{camachoCholeraEpidemicYemen2018,
  title = {Cholera Epidemic in {{Yemen}}, 2016–18: An Analysis of Surveillance Data},
  shorttitle = {Cholera Epidemic in {{Yemen}}, 2016–18},
  author = {Camacho, Anton and Bouhenia, Malika and Alyusfi, Reema and Alkohlani, Abdulhakeem and Naji, Munna Abdulla Mohammed and de Radiguès, Xavier and Abubakar, Abdinasir M. and Almoalmi, Abdulkareem and Seguin, Caroline and Sagrado, Maria Jose and Poncin, Marc and McRae, Melissa and Musoke, Mohammed and Rakesh, Ankur and Porten, Klaudia and Haskew, Christopher and Atkins, Katherine E. and Eggo, Rosalind M. and Azman, Andrew S. and Broekhuijsen, Marije and Saatcioglu, Mehmet Akif and Pezzoli, Lorenzo and Quilici, Marie-Laure and Al-Mesbahy, Abdul Rahman and Zagaria, Nevio and Luquero, Francisco J.},
  date = {2018-06-01},
  journaltitle = {The Lancet Global Health},
  shortjournal = {The Lancet Global Health},
  volume = {6},
  pages = {e680-e690},
  issn = {2214-109X},
  doi = {10.1016/S2214-109X(18)30230-4},
  url = {https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(18)30230-4/abstract},
  urldate = {2019-10-29},
  abstract = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}h3{$>$}Background{$<$}/h3{$><$}p{$>$}In war-torn Yemen, reports of confirmed cholera started in late September, 2016. The disease continues to plague Yemen today in what has become the largest documented cholera epidemic of modern times. We aimed to describe the key epidemiological features of this epidemic, including the drivers of cholera transmission during the outbreak.{$<$}/p{$><$}h3{$>$}Methods{$<$}/h3{$><$}p{$>$}The Yemen Health Authorities set up a national cholera surveillance system to collect information on suspected cholera cases presenting at health facilities. Individual variables included symptom onset date, age, severity of dehydration, and rapid diagnostic test result. Suspected cholera cases were confirmed by culture, and a subset of samples had additional phenotypic and genotypic analysis. We first conducted descriptive analyses at national and governorate levels. We divided the epidemic into three time periods: the first wave (Sept 28, 2016, to April 23, 2017), the increasing phase of the second wave (April 24, 2017, to July 2, 2017), and the decreasing phase of the second wave (July 3, 2017, to March 12, 2018). We reconstructed the changes in cholera transmission over time by estimating the instantaneous reproduction number, \emph{R}\textsubscript{t}. Finally, we estimated the association between rainfall and the daily cholera incidence during the increasing phase of the second epidemic wave by fitting a spatiotemporal regression model.{$<$}/p{$><$}h3{$>$}Findings{$<$}/h3{$><$}p{$>$}From Sept 28, 2016, to March 12, 2018, 1 103 683 suspected cholera cases (attack rate 3·69\%) and 2385 deaths (case fatality risk 0·22\%) were reported countrywide. The epidemic consisted of two distinct waves with a surge in transmission in May, 2017, corresponding to a median \emph{R}\textsubscript{t} of more than 2 in 13 of 23 governorates. Microbiological analyses suggested that the same \emph{Vibrio cholerae} O1 Ogawa strain circulated in both waves. We found a positive, non-linear, association between weekly rainfall and suspected cholera incidence in the following 10 days; the relative risk of cholera after a weekly rainfall of 25 mm was 1·42 (95\% CI 1·31–1·55) compared with a week without rain.{$<$}/p{$><$}h3{$>$}Interpretation{$<$}/h3{$><$}p{$>$}Our analysis suggests that the small first cholera epidemic wave seeded cholera across Yemen during the dry season. When the rains returned in April, 2017, they triggered widespread cholera transmission that led to the large second wave. These results suggest that cholera could resurge during the ongoing 2018 rainy season if transmission remains active. Therefore, health authorities and partners should immediately enhance current control efforts to mitigate the risk of a new cholera epidemic wave in Yemen.{$<$}/p{$><$}h3{$>$}Funding{$<$}/h3{$><$}p{$>$}Health Authorities of Yemen, WHO, and Médecins Sans Frontières.{$<$}/p{$>$}},
  eprint = {29731398},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/3LTL4IF4/1-s2.0-S2214109X18302304-mmc1.pdf;/mnt/data/Google Drive/Zotero/storage/5X397UB8/Camacho et al. - 2018 - Cholera epidemic in Yemen, 2016–18 an analysis of.pdf;/mnt/data/Google Drive/Zotero/storage/7F6Q9SUT/fulltext.html},
  langid = {english},
  number = {6}
}

@article{camachoTemporalChangesEbola2015,
  title = {Temporal {{Changes}} in {{Ebola Transmission}} in {{Sierra Leone}} and {{Implications}} for {{Control Requirements}}: A {{Real}}-Time {{Modelling Study}}},
  shorttitle = {Temporal {{Changes}} in {{Ebola Transmission}} in {{Sierra Leone}} and {{Implications}} for {{Control Requirements}}},
  author = {Camacho, Anton and Kucharski, Adam and Aki-Sawyerr, Yvonne and White, Mark A. and Flasche, Stefan and Baguelin, Marc and Pollington, Timothy and Carney, Julia R. and Glover, Rebecca and Smout, Elizabeth and Tiffany, Amanda and Edmunds, W. John and Funk, Sebastian},
  date = {2015-02-10},
  journaltitle = {PLOS Currents Outbreaks},
  shortjournal = {PLoS Curr},
  issn = {2157-3999},
  doi = {10.1371/currents.outbreaks.406ae55e83ec0b5193e30856b9235ed2},
  url = {index.html%3Fp=55052.html},
  urldate = {2020-04-12},
  file = {/mnt/data/Google Drive/Zotero/storage/Q2PBGAHY/Camacho et al. - 2015 - Temporal Changes in Ebola Transmission in Sierra L.pdf;/mnt/data/Google Drive/Zotero/storage/3HPFHCTN/index.htmlp=55052.html},
  langid = {english}
}

@online{CanSymptomsSurveys,
  title = {Can {{Symptoms Surveys Improve COVID}}-19 {{Forecasts}}? | {{DELPHI}}},
  url = {https://delphi.cmu.edu/blog/2020/09/21/can-symptoms-surveys-improve-covid-19-forecasts/},
  urldate = {2020-12-15},
  file = {/mnt/data/Google Drive/Zotero/storage/C2SMT33U/can-symptoms-surveys-improve-covid-19-forecasts.html}
}

@article{chatzilenaContemporaryStatisticalInference2019,
  title = {Contemporary Statistical Inference for Infectious Disease Models Using {{Stan}}},
  author = {Chatzilena, Anastasia and van Leeuwen, Edwin and Ratmann, Oliver and Baguelin, Marc and Demiris, Nikolaos},
  date = {2019-10-05},
  journaltitle = {Epidemics},
  shortjournal = {Epidemics},
  pages = {100367},
  issn = {1755-4365},
  doi = {10.1016/j.epidem.2019.100367},
  url = {http://www.sciencedirect.com/science/article/pii/S1755436519300325},
  urldate = {2019-11-06},
  abstract = {This paper is concerned with the application of recent statistical advances to inference of infectious disease dynamics. We describe the fitting of a class of epidemic models using Hamiltonian Monte Carlo and variational inference as implemented in the freely available Stan software. We apply the two methods to real data from outbreaks as well as routinely collected observations. Our results suggest that both inference methods are computationally feasible in this context, and show a trade-off between statistical efficiency versus computational speed. The latter appears particularly relevant for real-time applications.},
  file = {/mnt/data/Google Drive/Zotero/storage/VUJUVH3M/Chatzilena et al. - 2019 - Contemporary statistical inference for infectious .pdf;/mnt/data/Google Drive/Zotero/storage/VPECFYK6/S1755436519300325.html},
  keywords = {Automatic differentiation variational inference,Epidemic models,Hamiltonian Monte Carlo,No-U-turn sampler,Stan},
  langid = {english},
  options = {useprefix=true}
}

@article{chatzilenaContemporaryStatisticalInference2019a,
  title = {Contemporary Statistical Inference for Infectious Disease Models Using {{Stan}}},
  author = {Chatzilena, Anastasia and van Leeuwen, Edwin and Ratmann, Oliver and Baguelin, Marc and Demiris, Nikolaos},
  date = {2019-12},
  journaltitle = {Epidemics},
  shortjournal = {Epidemics},
  volume = {29},
  pages = {100367},
  issn = {17554365},
  doi = {10.1016/j.epidem.2019.100367},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1755436519300325},
  urldate = {2020-05-17},
  file = {/mnt/data/Google Drive/Zotero/storage/TM6MVY4E/Chatzilena et al. - 2019 - Contemporary statistical inference for infectious .pdf},
  langid = {english},
  options = {useprefix=true}
}

@article{chenDifferentialExpressionAnalysis2015,
  title = {Differential Expression Analysis of {{RNA}} Sequencing Data by Incorporating Non-Exonic Mapped Reads},
  author = {Chen, Hung-I. and Liu, Yuanhang and Zou, Yi and Lai, Zhao and Sarkar, Devanand and Huang, Yufei and Chen, Yidong},
  date = {2015},
  journaltitle = {BMC genomics},
  shortjournal = {BMC Genomics},
  volume = {16 Suppl 7},
  pages = {S14},
  issn = {1471-2164},
  doi = {10.1186/1471-2164-16-S7-S14},
  abstract = {BACKGROUND: RNA sequencing (RNA-seq) is a powerful tool for genome-wide expression profiling of biological samples with the advantage of high-throughput and high resolution. There are many existing algorithms nowadays for quantifying expression levels and detecting differential gene expression, but none of them takes the misaligned reads that are mapped to non-exonic regions into account. We developed a novel algorithm, XBSeq, where a statistical model was established based on the assumption that observed signals are the convolution of true expression signals and sequencing noises. The mapped reads in non-exonic regions are considered as sequencing noises, which follows a Poisson distribution. Given measureable observed and noise signals from RNA-seq data, true expression signals, assuming governed by the negative binomial distribution, can be delineated and thus the accurate detection of differential expressed genes. RESULTS: We implemented our novel XBSeq algorithm and evaluated it by using a set of simulated expression datasets under different conditions, using a combination of negative binomial and Poisson distributions with parameters derived from real RNA-seq data. We compared the performance of our method with other commonly used differential expression analysis algorithms. We also evaluated the changes in true and false positive rates with variations in biological replicates, differential fold changes, and expression levels in non-exonic regions. We also tested the algorithm on a set of real RNA-seq data where the common and different detection results from different algorithms were reported. CONCLUSIONS: In this paper, we proposed a novel XBSeq, a differential expression analysis algorithm for RNA-seq data that takes non-exonic mapped reads into consideration. When background noise is at baseline level, the performance of XBSeq and DESeq are mostly equivalent. However, our method surpasses DESeq and other algorithms with the increase of non-exonic mapped reads. Only in very low read count condition XBSeq had a slightly higher false discovery rate, which may be improved by adjusting the background noise effect in this situation. Taken together, by considering non-exonic mapped reads, XBSeq can provide accurate expression measurement and thus detect differential expressed genes even in noisy conditions.},
  eprint = {26099631},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/6YNWKPLY/Chen et al. - 2015 - Differential expression analysis of RNA sequencing.pdf},
  keywords = {Algorithms,Animals,Gene Expression Profiling,Gene Expression Regulation,Mice,Models; Statistical,Poisson Distribution,Sequence Analysis; RNA},
  langid = {english},
  pmcid = {PMC4474535}
}

@article{chenDifferentialMethylationAnalysis2017,
  title = {Differential Methylation Analysis of Reduced Representation Bisulfite Sequencing Experiments Using {{edgeR}}},
  author = {Chen, Yunshun and Pal, Bhupinder and Visvader, Jane E. and Smyth, Gordon K.},
  date = {2017},
  journaltitle = {F1000Research},
  shortjournal = {F1000Res},
  volume = {6},
  pages = {2055},
  issn = {2046-1402},
  doi = {10.12688/f1000research.13196.2},
  abstract = {Cytosine methylation is an important DNA epigenetic modification. In vertebrates, methylation occurs at CpG sites, which are dinucleotides where a cytosine is immediately followed by a guanine in the DNA sequence from 5' to 3'. When located in the promoter region of a gene, DNA methylation is often associated with transcriptional silencing of the gene. Aberrant DNA methylation is associated with the development of various diseases such as cancer. Bisulfite sequencing (BS-seq) is the current "gold-standard" technology for high-resolution profiling of DNA methylation. Reduced representation bisulfite sequencing (RRBS) is an efficient form of BS-seq that targets CpG-rich DNA regions in order to save sequencing costs. A typical bioinformatics aim is to identify CpGs that are differentially methylated (DM) between experimental conditions. This workflow demonstrates that differential methylation analysis of RRBS data can be conducted using software and methodology originally developed for RNA-seq data. The RNA-seq pipeline is adapted to methylation by adding extra columns to the design matrix to account for read coverage at each CpG, after which the RRBS and RNA-seq pipelines are almost identical. This approach is statistically natural and gives analysts access to a rich collection of analysis tools including generalized linear models, gene set testing and pathway analysis. The article presents a complete start to finish case study analysis of RRBS profiles of different cell populations from the mouse mammary gland using the Bioconductor package edgeR. We show that lineage-committed cells are typically hyper-methylated compared to progenitor cells and this is true on all the autosomes but not the sex chromosomes. We demonstrate a strong negative correlation between methylation of promoter regions and gene expression as measured by RNA-seq for the same cell types, showing that methylation is a regulatory mechanism involved in epithelial linear commitment.},
  eprint = {29333247},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/RL66H59B/Chen et al. - 2018 - Differential methylation analysis of reduced repre.pdf;/mnt/data/Google Drive/Zotero/storage/TQQIKCWG/Chen et al. - 2017 - Differential methylation analysis of reduced repre.pdf;/mnt/data/Google Drive/Zotero/storage/73NX4L6C/v2.html},
  keywords = {Bioconductor,BS-seq,differential methylation analysis,Methylation},
  langid = {english},
  pmcid = {PMC5747346.2}
}

@article{chenGMPRRobustNormalization2018,
  title = {{{GMPR}}: {{A}} Robust Normalization Method for Zero-Inflated Count Data with Application to Microbiome Sequencing Data},
  shorttitle = {{{GMPR}}},
  author = {Chen, Li and Reeve, James and Zhang, Lujun and Huang, Shengbing and Wang, Xuefeng and Chen, Jun},
  date = {2018-04-02},
  journaltitle = {PeerJ},
  shortjournal = {PeerJ},
  volume = {6},
  issn = {2167-8359},
  doi = {10.7717/peerj.4600},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5885979/},
  urldate = {2019-08-14},
  abstract = {Normalization is the first critical step in microbiome sequencing data analysis used to account for variable library sizes. Current RNA-Seq based normalization methods that have been adapted for microbiome data fail to consider the unique characteristics of microbiome data, which contain a vast number of zeros due to the physical absence or under-sampling of the microbes. Normalization methods that specifically address the zero-inflation remain largely undeveloped. Here we propose geometric mean of pairwise ratios—a simple but effective normalization method—for zero-inflated sequencing data such as microbiome data. Simulation studies and real datasets analyses demonstrate that the proposed method is more robust than competing methods, leading to more powerful detection of differentially abundant taxa and higher reproducibility of the relative abundances of taxa.},
  eprint = {29629248},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/ZEF84UIL/Chen et al. - 2018 - GMPR A robust normalization method for zero-infla.pdf},
  pmcid = {PMC5885979}
}

@online{christianoDeepReinforcementLearning2017,
  title = {Deep Reinforcement Learning from Human Preferences},
  author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  date = {2017-07-13},
  url = {http://arxiv.org/abs/1706.03741},
  urldate = {2019-12-05},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
  archiveprefix = {arXiv},
  eprint = {1706.03741},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/7I5CFBB6/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf;/mnt/data/Google Drive/Zotero/storage/73CK9WFQ/1706.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{christianoSupervisingStrongLearners2018,
  title = {Supervising Strong Learners by Amplifying Weak Experts},
  author = {Christiano, Paul and Shlegeris, Buck and Amodei, Dario},
  date = {2018-10-19},
  url = {http://arxiv.org/abs/1810.08575},
  urldate = {2019-12-05},
  abstract = {Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017b), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.},
  archiveprefix = {arXiv},
  eprint = {1810.08575},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/AGR998GI/Christiano et al. - 2018 - Supervising strong learners by amplifying weak exp.pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryclass = {cs, stat}
}

@article{claeskensForecastCombinationPuzzle2016,
  title = {The Forecast Combination Puzzle: {{A}} Simple Theoretical Explanation},
  shorttitle = {The Forecast Combination Puzzle},
  author = {Claeskens, Gerda and Magnus, Jan R. and Vasnev, Andrey L. and Wang, Wendun},
  date = {2016-07},
  journaltitle = {International Journal of Forecasting},
  shortjournal = {International Journal of Forecasting},
  volume = {32},
  pages = {754--762},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2015.12.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207016000327},
  urldate = {2021-07-02},
  abstract = {This paper offers a theoretical explanation for the stylized fact that forecast combinations with estimated optimal weights often perform poorly in applications. The properties of the forecast combination are typically derived under the assumption that the weights are fixed, while in practice they need to be estimated. If the fact that the weights are random rather than fixed is taken into account during the optimality derivation, then the forecast combination will be biased (even when the original forecasts are unbiased), and its variance will be larger than in the fixed-weight case. In particular, there is no guarantee that the ‘optimal’ forecast combination will be better than the equal-weight case, or even improve on the original forecasts. We provide the underlying theory, some special cases, and a numerical illustration.},
  file = {/mnt/data/Google Drive/Zotero/storage/4TUUCQXG/Claeskens et al. - 2016 - The forecast combination puzzle A simple theoreti.pdf},
  langid = {english},
  number = {3}
}

@article{clarkeComparingBayesModel,
  title = {Comparing {{Bayes Model Averaging}} and {{Stacking When Model Approximation Error Cannot}} Be {{Ignored}}},
  author = {Clarke, Bertrand},
  pages = {30},
  abstract = {We compare Bayes Model Averaging, BMA, to a non-Bayes form of model averaging called stacking. In stacking, the weights are no longer posterior probabilities of models; they are obtained by a technique based on cross-validation. When the correct data generating model (DGM) is on the list of models under consideration BMA is never worse than stacking and often is demonstrably better, provided that the noise level is of order commensurate with the coefficients and explanatory variables. Here, however, we focus on the case that the correct DGM is not on the model list and may not be well approximated by the elements on the model list.},
  file = {/mnt/data/Google Drive/Zotero/storage/SFV2ENGU/Clarke - Comparing Bayes Model Averaging and Stacking When .pdf},
  langid = {english}
}

@article{clemenDebiasingExpertOverconfidence,
  title = {Debiasing {{Expert Overconﬁdence}}: {{A Bayesian Calibration Model}}},
  author = {Clemen, Robert T and Lichtendahl, Kenneth C},
  pages = {16},
  abstract = {In a decision and risk analysis, experts may provide subjective probability distributions that encode their beliefs about future uncertain events. For continuous variables, experts often provide these judgments in the form of quantiles of the distribution (e.g., 5th, 50th, and 95th percentiles). Psychologists have shown, though, that such subjective distributions tend to be too narrow, representing overconfidence on the part of the expert. We propose an approach for modeling and debiasing expert overconfidence. Based on past performance data (previous assessments and realizations for a number of uncertain variables), and using Bayesian methods to update prior distributions on the model parameters, we show how our model can be used to debias expert probabilities. We develop and demonstrate both a single-expert model and a multiple-expert hierarchical model.},
  file = {/mnt/data/Google Drive/Zotero/storage/QZGUKK2P/Clemen and Lichtendahl - Debiasing Expert Overconﬁdence A Bayesian Calibra.pdf},
  langid = {english}
}

@article{CoIntegrationErrorCorrection,
  title = {Co-{{Integration}} and {{Error Correction}}: {{Representation}}, {{Estimation}}, and {{Testing}}},
  pages = {27},
  file = {/mnt/data/Google Drive/Zotero/storage/TAINM6HK/Co-Integration and Error Correction Representatio.pdf},
  langid = {english}
}

@online{CollaborativeMultiyearMultimodel,
  title = {A Collaborative Multiyear, Multimodel Assessment of Seasonal Influenza Forecasting in the {{United States}} | {{PNAS}}},
  url = {https://www.pnas.org/content/116/8/3146},
  urldate = {2020-04-12},
  file = {/mnt/data/Google Drive/Zotero/storage/Z66CQLI6/3146.html}
}

@article{colon-gonzalezProbabilisticSeasonalDengue2021,
  title = {Probabilistic Seasonal Dengue Forecasting in {{Vietnam}}: {{A}} Modelling Study Using Superensembles},
  shorttitle = {Probabilistic Seasonal Dengue Forecasting in {{Vietnam}}},
  author = {Colón-González, Felipe J. and Bastos, Leonardo Soares and Hofmann, Barbara and Hopkin, Alison and Harpham, Quillon and Crocker, Tom and Amato, Rosanna and Ferrario, Iacopo and Moschini, Francesca and James, Samuel and Malde, Sajni and Ainscoe, Eleanor and Nam, Vu Sinh and Tan, Dang Quang and Khoa, Nguyen Duc and Harrison, Mark and Tsarouchi, Gina and Lumbroso, Darren and Brady, Oliver J. and Lowe, Rachel},
  date = {2021-03-04},
  journaltitle = {PLOS Medicine},
  shortjournal = {PLOS Medicine},
  volume = {18},
  pages = {e1003542},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1003542},
  url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003542},
  urldate = {2021-03-06},
  abstract = {Background With enough advanced notice, dengue outbreaks can be mitigated. As a climate-sensitive disease, environmental conditions and past patterns of dengue can be used to make predictions about future outbreak risk. These predictions improve public health planning and decision-making to ultimately reduce the burden of disease. Past approaches to dengue forecasting have used seasonal climate forecasts, but the predictive ability of a system using different lead times in a year-round prediction system has been seldom explored. Moreover, the transition from theoretical to operational systems integrated with disease control activities is rare. Methods and findings We introduce an operational seasonal dengue forecasting system for Vietnam where Earth observations, seasonal climate forecasts, and lagged dengue cases are used to drive a superensemble of probabilistic dengue models to predict dengue risk up to 6 months ahead. Bayesian spatiotemporal models were fit to 19 years (2002–2020) of dengue data at the province level across Vietnam. A superensemble of these models then makes probabilistic predictions of dengue incidence at various future time points aligned with key Vietnamese decision and planning deadlines. We demonstrate that the superensemble generates more accurate predictions of dengue incidence than the individual models it incorporates across a suite of time horizons and transmission settings. Using historical data, the superensemble made slightly more accurate predictions (continuous rank probability score [CRPS] = 66.8, 95\% CI 60.6–148.0) than a baseline model which forecasts the same incidence rate every month (CRPS = 79.4, 95\% CI 78.5–80.5) at lead times of 1 to 3 months, albeit with larger uncertainty. The outbreak detection capability of the superensemble was considerably larger (69\%) than that of the baseline model (54.5\%). Predictions were most accurate in southern Vietnam, an area that experiences semi-regular seasonal dengue transmission. The system also demonstrated added value across multiple areas compared to previous practice of not using a forecast. We use the system to make a prospective prediction for dengue incidence in Vietnam for the period May to October 2020. Prospective predictions made with the superensemble were slightly more accurate (CRPS = 110, 95\% CI 102–575) than those made with the baseline model (CRPS = 125, 95\% CI 120–168) but had larger uncertainty. Finally, we propose a framework for the evaluation of probabilistic predictions. Despite the demonstrated value of our forecasting system, the approach is limited by the consistency of the dengue case data, as well as the lack of publicly available, continuous, and long-term data sets on mosquito control efforts and serotype-specific case data. Conclusions This study shows that by combining detailed Earth observation data, seasonal climate forecasts, and state-of-the-art models, dengue outbreaks can be predicted across a broad range of settings, with enough lead time to meaningfully inform dengue control. While our system omits some important variables not currently available at a subnational scale, the majority of past outbreaks could be predicted up to 3 months ahead. Over the next 2 years, the system will be prospectively evaluated and, if successful, potentially extended to other areas and other climate-sensitive disease systems.},
  file = {/mnt/data/Google Drive/Zotero/storage/9FBQQDLX/Colón-González et al. - 2021 - Probabilistic seasonal dengue forecasting in Vietn.pdf;/mnt/data/Google Drive/Zotero/storage/BQ47TPND/article.html},
  keywords = {Decision making,Dengue fever,Forecasting,Humidity,Mosquitoes,Public and occupational health,Seasons,Vietnam},
  langid = {english},
  number = {3}
}

@article{cookeHighlightsExpertJudgment,
  title = {Highlights of the {{Expert Judgment Policy Symposium}} and {{Technical Workshop}}},
  author = {Cooke, Roger M and Probst, Katherine N},
  pages = {32},
  file = {/mnt/data/Google Drive/Zotero/storage/9IRP3GZ2/Cooke and Probst - Highlights of the Expert Judgment Policy Symposium.pdf},
  langid = {english}
}

@article{cooperMethodDetectingCharacterizing2015,
  title = {A Method for Detecting and Characterizing Outbreaks of Infectious Disease from Clinical Reports},
  author = {Cooper, Gregory F. and Villamarin, Ricardo and (Rich) Tsui, Fu-Chiang and Millett, Nicholas and Espino, Jeremy U. and Wagner, Michael M.},
  date = {2015-02},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {Journal of Biomedical Informatics},
  volume = {53},
  pages = {15--26},
  issn = {15320464},
  doi = {10.1016/j.jbi.2014.08.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046414001920},
  urldate = {2019-10-08},
  abstract = {Outbreaks of infectious disease can pose a significant threat to human health. Thus, detecting and characterizing outbreaks quickly and accurately remains an important problem. This paper describes a Bayesian framework that links clinical diagnosis of individuals in a population to epidemiological modeling of disease outbreaks in the population. Computer-based diagnosis of individuals who seek healthcare is used to guide the search for epidemiological models of population disease that explain the pattern of diagnoses well. We applied this framework to develop a system that detects influenza outbreaks from emergency department (ED) reports. The system diagnoses influenza in individuals probabilistically from evidence in ED reports that are extracted using natural language processing. These diagnoses guide the search for epidemiological models of influenza that explain the pattern of diagnoses well. Those epidemiological models with a high posterior probability determine the most likely outbreaks of specific diseases; the models are also used to characterize properties of an outbreak, such as its expected peak day and estimated size. We evaluated the method using both simulated data and data from a real influenza outbreak. The results provide support that the approach can detect and characterize outbreaks early and well enough to be valuable. We describe several extensions to the approach that appear promising.},
  file = {/mnt/data/Google Drive/Zotero/storage/TPYMSQ39/Cooper et al. - 2015 - A method for detecting and characterizing outbreak.pdf},
  langid = {english}
}

@article{coriNewFrameworkSoftware2013,
  title = {A {{New Framework}} and {{Software}} to {{Estimate Time}}-{{Varying Reproduction Numbers During Epidemics}}},
  author = {Cori, Anne and Ferguson, Neil M. and Fraser, Christophe and Cauchemez, Simon},
  date = {2013-11-01},
  journaltitle = {American Journal of Epidemiology},
  shortjournal = {Am J Epidemiol},
  volume = {178},
  pages = {1505--1512},
  issn = {0002-9262},
  doi = {10.1093/aje/kwt133},
  url = {https://academic.oup.com/aje/article/178/9/1505/89262},
  urldate = {2019-10-30},
  abstract = {Abstract.  The quantification of transmissibility during epidemics is essential to designing and adjusting public health responses. Transmissibility can be meas},
  file = {/mnt/data/Google Drive/Zotero/storage/827JXSHL/Cori et al. - 2013 - A New Framework and Software to Estimate Time-Vary.pdf;/mnt/data/Google Drive/Zotero/storage/UY7M5Z9A/89262.html},
  langid = {english},
  number = {9}
}

@article{coriNewFrameworkSoftware2013a,
  title = {A {{New Framework}} and {{Software}} to {{Estimate Time}}-{{Varying Reproduction Numbers During Epidemics}}},
  author = {Cori, Anne and Ferguson, Neil M. and Fraser, Christophe and Cauchemez, Simon},
  date = {2013-11-01},
  journaltitle = {American Journal of Epidemiology},
  shortjournal = {Am J Epidemiol},
  volume = {178},
  pages = {1505--1512},
  issn = {0002-9262},
  doi = {10.1093/aje/kwt133},
  url = {https://academic.oup.com/aje/article/178/9/1505/89262},
  urldate = {2019-11-01},
  abstract = {Abstract.  The quantification of transmissibility during epidemics is essential to designing and adjusting public health responses. Transmissibility can be meas},
  file = {/mnt/data/Google Drive/Zotero/storage/3HRUA5K9/kwt133supp.doc;/mnt/data/Google Drive/Zotero/storage/V5NSW7DF/Cori et al. - 2013 - A New Framework and Software to Estimate Time-Vary.pdf;/mnt/data/Google Drive/Zotero/storage/FMCT4D4Z/governor.html},
  langid = {english},
  number = {9}
}

@online{COVID19DataExplorer,
  title = {{{COVID}}-19 {{Data Explorer}}},
  url = {https://ourworldindata.org/coronavirus-data-explorer},
  urldate = {2021-05-30},
  abstract = {Research and data to make progress against the world’s largest problems},
  file = {/mnt/data/Google Drive/Zotero/storage/LUGP5UBF/coronavirus-data-explorer.html},
  organization = {{Our World in Data}}
}

@dataset{cramerCOVID19ForecastHub2020,
  title = {{{COVID}}-19 {{Forecast Hub}}: 4 {{December}} 2020 Snapshot},
  shorttitle = {{{COVID}}-19 {{Forecast Hub}}},
  author = {Cramer, Estee and Nicholas G Reich and Serena Yijin Wang and Jarad Niemi and Abdul Hannan and Katie House and Youyang Gu and Shanghong Xie and Steve Horstman and {aniruddhadiga} and Robert Walraven and {starkari} and Michael Lingzhi Li and Graham Gibson and Lauren Castro and Dean Karlen and Nutcha Wattanachit and {jinghuichen} and {zyt9lsb} and {aagarwal1996} and Spencer Woody and Evan Ray and Frost Tianjian Xu and Hannah Biegel and GuidoEspana and Xinyue X and Johannes Bracher and Elizabeth Lee and {har96} and {leyouz}},
  date = {2020-12-04},
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.3963371},
  url = {https://zenodo.org/record/3963371},
  urldate = {2021-05-29},
  abstract = {This update to the COVID-19 Forecast Hub repository is a snapshot as of 4 December 2020 of the data hosted by and visualized at~https://covid19forecasthub.org/.},
  file = {/mnt/data/Google Drive/Zotero/storage/AVWA2UPE/4305938.html}
}

@article{cramerEvaluationIndividualEnsemble2021,
  title = {Evaluation of Individual and Ensemble Probabilistic Forecasts of {{COVID}}-19 Mortality in the {{US}}},
  author = {Cramer, Estee and Ray, Evan L. and Lopez, Velma K. and Bracher, Johannes and Brennen, Andrea and Rivadeneira, Alvaro J. Castro and Gerding, Aaron and Gneiting, Tilmann and House, Katie H. and Huang, Yuxin and Jayawardena, Dasuni and Kanji, Abdul H. and Khandelwal, Ayush and Le, Khoa and Mühlemann, Anja and Niemi, Jarad and Shah, Apurv and Stark, Ariane and Wang, Yijin and Wattanachit, Nutcha and Zorn, Martha W. and Gu, Youyang and Jain, Sansiddh and Bannur, Nayana and Deva, Ayush and Kulkarni, Mihir and Merugu, Srujana and Raval, Alpan and Shingi, Siddhant and Tiwari, Avtansh and White, Jerome and Woody, Spencer and Dahan, Maytal and Fox, Spencer and Gaither, Kelly and Lachmann, Michael and Meyers, Lauren Ancel and Scott, James G. and Tec, Mauricio and Srivastava, Ajitesh and George, Glover E. and Cegan, Jeffrey C. and Dettwiller, Ian D. and England, William P. and Farthing, Matthew W. and Hunter, Robert H. and Lafferty, Brandon and Linkov, Igor and Mayo, Michael L. and Parno, Matthew D. and Rowland, Michael A. and Trump, Benjamin D. and Corsetti, Sabrina M. and Baer, Thomas M. and Eisenberg, Marisa C. and Falb, Karl and Huang, Yitao and Martin, Emily T. and McCauley, Ella and Myers, Robert L. and Schwarz, Tom and Sheldon, Daniel and Gibson, Graham Casey and Yu, Rose and Gao, Liyao and Ma, Yian and Wu, Dongxia and Yan, Xifeng and Jin, Xiaoyong and Wang, Yu-Xiang and Chen, YangQuan and Guo, Lihong and Zhao, Yanting and Gu, Quanquan and Chen, Jinghui and Wang, Lingxiao and Xu, Pan and Zhang, Weitong and Zou, Difan and Biegel, Hannah and Lega, Joceline and Snyder, Timothy L. and Wilson, Davison D. and McConnell, Steve and Walraven, Robert and Shi, Yunfeng and Ban, Xuegang and Hong, Qi-Jun and Kong, Stanley and Turtle, James A. and Ben-Nun, Michal and Riley, Pete and Riley, Steven and Koyluoglu, Ugur and DesRoches, David and Hamory, Bruce and Kyriakides, Christina and Leis, Helen and Milliken, John and Moloney, Michael and Morgan, James and Ozcan, Gokce and Schrader, Chris and Shakhnovich, Elizabeth and Siegel, Daniel and Spatz, Ryan and Stiefeling, Chris and Wilkinson, Barrie and Wong, Alexander and Gao, Zhifeng and Bian, Jiang and Cao, Wei and Ferres, Juan Lavista and Li, Chaozhuo and Liu, Tie-Yan and Xie, Xing and Zhang, Shun and Zheng, Shun and Vespignani, Alessandro and Chinazzi, Matteo and Davis, Jessica T. and Mu, Kunpeng and y Piontti, Ana Pastore and Xiong, Xinyue and Zheng, Andrew and Baek, Jackie and Farias, Vivek and Georgescu, Andreea and Levi, Retsef and Sinha, Deeksha and Wilde, Joshua and Penna, Nicolas D. and Celi, Leo A. and Sundar, Saketh and Cavany, Sean and España, Guido and Moore, Sean and Oidtman, Rachel and Perkins, Alex and Osthus, Dave and Castro, Lauren and Fairchild, Geoffrey and Michaud, Isaac and Karlen, Dean and Lee, Elizabeth C. and Dent, Juan and Grantz, Kyra H. and Kaminsky, Joshua and Kaminsky, Kathryn and Keegan, Lindsay T. and Lauer, Stephen A. and Lemaitre, Joseph C. and Lessler, Justin and Meredith, Hannah R. and Perez-Saez, Javier and Shah, Sam and Smith, Claire P. and Truelove, Shaun A. and Wills, Josh and Kinsey, Matt and Obrecht, R. F. and Tallaksen, Katharine and Burant, John C. and Wang, Lily and Gao, Lei and Gu, Zhiling and Kim, Myungjin and Li, Xinyi and Wang, Guannan and Wang, Yueying and Yu, Shan and Reiner, Robert C. and Barber, Ryan and Gaikedu, Emmanuela and Hay, Simon and Lim, Steve and Murray, Chris and Pigott, David and Prakash, B. Aditya and Adhikari, Bijaya and Cui, Jiaming and Rodríguez, Alexander and Tabassum, Anika and Xie, Jiajia and Keskinocak, Pinar and Asplund, John and Baxter, Arden and Oruc, Buse Eylul and Serban, Nicoleta and Arik, Sercan O. and Dusenberry, Mike and Epshteyn, Arkady and Kanal, Elli and Le, Long T. and Li, Chun-Liang and Pfister, Tomas and Sava, Dario and Sinha, Rajarishi and Tsai, Thomas and Yoder, Nate and Yoon, Jinsung and Zhang, Leyou and Abbott, Sam and Bosse, Nikos I. and Funk, Sebastian and Hellewel, Joel and Meakin, Sophie R. and Munday, James D. and Sherratt, Katherine and Zhou, Mingyuan and Kalantari, Rahi and Yamana, Teresa K. and Pei, Sen and Shaman, Jeffrey and Ayer, Turgay and Adee, Madeline and Chhatwal, Jagpreet and Dalgic, Ozden O. and Ladd, Mary A. and Linas, Benjamin P. and Mueller, Peter and Xiao, Jade and Li, Michael L. and Bertsimas, Dimitris and Lami, Omar Skali and Soni, Saksham and Bouardi, Hamza Tazi and Wang, Yuanjia and Wang, Qinxia and Xie, Shanghong and Zeng, Donglin and Green, Alden and Bien, Jacob and Hu, Addison J. and Jahja, Maria and Narasimhan, Balasubramanian and Rajanala, Samyak and Rumack, Aaron and Simon, Noah and Tibshirani, Ryan and Tibshirani, Rob and Ventura, Valerie and Wasserman, Larry and O’Dea, Eamon B. and Drake, John M. and Pagano, Robert and Walker, Jo W. and Slayton, Rachel B. and Johansson, Michael and Biggerstaff, Matthew and Reich, Nicholas G.},
  date = {2021-02-05},
  journaltitle = {medRxiv},
  pages = {2021.02.03.21250974},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2021.02.03.21250974},
  url = {https://www.medrxiv.org/content/10.1101/2021.02.03.21250974v1},
  urldate = {2021-04-06},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Short-term probabilistic forecasts of the trajectory of the COVID-19 pandemic in the United States have served as a visible and important communication channel between the scientific modeling community and both the general public and decision-makers. Forecasting models provide specific, quantitative, and evaluable predictions that inform short-term decisions such as healthcare staffing needs, school closures, and allocation of medical supplies. In 2020, the COVID-19 Forecast Hub (https://covid19forecasthub.org/) collected, disseminated, and synthesized hundreds of thousands of specific predictions from more than 50 different academic, industry, and independent research groups. This manuscript systematically evaluates 23 models that regularly submitted forecasts of reported weekly incident COVID-19 mortality counts in the US at the state and national level. One of these models was a multi-model ensemble that combined all available forecasts each week. The performance of individual models showed high variability across time, geospatial units, and forecast horizons. Half of the models evaluated showed better accuracy than a naïve baseline model. In combining the forecasts from all teams, the ensemble showed the best overall probabilistic accuracy of any model. Forecast accuracy degraded as models made predictions farther into the future, with probabilistic accuracy at a 20-week horizon more than 5 times worse than when predicting at a 1-week horizon. This project underscores the role that collaboration and active coordination between governmental public health agencies, academic modeling teams, and industry partners can play in developing modern modeling capabilities to support local, state, and federal response to outbreaks.{$<$}/p{$>$}},
  file = {/mnt/data/Google Drive/Zotero/storage/W82X9ZN5/Cramer et al. - 2021 - Evaluation of individual and ensemble probabilisti.pdf;/mnt/data/Google Drive/Zotero/storage/7MC6LGTC/2021.02.03.21250974v1.html},
  langid = {english}
}

@article{cuiHEPeakHMMbasedExome2015,
  title = {{{HEPeak}}: An {{HMM}}-Based Exome Peak-Finding Package for {{RNA}} Epigenome Sequencing Data},
  shorttitle = {{{HEPeak}}},
  author = {Cui, Xiaodong and Meng, Jia and Rao, Manjeet K. and Chen, Yidong and Huang, Yufei},
  date = {2015},
  journaltitle = {BMC genomics},
  shortjournal = {BMC Genomics},
  volume = {16 Suppl 4},
  pages = {S2},
  issn = {1471-2164},
  doi = {10.1186/1471-2164-16-S4-S2},
  abstract = {BACKGROUND: Methylated RNA Immunoprecipatation combined with RNA sequencing (MeRIP-seq) is revolutionizing the de novo study of RNA epigenomics at a higher resolution. However, this new technology poses unique bioinformatics problems that call for novel and sophisticated statistical computational solutions, aiming at identifying and characterizing transcriptome-wide methyltranscriptome. RESULTS: We developed HEP, a Hidden Markov Model (HMM)-based Exome Peak-finding algorithm for predicting transcriptome methylation sites using MeRIP-seq data. In contrast to exomePeak, our previously developed MeRIP-seq peak calling algorithm, HEPeak models the correlation between continuous bins in an m6A peak region and it is a model-based approach, which admits rigorous statistical inference. HEPeak was evaluated on a simulated MeRIP-seq dataset and achieved higher sensitivity and specificity than exomePeak. HEPeak was also applied to real MeRIP-seq datasets from human HEK293T cell line and mouse midbrain cells and was shown to be able to recapitulate known m6A distribution in transcripts and identify novel m6A sites in long non-coding RNAs. CONCLUSIONS: In this paper, a novel HMM-based peak calling algorithm, HEPeak, was developed for peak calling for MeRIP-seq data. HEPeak is written in R and is publicly available.},
  eprint = {25917296},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/G7BDPQPG/Cui et al. - 2015 - HEPeak an HMM-based exome peak-finding package fo.pdf},
  keywords = {Algorithms,Animals,Cells; Cultured,Epigenomics,Exome,HEK293 Cells,Humans,Markov Chains,Mesencephalon,Methylation,Mice,RNA Processing; Post-Transcriptional,RNA; Messenger,Sequence Analysis; RNA},
  langid = {english},
  pmcid = {PMC4416174}
}

@article{cuiMeTDiffNovelDifferential2018,
  title = {{{MeTDiff}}: {{A Novel Differential RNA Methylation Analysis}} for {{MeRIP}}-{{Seq Data}}},
  shorttitle = {{{MeTDiff}}},
  author = {Cui, X. and Zhang, L. and Meng, J. and Rao, M. K. and Chen, Y. and Huang, Y.},
  date = {2018-03},
  journaltitle = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  volume = {15},
  pages = {526--534},
  issn = {1545-5963},
  doi = {10.1109/TCBB.2015.2403355},
  abstract = {N6-Methyladenosine (m6A) transcriptome methylation is an exciting new research area that just captures the attention of research community. We present in this paper, MeTDiff, a novel computational tool for predicting differential m6A methylation sites from Methylated RNA immunoprecipitation sequencing (MeRIP-Seq) data. Compared with the existing algorithm exomePeak, the advantages of MeTDiff are that it explicitly models the reads variation in data and also devices a more power likelihood ratio test for differential methylation site prediction. Comprehensive evaluation of MeTDiff's performance using both simulated and real datasets showed that MeTDiff is much more robust and achieved much higher sensitivity and specificity over exomePeak.},
  file = {/mnt/data/Google Drive/Zotero/storage/DTRMZZMY/Cui et al. - 2018 - MeTDiff A Novel Differential RNA Methylation Anal.pdf;/mnt/data/Google Drive/Zotero/storage/X7MDSBTD/7052329.html},
  keywords = {beta-binomial modeling,bioinformatics,Bioinformatics,Computational biology,Computational modeling,data analysis,differential m6A methylation sites,differential methylation site prediction,differential RNA methylation,differential RNA methylation analysis,exomePeak,genetics,IEEE transactions,IP networks,MeRIP-Seq data,MeTDiff,methylated RNA immunoprecipitation sequencing data,molecular biophysics,molecular configurations,N6-Methyladenosine (m6A),N6-methyladenosine transcriptome methylation,power likelihood ratio test,RNA,Robustness,Sequential analysis},
  number = {2}
}

@article{cuiNovelAlgorithmCalling2016,
  title = {A Novel Algorithm for Calling {{mRNA m6A}} Peaks by Modeling Biological Variances in {{MeRIP}}-Seq Data},
  author = {Cui, Xiaodong and Meng, Jia and Zhang, Shaowu and Chen, Yidong and Huang, Yufei},
  date = {2016-06-15},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {32},
  pages = {i378-i385},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btw281},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4908365/},
  urldate = {2018-09-10},
  abstract = {Motivation: N6-methyl-adenosine (m6A) is the most prevalent mRNA methylation but precise prediction of its mRNA location is important for understanding its function. A recent sequencing technology, known as Methylated RNA Immunoprecipitation Sequencing technology (MeRIP-seq), has been developed for transcriptome-wide profiling of m6A. We previously developed a peak calling algorithm called exomePeak. However, exomePeak over-simplifies data characteristics and ignores the reads’ variances among replicates or reads dependency across a site region. To further improve the performance, new model is needed to address these important issues of MeRIP-seq data., Results: We propose a novel, graphical model-based peak calling method, MeTPeak, for transcriptome-wide detection of m6A sites from MeRIP-seq data. MeTPeak explicitly models read count of an m6A site and introduces a hierarchical layer of Beta variables to capture the variances and a Hidden Markov model to characterize the reads dependency across a site. In addition, we developed a constrained Newton’s method and designed a log-barrier function to compute analytically intractable, positively constrained Beta parameters. We applied our algorithm to simulated and real biological datasets and demonstrated significant improvement in detection performance and robustness over exomePeak. Prediction results on publicly available MeRIP-seq datasets are also validated and shown to be able to recapitulate the known patterns of m6A, further validating the improved performance of MeTPeak., Availability and implementation: The package ‘MeTPeak’ is implemented in R and C\,++, and additional details are available at https://github.com/compgenomics/MeTPeak, Contact: yufei.huang@utsa.edu or xdchoi@gmail.com, Supplementary information: Supplementary data are available at Bioinformatics online.},
  eprint = {27307641},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/IYJGCLY9/Cui et al. - 2016 - A novel algorithm for calling mRNA m6A peaks by mo.pdf;/mnt/data/Google Drive/Zotero/storage/N727H4U4/Cui et al. - 2016 - A novel algorithm for calling mRNA m6A peaks by mo.pdf},
  number = {12},
  pmcid = {PMC4908365}
}

@article{czadoPredictiveModelAssessment2009a,
  title = {Predictive {{Model Assessment}} for {{Count Data}}},
  author = {Czado, Claudia and Gneiting, Tilmann and Held, Leonhard},
  date = {2009},
  journaltitle = {Biometrics},
  volume = {65},
  pages = {1254--1261},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2009.01191.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2009.01191.x},
  urldate = {2020-08-12},
  abstract = {We discuss tools for the evaluation of probabilistic forecasts and the critique of statistical models for count data. Our proposals include a nonrandomized version of the probability integral transform, marginal calibration diagrams, and proper scoring rules, such as the predictive deviance. In case studies, we critique count regression models for patent data, and assess the predictive performance of Bayesian age-period-cohort models for larynx cancer counts in Germany. The toolbox applies in Bayesian or classical and parametric or nonparametric settings and to any type of ordered discrete outcomes.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2009.01191.x},
  file = {/mnt/data/Google Drive/Zotero/storage/SKRXXEYJ/Czado et al. - 2009 - Predictive Model Assessment for Count Data.pdf;/mnt/data/Google Drive/Zotero/storage/Y3Z8AY8F/j.1541-0420.2009.01191.html},
  keywords = {Calibration,Forecast verification,Model diagnostics,Predictive deviance,Probability integral transform,Proper scoring rule},
  langid = {english},
  number = {4}
}

@online{Dash,
  title = {Dash},
  url = {http://covid19icu.cl.cam.ac.uk/},
  urldate = {2020-04-12},
  file = {/mnt/data/Google Drive/Zotero/storage/463VXN77/covid19icu.cl.cam.ac.uk.html}
}

@report{daviesEffectNonpharmaceuticalInterventions2020,
  title = {The Effect of Non-Pharmaceutical Interventions on {{COVID}}-19 Cases, Deaths and Demand for Hospital Services in the {{UK}}: A Modelling Study},
  shorttitle = {The Effect of Non-Pharmaceutical Interventions on {{COVID}}-19 Cases, Deaths and Demand for Hospital Services in the {{UK}}},
  author = {Davies, Nicholas G and Kucharski, Adam J and Eggo, Rosalind M and Gimma, Amy and {CMMID COVID-19 Working Group} and Edmunds, W. John},
  date = {2020-04-06},
  institution = {{Infectious Diseases (except HIV/AIDS)}},
  doi = {10.1101/2020.04.01.20049908},
  url = {http://medrxiv.org/lookup/doi/10.1101/2020.04.01.20049908},
  urldate = {2020-04-12},
  abstract = {Background  Non-pharmaceutical interventions have been implemented to reduce transmission of SARS-CoV-2 in the UK. Projecting the size of an unmitigated epidemic and the potential effect of different control measures has been critical to support evidence-based policymaking during the early stages of the epidemic.  Methods We used a stochastic age-structured transmission model to explore a range of intervention scenarios, including the introduction of school closures, social distancing, shielding of elderly groups, self-isolation of symptomatic cases, and extreme "lockdown"-type restrictions. We simulated different durations of interventions and triggers for introduction, as well as combinations of interventions. For each scenario, we projected estimated new cases over time, patients requiring inpatient and critical care (intensive care unit, ICU) treatment, and deaths. Findings  We found that mitigation measures aimed at reducing transmission would likely have decreased the reproduction number, but not sufficiently to prevent ICU demand from exceeding NHS availability. To keep ICU bed demand below capacity in the model, more extreme restrictions were necessary. In a scenario where "lockdown"-type interventions were put in place to reduce transmission, these interventions would need to be in place for a large proportion of the coming year in order to prevent healthcare demand exceeding availability. Interpretation The characteristics of SARS-CoV-2 mean that extreme measures are likely required to bring the epidemic under control and to prevent very large numbers of deaths and an excess of demand on hospital beds, especially those in ICUs.},
  file = {/mnt/data/Google Drive/Zotero/storage/7SMLCJY4/Davies et al. - 2020 - The effect of non-pharmaceutical interventions on .pdf},
  langid = {english},
  type = {preprint}
}

@article{dawidPresentPositionPotential1984,
  title = {Present {{Position}} and {{Potential Developments}}: {{Some Personal Views Statistical Theory}} the {{Prequential Approach}}},
  shorttitle = {Present {{Position}} and {{Potential Developments}}},
  author = {Dawid, A. P.},
  date = {1984},
  journaltitle = {Journal of the Royal Statistical Society: Series A (General)},
  volume = {147},
  pages = {278--290},
  issn = {2397-2327},
  doi = {10.2307/2981683},
  url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2981683},
  urldate = {2020-08-12},
  abstract = {The prequential approach is founded on the premiss that the purpose of statistical inference is to make sequential probability forecasts for future observations, rather than to express information about parameters. Many traditional parametric concepts, such as consistency and efficiency, prove to have natural counterparts in this formulation, which sheds new light on these and suggests fruitful extensions.},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.2307/2981683},
  file = {/mnt/data/Google Drive/Zotero/storage/PX9RNJBW/Dawid - 1984 - Present Position and Potential Developments Some .pdf;/mnt/data/Google Drive/Zotero/storage/UXRWFPAE/2981683.html},
  keywords = {consistency,efficiency,likelihood,prequential principle,probability forecasting},
  langid = {english},
  number = {2}
}

@article{deasyForecastingUltraearlyIntensive2020,
  title = {Forecasting Ultra-Early Intensive Care Strain from {{COVID}}-19 in {{England}}},
  author = {Deasy, Jacob and Rocheteau, Emma and Kohler, Katharina and Stubbs, Daniel J. and Barbiero, Pietro and Liò, Pietro and Ercole, Ari},
  date = {2020-04-07},
  journaltitle = {medRxiv},
  pages = {2020.03.19.20039057},
  doi = {10.1101/2020.03.19.20039057},
  url = {https://www.medrxiv.org/content/10.1101/2020.03.19.20039057v3},
  urldate = {2020-04-12},
  abstract = {{$<$}p{$>$}The COVID-19 pandemic has led to unprecedented strain on intensive care unit (ICU) admission in parts of the world. Strategies to create surge ICU capacity requires complex local and national service reconfiguration and reduction or cancellation of elective activity. Theses measures require time to implement and have an inevitable lag before additional capacity comes on-line. An accurate short-range forecast would be helpful in guiding such difficult, costly and ethically challenging decisions. At the time this work began, cases in England were starting to increase. Here we present an attempt at an agile short-range forecast based on published real-time COVID-19 case data from the seven National Health Service commissioning regions in England (East of England, London, Midlands, North East and Yorkshire, North West, South East and South West). We use a Monte Carlo approach to model the likely impact of current diagnoses on regional ICU capacity over a 14 day horizon. Our model is designed to be parsimonious and based on plausible epidemiological data from the literature available. On the basis of the modelling assumptions made, ICU occupancy is likely to increase dramatically in the the days following the time of modelling. If the current exponential growth continues, 5 out of 7 commissioning regions will have more critically ill COVID-19 patients than there are ICU beds within two weeks\textbackslash todo\{last thing to do\}. Despite variable growth in absolute patients, all commissioning regions are forecast to be heavily burdened under the assumptions used. Whilst, like any forecast model, there remain uncertainties both in terms of model specification and robust epidemiological data in this early prospective phase, it would seem that surge capacity will be required in the very near future. We hope that our model will help policy decision makers with their preparations. The uncertainties in the data highlight the urgent need for ongoing real-time surveillance to allow forecasts to be constantly updated using high quality local patient-facing data as it emerges.{$<$}/p{$>$}},
  file = {/mnt/data/Google Drive/Zotero/storage/REVI8V8L/Deasy et al. - 2020 - Forecasting ultra-early intensive care strain from.pdf;/mnt/data/Google Drive/Zotero/storage/R3EEKC86/2020.03.19.20039057v3.html},
  langid = {english}
}

@online{DelphiCOVIDcastProject,
  title = {Delphi’s {{COVIDcast Project}}: {{API}} and {{Client Access Tools}} (1)},
  url = {https://cmu-delphi.github.io/covidcast/talks/intro-api/talk.html#(1)},
  urldate = {2020-11-29},
  file = {/mnt/data/Google Drive/Zotero/storage/IXM7JPCB/talk.html}
}

@article{demetrescuBiasCorrectionsExponentially,
  title = {Bias {{Corrections}} for {{Exponentially Transformed Forecasts}}: Is It Worth the Effort?},
  author = {Demetrescu, Matei and Kiel, CAU and Golosnoy, Vasyl and Bochum, Ruhr-University},
  pages = {22},
  abstract = {In many economic applications the log transformation of the process of interest allows to model and to forecast log values as linear time series. However, a reverse transformation of the log forecasts introduces a bias which should accounted for. In this paper we compare different bias correction methods for the reverse transformation of log series following a linear autoregressive process. We find that the correction method to choose in finite samples depends much on the empirical error distribution whereby for some cases no bias correction is advantageous. Our results are illustrated both in Monte Carlo simulations and in an empirical study.},
  file = {/mnt/data/Google Drive/Zotero/storage/Q7WTUXP3/Demetrescu et al. - Bias Corrections for Exponentially Transformed For.pdf},
  langid = {english}
}

@online{DESeq2TestingRatio,
  title = {{{DESeq2}} Testing Ratio of Ratios ({{RIP}}-{{Seq}}, {{CLIP}}-{{Seq}}, Ribosomal Profiling)},
  url = {https://support.bioconductor.org/p/61509/},
  urldate = {2020-02-21},
  file = {/mnt/data/Google Drive/Zotero/storage/JN9LPDBE/61509.html}
}

@online{deutschewellewww.dw.comCoronavirusGermanyImpose,
  title = {Coronavirus: {{Germany}} to Impose One-Month Partial Lockdown | {{DW}} | 28.10.2020},
  shorttitle = {Coronavirus},
  author = {Deutsche Welle (www.dw.com), Deutsche},
  url = {https://www.dw.com/en/coronavirus-germany-to-impose-one-month-partial-lockdown/a-55421241},
  urldate = {2021-06-29},
  abstract = {German Chancellor Angela Merkel has announced tough new measures from Monday, November 2, in an attempt to curb the spread of the coronavirus pandemic. But will the German people be compliant?},
  file = {/mnt/data/Google Drive/Zotero/storage/PZB9PK7Z/a-55421241.html},
  langid = {british}
}

@article{diazStatisticalPostprocessingEnsemble2020,
  title = {Statistical Post-Processing of Ensemble Forecasts of Temperature in {{Santiago}} de {{Chile}}},
  author = {Díaz, Mailiu and Nicolis, Orietta and Marín, Julio César and Baran, Sándor},
  date = {2020},
  journaltitle = {Meteorological Applications},
  volume = {27},
  pages = {e1818},
  issn = {1469-8080},
  doi = {10.1002/met.1818},
  url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/met.1818},
  urldate = {2020-04-08},
  abstract = {Modelling forecast uncertainty is a difficult task in any forecasting problem. In weather forecasting a possible solution is the use of forecast ensembles, which are obtained from multiple runs of numerical weather prediction models with various initial conditions and model parametrizations to provide information about the expected uncertainty. Currently all major meteorological centres issue forecasts using their operational ensemble prediction systems. However, it is a general problem that the spread of the ensemble is too small compared to observations at specific sites resulting in under-dispersive forecasts, leading to a lack of calibration. In order to correct this problem, various statistical calibration techniques have been developed in the last two decades. In the present work different post-processing techniques were tested for calibrating nine member ensemble forecasts of temperature for Santiago de Chile, obtained by the Weather Research and Forecasting model using different planetary boundary layer and land surface model parametrizations. In particular, the ensemble model output statistics and Bayesian model averaging techniques were implemented and, since the observations are characterized by large altitude differences, the estimation of model parameters was adapted to the actual conditions at hand. Compared to the raw ensemble, all tested post-processing approaches significantly improve the calibration of probabilistic forecasts and the accuracy of point forecasts. The ensemble model output statistics method using parameter estimation based on expert clustering of stations (according to their altitudes) shows the best forecast skill.},
  file = {/mnt/data/Google Drive/Zotero/storage/U8R842AZ/Díaz et al. - 2020 - Statistical post-processing of ensemble forecasts .pdf;/mnt/data/Google Drive/Zotero/storage/W8BARAGL/met.html},
  keywords = {Bayesian model averaging,ensemble model output statistics,ensemble post-processing,probabilistic forecasting,temperature forecast},
  langid = {english},
  number = {1}
}

@article{dilliesComprehensiveEvaluationNormalization2013,
  title = {A Comprehensive Evaluation of Normalization Methods for {{Illumina}} High-Throughput {{RNA}} Sequencing Data Analysis},
  author = {Dillies, M.-A. and Rau, A. and Aubert, J. and Hennequet-Antier, C. and Jeanmougin, M. and Servant, N. and Keime, C. and Marot, G. and Castel, D. and Estelle, J. and Guernec, G. and Jagla, B. and Jouneau, L. and Laloe, D. and Le Gall, C. and Schaeffer, B. and Le Crom, S. and Guedj, M. and Jaffrezic, F. and {on behalf of The French StatOmique Consortium}},
  date = {2013-11-01},
  journaltitle = {Briefings in Bioinformatics},
  volume = {14},
  pages = {671--683},
  issn = {1467-5463, 1477-4054},
  doi = {10.1093/bib/bbs046},
  url = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbs046},
  urldate = {2018-12-01},
  abstract = {During the last 3 years, a number of approaches for the normalization of RNA sequencing data have emerged in the literature, differing both in the type of bias adjustment and in the statistical strategy adopted. However, as data continue to accumulate, there has been no clear consensus on the appropriate normalization method to be used or the impact of a chosen method on the downstream analysis. In this work, we focus on a comprehensive comparison of seven recently proposed normalization methods for the differential analysis of RNA-seq data, with an emphasis on the use of varied real and simulated datasets involving different species and experimental designs to represent data characteristics commonly observed in practice. Based on this comparison study, we propose practical recommendations on the appropriate normalization method to be used and its impact on the differential analysis of RNA-seq data.},
  file = {/mnt/data/Google Drive/Zotero/storage/XAHDLGIN/Dillies et al. - 2013 - A comprehensive evaluation of normalization method.pdf},
  langid = {english},
  number = {6}
}

@article{dilmaghaniGenderGapCompetitive2021,
  title = {The Gender Gap in Competitive Chess across Countries: {{Commanding}} Queens in Command Economies},
  shorttitle = {The Gender Gap in Competitive Chess across Countries},
  author = {Dilmaghani, Maryam},
  date = {2021-06-01},
  journaltitle = {Journal of Comparative Economics},
  shortjournal = {Journal of Comparative Economics},
  volume = {49},
  pages = {425--441},
  issn = {0147-5967},
  doi = {10.1016/j.jce.2020.09.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0147596720300688},
  urldate = {2021-06-04},
  abstract = {Currently, the World Chess Federation lists 1,643 male Grandmasters against only 37 females. While the greater attainment of men in competitive chess is well known, the lesser known fact is that the gender gap in competitive chess varies strongly across countries. For instance, for every 100 male players with the attainment level of Candidate Master, there are about 48, 47, 38, 4, 3, and 2 female players in Vietnam, Georgia, China, US (Japan), France (Sweden), and Denmark (Finland), respectively. Noting these large gender discrepancies, this paper constructs a cross-country panel to explore the determinants of these gaps. Controlling for main economic development indicators and several measures of gender equality, a legacy of command economy is found to be the most significant predictor of a smaller gender gap in competitive chess across countries. Various explanations and their implications are discussed.},
  file = {/mnt/data/Google Drive/Zotero/storage/YQI7QHTM/S0147596720300688.html},
  keywords = {Competitiveness,Gender,Gender Role Attitudes,Political Regimes,Socialism},
  langid = {english},
  number = {2}
}

@online{DoesVolatilitySpillover,
  title = {Does Volatility Spillover among Stock Markets Varies from Normal to Turbulent Periods? {{Evidence}} from Emerging Markets of {{Asia}} | {{Elsevier Enhanced Reader}}},
  shorttitle = {Does Volatility Spillover among Stock Markets Varies from Normal to Turbulent Periods?},
  doi = {10.1016/j.jfds.2017.06.001},
  url = {https://reader.elsevier.com/reader/sd/pii/S2405918816300460?token=77145D57F9055ED9A36C71F8C94856863CFB047BE05FBAA3C23B5B16A0A5BD7036680A0E48325643BD7AF835C0631943},
  urldate = {2019-06-19},
  file = {/mnt/data/Google Drive/Zotero/storage/ZXVTA2XL/S2405918816300460.html},
  langid = {english}
}

@online{doiCovid19TemporalVariation,
  title = {Covid-19: {{Temporal}} Variation in Transmission during the {{COVID}}-19 Outbreak},
  shorttitle = {Covid-19},
  author = {published yet DOI, Authors Affiliations Published Not},
  url = {https://epiforecasts.io/covid/},
  urldate = {2021-05-30},
  file = {/mnt/data/Google Drive/Zotero/storage/VC3R2A6X/covid.html},
  organization = {{Covid-19}}
}

@incollection{DoingBayesianData2015,
  title = {Doing {{Bayesian Data Analysis}} - {{Kruschke}}},
  booktitle = {Doing {{Bayesian Data Analysis}}},
  date = {2015},
  pages = {i-ii},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-405888-0.09999-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780124058880099992},
  urldate = {2019-04-15},
  file = {/mnt/data/Google Drive/Zotero/storage/PCMMKEAK/2015 - Front Matter.pdf},
  isbn = {978-0-12-405888-0},
  langid = {english}
}

@article{dominissiniTopologyHumanMouse2012,
  title = {Topology of the Human and Mouse {{m6A RNA}} Methylomes Revealed by {{m6A}}-Seq},
  author = {Dominissini, Dan and Moshitch-Moshkovitz, Sharon and Schwartz, Schraga and Salmon-Divon, Mali and Ungar, Lior and Osenberg, Sivan and Cesarkas, Karen and Jacob-Hirsch, Jasmine and Amariglio, Ninette and Kupiec, Martin and Sorek, Rotem and Rechavi, Gideon},
  date = {2012-04-29},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {485},
  pages = {201--206},
  issn = {1476-4687},
  doi = {10.1038/nature11112},
  abstract = {An extensive repertoire of modifications is known to underlie the versatile coding, structural and catalytic functions of RNA, but it remains largely uncharted territory. Although biochemical studies indicate that N(6)-methyladenosine (m(6)A) is the most prevalent internal modification in messenger RNA, an in-depth study of its distribution and functions has been impeded by a lack of robust analytical methods. Here we present the human and mouse m(6)A modification landscape in a transcriptome-wide manner, using a novel approach, m(6)A-seq, based on antibody-mediated capture and massively parallel sequencing. We identify over 12,000 m(6)A sites characterized by a typical consensus in the transcripts of more than 7,000 human genes. Sites preferentially appear in two distinct landmarks--around stop codons and within long internal exons--and are highly conserved between human and mouse. Although most sites are well preserved across normal and cancerous tissues and in response to various stimuli, a subset of stimulus-dependent, dynamically modulated sites is identified. Silencing the m(6)A methyltransferase significantly affects gene expression and alternative splicing patterns, resulting in modulation of the p53 (also known as TP53) signalling pathway and apoptosis. Our findings therefore suggest that RNA decoration by m(6)A has a fundamental role in regulation of gene expression.},
  eprint = {22575960},
  eprinttype = {pmid},
  keywords = {Adenosine,Alternative Splicing,Animals,Base Sequence,Cell Line; Tumor,Conserved Sequence,Evolution; Molecular,Hep G2 Cells,Humans,Immunoprecipitation,Metabolome,Methoden,Methylation,Methyltransferases,Mice,RNA,RNA-Binding Proteins,RNA; Ribosomal,RNA; Transfer,Transcriptome},
  langid = {english},
  number = {7397}
}

@article{dominissiniTranscriptomewideMappingMethyladenosine2013,
  title = {Transcriptome-Wide Mapping of {{N}}(6)-Methyladenosine by m(6){{A}}-Seq Based on Immunocapturing and Massively Parallel Sequencing},
  author = {Dominissini, Dan and Moshitch-Moshkovitz, Sharon and Salmon-Divon, Mali and Amariglio, Ninette and Rechavi, Gideon},
  date = {2013-01},
  journaltitle = {Nature Protocols},
  shortjournal = {Nat Protoc},
  volume = {8},
  pages = {176--189},
  issn = {1750-2799},
  doi = {10.1038/nprot.2012.148},
  abstract = {N(6)-methyladenosine-sequencing (m(6)A-seq) is an immunocapturing approach for the unbiased transcriptome-wide localization of m(6)A in high resolution. To our knowledge, this is the first protocol to allow a global view of this ubiquitous RNA modification, and it is based on antibody-mediated enrichment of methylated RNA fragments followed by massively parallel sequencing. Building on principles of chromatin immunoprecipitation-sequencing (ChIP-seq) and methylated DNA immunoprecipitation (MeDIP), read densities of immunoprecipitated RNA relative to untreated input control are used to identify methylated sites. A consensus motif is deduced, and its distance to the point of maximal enrichment is assessed; these measures further corroborate the success of the protocol. Identified locations are intersected in turn with gene architecture to draw conclusions regarding the distribution of m(6)A between and within gene transcripts. When applied to human and mouse transcriptomes, m(6)A-seq generated comprehensive methylation profiles revealing, for the first time, tenets governing the nonrandom distribution of m(6)A. The protocol can be completed within \textasciitilde 9 d for four different sample pairs (each consists of an immunoprecipitation and corresponding input).},
  eprint = {23288318},
  eprinttype = {pmid},
  keywords = {Adenosine,Animals,Gene Expression Profiling,Humans,Immunoprecipitation,Methylation,Mice,RNA Processing; Post-Transcriptional},
  langid = {english},
  number = {1}
}

@article{doNewApproachModeling2006,
  title = {A {{New Approach}} to {{Modeling}} and {{Estimation}} for {{Pairs Trading}}},
  author = {Do, Binh and Faff, Robert and Hamza, Kais},
  date = {2006},
  journaltitle = {2006},
  pages = {31},
  abstract = {Pairs trading is an speculative investment strategy based on relative mispricing between a pair of stocks. Essentially, the strategy involves choosing a pair of stocks that historically move together. By taking a long-short position on this pair when they diverge, a profit will be made when they next converge to the mean by unwinding the position. Literature on this topic is rare due to its proprietary nature. Where it does exist, the strategies are either adhoc or applicable to special cases only, with little theoretical verification. This paper analyzes these existing methods in detail and proposes a general approach to modeling relative mispricing for pairs trading purposes, with reference to the mainstream asset pricing theory. Several estimation techniques are discussed and tested for state space formulation, with Expectation Maximization producing stable results. Initial empirical evidence shows clear mean reversion behavior in selected pairs’ relative pricing.},
  file = {/mnt/data/Google Drive/Zotero/storage/KE55LP3V/Do et al. - A New Approach to Modeling and Estimation for Pair.pdf},
  langid = {english}
}

@article{dongInteractiveWebbasedDashboard2020,
  title = {An Interactive Web-Based Dashboard to Track {{COVID}}-19 in Real Time},
  author = {Dong, Ensheng and Du, Hongru and Gardner, Lauren},
  date = {2020-05-01},
  journaltitle = {The Lancet Infectious Diseases},
  shortjournal = {The Lancet Infectious Diseases},
  volume = {20},
  pages = {533--534},
  publisher = {{Elsevier}},
  issn = {1473-3099, 1474-4457},
  doi = {10.1016/S1473-3099(20)30120-1},
  url = {https://www.thelancet.com/journals/laninf/article/PIIS1473-3099(20)30120-1/abstract},
  urldate = {2020-08-07},
  abstract = {In December, 2019, a local outbreak of pneumonia of initially unknown cause was detected in Wuhan (Hubei, China), and was quickly determined to be caused by a novel coronavirus,1 namely severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The outbreak has since spread to every province of mainland China as well as 27 other countries and regions, with more than 70\hphantom{,}000 confirmed cases as of Feb 17, 2020.2 In response to this ongoing public health emergency, we developed an online interactive dashboard, hosted by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University, Baltimore, MD, USA, to visualise and track reported cases of coronavirus disease 2019 (COVID-19) in real time.},
  eprint = {32087114},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/9YPUUGK6/Dong et al. - 2020 - An interactive web-based dashboard to track COVID-.pdf;/mnt/data/Google Drive/Zotero/storage/2SPI59IE/fulltext.html},
  langid = {english},
  number = {5}
}

@online{donnatModelingHeterogeneityCOVID192020,
  title = {Modeling the {{Heterogeneity}} in {{COVID}}-19's {{Reproductive Number}} and Its {{Impact}} on {{Predictive Scenarios}}},
  author = {Donnat, Claire and Holmes, Susan},
  date = {2020-04-10},
  url = {http://arxiv.org/abs/2004.05272},
  urldate = {2020-05-19},
  abstract = {The current COVID-19 pandemic is leading experts to assess the risks posed by the disease and compare policies geared towards stalling its evolution as a global pandemic. In this setting, the virus' basic reproductive number R\_0, which characterizes the average number of secondary cases generated by each primary case, takes on a significant importance in the quantification of the potential scope of the pandemic. Yet, in most models, R\_0 is assumed to be a universal constant for the virus across outbreak clusters and populations -- thus neglecting the inherent variability of the transmission process due to varying population densities, demographics, temporal factors, etc. In fact, it can be shown that the reproduction number is highly variable. Considering its expected value thus leads to biased or loose results in the reported predictive scenarios, especially as these are tailored to a given country or region. The goal of this paper is the examination of the impact of the reproductive number R's variability on important output metrics, and the percolation of this variability in projected scenarios so as to provide uncertainty quantification. In this perspective, instead of considering a single R\_0, we consider a distribution of reproductive numbers R and devise a simple Bayesian hierarchical model that builds upon current methods for estimating the R to integrate its heterogeneity. We then simulate the spread of the epidemic, and the impact of different social distancing strategies using a probabilistic framework that models hospital occupancy. This shows the strong impact of this added variability on the reported results. We emphasize that our goal is not to replace benchmark methods for estimating the basic reproductive numbers, but rather to discuss the importance of the impact of R's heterogeneity on uncertainty quantification for the current COVID-19 pandemic.},
  archiveprefix = {arXiv},
  eprint = {2004.05272},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/J5RVUD26/Donnat and Holmes - 2020 - Modeling the Heterogeneity in COVID-19's Reproduct.pdf},
  keywords = {Quantitative Biology - Populations and Evolution,Statistics - Applications},
  langid = {english},
  primaryclass = {q-bio, stat}
}

@online{donnatModelingHeterogeneityCOVID192020a,
  title = {Modeling the {{Heterogeneity}} in {{COVID}}-19's {{Reproductive Number}} and Its {{Impact}} on {{Predictive Scenarios}}},
  author = {Donnat, Claire and Holmes, Susan},
  date = {2020-04-10},
  url = {http://arxiv.org/abs/2004.05272},
  urldate = {2020-05-25},
  abstract = {The current COVID-19 pandemic is leading experts to assess the risks posed by the disease and compare policies geared towards stalling its evolution as a global pandemic. In this setting, the virus' basic reproductive number R\_0, which characterizes the average number of secondary cases generated by each primary case, takes on a significant importance in the quantification of the potential scope of the pandemic. Yet, in most models, R\_0 is assumed to be a universal constant for the virus across outbreak clusters and populations -- thus neglecting the inherent variability of the transmission process due to varying population densities, demographics, temporal factors, etc. In fact, it can be shown that the reproduction number is highly variable. Considering its expected value thus leads to biased or loose results in the reported predictive scenarios, especially as these are tailored to a given country or region. The goal of this paper is the examination of the impact of the reproductive number R's variability on important output metrics, and the percolation of this variability in projected scenarios so as to provide uncertainty quantification. In this perspective, instead of considering a single R\_0, we consider a distribution of reproductive numbers R and devise a simple Bayesian hierarchical model that builds upon current methods for estimating the R to integrate its heterogeneity. We then simulate the spread of the epidemic, and the impact of different social distancing strategies using a probabilistic framework that models hospital occupancy. This shows the strong impact of this added variability on the reported results. We emphasize that our goal is not to replace benchmark methods for estimating the basic reproductive numbers, but rather to discuss the importance of the impact of R's heterogeneity on uncertainty quantification for the current COVID-19 pandemic.},
  archiveprefix = {arXiv},
  eprint = {2004.05272},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/VWXSMSET/Donnat and Holmes - 2020 - Modeling the Heterogeneity in COVID-19's Reproduct.pdf},
  keywords = {Quantitative Biology - Populations and Evolution,Statistics - Applications},
  langid = {english},
  primaryclass = {q-bio, stat}
}

@online{donnatModelingHeterogeneityCOVID192020b,
  title = {Modeling the {{Heterogeneity}} in {{COVID}}-19's {{Reproductive Number}} and Its {{Impact}} on {{Predictive Scenarios}}},
  author = {Donnat, Claire and Holmes, Susan},
  date = {2020-04-10},
  url = {http://arxiv.org/abs/2004.05272},
  urldate = {2020-05-25},
  abstract = {The current COVID-19 pandemic is leading experts to assess the risks posed by the disease and compare policies geared towards stalling its evolution as a global pandemic. In this setting, the virus' basic reproductive number R\_0, which characterizes the average number of secondary cases generated by each primary case, takes on a significant importance in the quantification of the potential scope of the pandemic. Yet, in most models, R\_0 is assumed to be a universal constant for the virus across outbreak clusters and populations -- thus neglecting the inherent variability of the transmission process due to varying population densities, demographics, temporal factors, etc. In fact, it can be shown that the reproduction number is highly variable. Considering its expected value thus leads to biased or loose results in the reported predictive scenarios, especially as these are tailored to a given country or region. The goal of this paper is the examination of the impact of the reproductive number R's variability on important output metrics, and the percolation of this variability in projected scenarios so as to provide uncertainty quantification. In this perspective, instead of considering a single R\_0, we consider a distribution of reproductive numbers R and devise a simple Bayesian hierarchical model that builds upon current methods for estimating the R to integrate its heterogeneity. We then simulate the spread of the epidemic, and the impact of different social distancing strategies using a probabilistic framework that models hospital occupancy. This shows the strong impact of this added variability on the reported results. We emphasize that our goal is not to replace benchmark methods for estimating the basic reproductive numbers, but rather to discuss the importance of the impact of R's heterogeneity on uncertainty quantification for the current COVID-19 pandemic.},
  archiveprefix = {arXiv},
  eprint = {2004.05272},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/KUST69GH/Donnat and Holmes - 2020 - Modeling the Heterogeneity in COVID-19's Reproduct.pdf},
  keywords = {Quantitative Biology - Populations and Evolution,Statistics - Applications},
  langid = {english},
  primaryclass = {q-bio, stat}
}

@online{DownloadHistoricalData2020a,
  title = {Download Historical Data (to 14 {{December}} 2020) on the Daily Number of New Reported {{COVID}}-19 Cases and Deaths Worldwide},
  date = {2020-12-14T13:00:00+0100},
  url = {https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide},
  urldate = {2021-05-30},
  abstract = {The downloadable data file was updated daily to 14 December 2020 using the latest available public data on COVID-19. You may use the data in line with ECDC’s copyright policy.},
  file = {/mnt/data/Google Drive/Zotero/storage/F4F3UBWA/download-todays-data-geographic-distribution-covid-19-cases-worldwide.html},
  langid = {english},
  organization = {{European Centre for Disease Prevention and Control}}
}

@online{DownloadRStudio,
  title = {Download {{RStudio}}},
  url = {https://rstudio.com/products/rstudio/download/},
  urldate = {2020-03-29},
  abstract = {RStudio is a set of integrated tools designed to help you be more productive with R. It includes a console, syntax-highlighting editor that supports direct code execution, and a variety of robust tools for plotting, viewing history, debugging and managing your workspace.},
  file = {/mnt/data/Google Drive/Zotero/storage/DJTUDCCL/download.html},
  langid = {english}
}

@article{dundarIntroductionDifferentialGene,
  title = {Introduction to Differential Gene Expression Analysis Using {{RNA}}-Seq},
  author = {Dündar, Written Friederike and Skrabanek, Luce and Zumbo, Paul},
  pages = {89},
  file = {/mnt/data/Google Drive/Zotero/storage/FD253CIE/Dündar et al. - Introduction to diﬀerential gene expression analys.pdf},
  langid = {english}
}

@online{EbolaCasesDeaths,
  title = {Ebola {{Cases}} and {{Deaths}} in the {{North Kivu Ebola Outbreak}} in the {{Democratic Republic}} of the {{Congo}} ({{DRC}}) - {{Humanitarian Data Exchange}}},
  url = {https://data.humdata.org/dataset/ebola-cases-and-deaths-drc-north-kivu},
  urldate = {2020-01-13},
  file = {/mnt/data/Google Drive/Zotero/storage/TBIT49SJ/ebola-cases-and-deaths-drc-north-kivu.html}
}

@article{ehlertSupervisedDrThomas,
  title = {Supervised by {{Dr}}. {{Thomas Dimpﬂ}}},
  author = {Ehlert, Jan},
  pages = {73},
  file = {/mnt/data/Google Drive/Zotero/storage/TIGQ5S6C/Ehlert - supervised by Dr. Thomas Dimpﬂ.pdf},
  langid = {english}
}

@report{engleAssetPricingFactor1988,
  title = {Asset {{Pricing}} with a {{Factor Arch Covariance Structure}}: {{Empirical Estimates}} for {{Treasury Bills}}},
  shorttitle = {Asset {{Pricing}} with a {{Factor Arch Covariance Structure}}},
  author = {Engle, Robert and Ng, Victor and Rothschild, Michael},
  date = {1988-11},
  pages = {t0065},
  institution = {{National Bureau of Economic Research}},
  location = {{Cambridge, MA}},
  doi = {10.3386/t0065},
  url = {http://www.nber.org/papers/t0065.pdf},
  urldate = {2019-06-19},
  file = {/mnt/data/Google Drive/Zotero/storage/PC5TWX6C/Engle et al. - 1988 - Asset Pricing with a Factor Arch Covariance Struct.pdf},
  langid = {english},
  number = {t0065}
}

@article{engleAutoregressiveConditionalHeteroscedasticity1982,
  title = {Autoregressive {{Conditional Heteroscedasticity}} with {{Estimates}} of the {{Variance}} of {{United Kingdom Inflation}}},
  author = {Engle, Robert F.},
  date = {1982},
  journaltitle = {Econometrica},
  volume = {50},
  pages = {987--1007},
  issn = {0012-9682},
  doi = {10.2307/1912773},
  abstract = {Traditional econometric models assume a constant one-period forecast variance. To generalize this implausible assumption, a new class of stochastic processes called autoregressive conditional heteroscedastic (ARCH) processes are introduced in this paper. These are mean zero, serially uncorrelated processes with nonconstant variances conditional on the past, but constant unconditional variances. For such processes, the recent past gives information about the one-period forecast variance. A regression model is then introduced with disturbances following an ARCH process. Maximum likelihood estimators are described and a simple scoring iteration formulated. Ordinary least squares maintains its optimality properties in this set-up, but maximum likelihood is more efficient. The relative efficiency is calculated and can be infinite. To test whether the disturbances follow an ARCH process, the Lagrange multiplier procedure is employed. The test is based simply on the autocorrelation of the squared OLS residuals. This model is used to estimate the means and variances of inflation in the U.K. The ARCH effect is found to be significant and the estimated variances increase substantially during the chaotic seventies.},
  eprint = {1912773},
  eprinttype = {jstor},
  number = {4}
}

@article{engleAutoregressiveConditionalHeteroscedasticity1982a,
  title = {Autoregressive {{Conditional Heteroscedasticity}} with {{Estimates}} of the {{Variance}} of {{United Kingdom Inflation}}},
  author = {Engle, Robert F.},
  date = {1982-07},
  journaltitle = {Econometrica},
  shortjournal = {Econometrica},
  volume = {50},
  pages = {987},
  issn = {00129682},
  doi = {10.2307/1912773},
  eprint = {1912773},
  eprinttype = {jstor},
  file = {/mnt/data/Google Drive/Zotero/storage/MHIXEV2L/Engle - 1982 - Autoregressive Conditional Heteroscedasticity with.pdf},
  langid = {english},
  number = {4}
}

@article{engleCoIntegrationErrorCorrection1987,
  title = {Co-{{Integration}} and {{Error Correction}}: {{Representation}}, {{Estimation}}, and {{Testing}}},
  shorttitle = {Co-{{Integration}} and {{Error Correction}}},
  author = {Engle, Robert F. and Granger, C. W. J.},
  date = {1987},
  journaltitle = {Econometrica},
  volume = {55},
  pages = {251--276},
  issn = {0012-9682},
  doi = {10.2307/1913236},
  abstract = {The relationship between co-integration and error correction models, first suggested in Granger (1981), is here extended and used to develop estimation procedures, tests, and empirical examples. If each element of a vector of time series x\textsubscript{t} first achieves stationarity after differencing, but a linear combination {$<$}tex-math{$>\$\backslash$}alpha \^\{\textbackslash prime \}x\_\{t\}\${$<$}/tex-math{$>$} is already stationary, the time series x\textsubscript{t} are said to be co-integrated with co-integrating vector α. There may be several such co-integrating vectors so that α becomes a matrix. Interpreting {$<$}tex-math{$>\$\backslash$}alpha \^\{\textbackslash prime \}x\_\{t\}=0\${$<$}/tex-math{$>$} as a long run equilibrium, co-integration implies that deviations from equilibrium are stationary, with finite variance, even though the series themselves are nonstationary and have infinite variance. The paper presents a representation theorem based on Granger (1983), which connects the moving average, autoregressive, and error correction representations for co-integrated systems. A vector autoregression in differenced variables is incompatible with these representations. Estimation of these models is discussed and a simple but asymptotically efficient two-step estimator is proposed. Testing for co-integration combines the problems of unit root tests and tests with parameters unidentified under the null. Seven statistics are formulated and analyzed. The critical values of these statistics are calculated based on a Monte Carlo simulation. Using these critical values, the power properties of the tests are examined and one test procedure is recommended for application. In a series of examples it is found that consumption and income are co-integrated, wages and prices are not, short and long interest rates are, and nominal GNP is co-integrated with M2, but not M1, M3, or aggregate liquid assets.},
  eprint = {1913236},
  eprinttype = {jstor},
  number = {2}
}

@article{engleGARCH101Use2001,
  title = {{{GARCH}} 101: {{The Use}} of {{ARCH}}/{{GARCH Models}} in {{Applied Econometrics}}},
  shorttitle = {{{GARCH}} 101},
  author = {Engle, Robert},
  date = {2001-11},
  journaltitle = {Journal of Economic Perspectives},
  shortjournal = {Journal of Economic Perspectives},
  volume = {15},
  pages = {157--168},
  issn = {0895-3309},
  doi = {10.1257/jep.15.4.157},
  url = {http://pubs.aeaweb.org/doi/10.1257/jep.15.4.157},
  urldate = {2019-09-13},
  file = {/mnt/data/Google Drive/Zotero/storage/EZFXGULV/Engle - 2001 - GARCH 101 The Use of ARCHGARCH Models in Applied.pdf},
  langid = {english},
  number = {4}
}

@article{epsteinScoringSystemProbability1969,
  title = {A {{Scoring System}} for {{Probability Forecasts}} of {{Ranked Categories}}},
  author = {Epstein, Edward S.},
  date = {1969-12-01},
  journaltitle = {Journal of Applied Meteorology},
  shortjournal = {J. Appl. Meteor.},
  volume = {8},
  pages = {985--987},
  publisher = {{American Meteorological Society}},
  issn = {0021-8952},
  doi = {10.1175/1520-0450(1969)008<0985:ASSFPF>2.0.CO;2},
  url = {https://journals.ametsoc.org/jamc/article/8/6/985/352208/A-Scoring-System-for-Probability-Forecasts-of},
  urldate = {2020-08-13},
  file = {/mnt/data/Google Drive/Zotero/storage/XAVX39GC/Epstein - 1969 - A Scoring System for Probability Forecasts of Rank.pdf;/mnt/data/Google Drive/Zotero/storage/CVK2YPKP/A-Scoring-System-for-Probability-Forecasts-of.html},
  keywords = {ranked probability score,RPS},
  langid = {english},
  number = {6}
}

@article{eraslanSinglecellRNAseqDenoising2019,
  title = {Single-Cell {{RNA}}-Seq Denoising Using a Deep Count Autoencoder},
  author = {Eraslan, Gökcen and Simon, Lukas M. and Mircea, Maria and Mueller, Nikola S. and Theis, Fabian J.},
  date = {2019-01-23},
  journaltitle = {Nature Communications},
  volume = {10},
  pages = {390},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-07931-2},
  url = {https://www.nature.com/articles/s41467-018-07931-2},
  urldate = {2019-04-26},
  abstract = {Single-cell RNA sequencing is a powerful method to study gene expression, but noise in the data can obstruct analysis. Here the authors develop a denoising method based on a deep count autoencoder network that scales linearly with the number of cells, and therefore is compatible with large data sets.},
  file = {/mnt/data/Google Drive/Zotero/storage/SFRPXZPB/Eraslan et al. - 2019 - Single-cell RNA-seq denoising using a deep count a.pdf;/mnt/data/Google Drive/Zotero/storage/7MN7STMU/s41467-018-07931-2.html},
  langid = {english},
  number = {1}
}

@online{EstimationContinuousRanked,
  title = {Estimation of the {{Continuous Ranked Probability Score}} with {{Limited Information}} and {{Applications}} to {{Ensemble Weather Forecasts}} | {{SpringerLink}}},
  url = {https://link.springer.com/article/10.1007/s11004-017-9709-7},
  urldate = {2020-04-02},
  file = {/mnt/data/Google Drive/Zotero/storage/4XESHFER/s11004-017-9709-7.html}
}

@online{EuropeanCovid19Forecast,
  title = {European {{Covid}}-19 {{Forecast Hub}}},
  url = {https://covid19forecasthub.eu/},
  urldate = {2021-05-30},
  file = {/mnt/data/Google Drive/Zotero/storage/JRFUHRDI/covid19forecasthub.eu.html}
}

@online{EvaluatingUseReproduction,
  title = {Evaluating the Use of the Reproduction Number as an Epidemiological Tool, Using Spatio-Temporal Trends of the {{Covid}}-19 Outbreak in {{England}} | {{medRxiv}}},
  url = {https://www.medrxiv.org/content/10.1101/2020.10.18.20214585v1},
  urldate = {2021-05-30},
  file = {/mnt/data/Google Drive/Zotero/storage/VC2FV68A/2020.10.18.html}
}

@online{evanl.rayChallengesTrainingEnsembles,
  title = {Challenges in Training Ensembles to Forecast {{COVID}}-19 Cases and Deaths in the {{United States}} - {{International Institute}} of {{Forecasters}}},
  author = {{Evan L. Ray} and {Logan C. Brooks} and {Jacob Bien} and {Johannes Bracher} and {Aaron Gerding} and {Aaron Rumack} and {Matthew Biggerstaff} and {Michael A. Johansson} and {Ryan J. Tibshirani} and {Nicholas G. Reich}},
  url = {https://forecasters.org/blog/2021/04/09/challenges-in-training-ensembles-to-forecast-covid-19-cases-and-deaths-in-the-united-states/},
  urldate = {2021-07-12},
  file = {/mnt/data/Google Drive/Zotero/storage/LQVPJLRR/challenges-in-training-ensembles-to-forecast-covid-19-cases-and-deaths-in-the-united-states.html},
  langid = {american}
}

@online{ExaminingMeanvolatilitySpillovers,
  title = {Examining Mean-Volatility Spillovers across National Stock Markets | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.jefas.2014.01.001},
  url = {https://reader.elsevier.com/reader/sd/pii/S207718861400002X?token=6DA00691A761E41EB35C21F376C3C3281321CD1F69F90989FCA29E289EFBB8FE37B90212E853B2C264E39B775C329237},
  urldate = {2019-06-19},
  file = {/mnt/data/Google Drive/Zotero/storage/GC4N5CAA/Examining mean-volatility spillovers across nation.pdf;/mnt/data/Google Drive/Zotero/storage/PYEJW9LM/S207718861400002X.html},
  langid = {english}
}

@book{fahrmeirRegressionModelsMethods2013,
  title = {Regression: Models, Methods and Applications},
  shorttitle = {Regression},
  author = {Fahrmeir, Ludwig and {Kneib, Thomas}},
  date = {2013},
  publisher = {{Springer}},
  location = {{New York}},
  file = {/mnt/data/Google Drive/Zotero/storage/UDRKAU26/Fahrmeir - 2013 - Regression models, methods and applications.pdf},
  isbn = {978-3-642-34332-2},
  langid = {english}
}

@article{famaCommonRiskFactors1993,
  title = {Common Risk Factors in the Returns on Stocks and Bonds},
  author = {Fama, Eugene F. and French, Kenneth R.},
  date = {1993-02-01},
  journaltitle = {Journal of Financial Economics},
  shortjournal = {Journal of Financial Economics},
  volume = {33},
  pages = {3--56},
  issn = {0304-405X},
  doi = {10.1016/0304-405X(93)90023-5},
  url = {http://www.sciencedirect.com/science/article/pii/0304405X93900235},
  urldate = {2019-09-14},
  abstract = {This paper identifies five common risk factors in the returns on stocks and bonds. There are three stock-market factors: an overall market factor and factors related to firm size and book-to-market equity. There are two bond-market factors, related to maturity and default risks. Stock returns have shared variation due to the stock-market factors, and they are linked to bond returns through shared variation in the bond-market factors. Except for low-grade corporates, the bond-market factors capture the common variation in bond returns. Most important, the five factors seem to explain average returns on stocks and bonds.},
  file = {/mnt/data/Google Drive/Zotero/storage/ZZ9HLL7W/Fama and French - 1993 - Common risk factors in the returns on stocks and b.pdf;/mnt/data/Google Drive/Zotero/storage/6233MJJ5/0304405X93900235.html},
  number = {1}
}

@article{famaCrossSectionExpectedStock1992,
  title = {The {{Cross}}-{{Section}} of {{Expected Stock Returns}}},
  author = {Fama, Eugene F. and French, Kenneth R.},
  date = {1992},
  journaltitle = {The Journal of Finance},
  volume = {47},
  pages = {427--465},
  issn = {1540-6261},
  doi = {10.1111/j.1540-6261.1992.tb04398.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1992.tb04398.x},
  urldate = {2019-08-26},
  abstract = {Two easily measured variables, size and book-to-market equity, combine to capture the cross-sectional variation in average stock returns associated with market β, size, leverage, book-to-market equity, and earnings-price ratios. Moreover, when the tests allow for variation in β that is unrelated to size, the relation between market β and average return is flat, even when β is the only explanatory variable.},
  file = {/mnt/data/Google Drive/Zotero/storage/B3MVEEPL/Fama and French - 1992 - The Cross-Section of Expected Stock Returns.pdf;/mnt/data/Google Drive/Zotero/storage/46WM9BFL/j.1540-6261.1992.tb04398.html},
  langid = {english},
  number = {2}
}

@article{fanCharacterizingTranscriptionalHeterogeneity2016,
  title = {Characterizing Transcriptional Heterogeneity through Pathway and Gene Set Overdispersion Analysis},
  author = {Fan, Jean and Salathia, Neeraj and Liu, Rui and Kaeser, Gwendolyn E. and Yung, Yun C. and Herman, Joseph L. and Kaper, Fiona and Fan, Jian-Bing and Zhang, Kun and Chun, Jerold and Kharchenko, Peter V.},
  date = {2016-03},
  journaltitle = {Nature Methods},
  volume = {13},
  pages = {241--244},
  issn = {1548-7105},
  doi = {10.1038/nmeth.3734},
  url = {https://www.nature.com/articles/nmeth.3734},
  urldate = {2019-05-03},
  abstract = {The transcriptional state of a cell reflects a variety of biological factors, from cell-type-specific features to transient processes such as the cell cycle, all of which may be of interest. However, identifying such aspects from noisy single-cell RNA-seq data remains challenging. We developed pathway and gene set overdispersion analysis (PAGODA) to resolve multiple, potentially overlapping aspects of transcriptional heterogeneity by testing gene sets for coordinated variability among measured cells.},
  file = {/mnt/data/Google Drive/Zotero/storage/VA49TQ3K/Fan et al. - 2016 - Characterizing transcriptional heterogeneity throu.pdf;/mnt/data/Google Drive/Zotero/storage/J79YC3R4/nmeth.html},
  langid = {english},
  number = {3}
}

@article{fengIdentifyingChIPseqEnrichment2012,
  title = {Identifying {{ChIP}}-Seq Enrichment Using {{MACS}}},
  author = {Feng, Jianxing and Liu, Tao and Qin, Bo and Zhang, Yong and Liu, Xiaole Shirley},
  date = {2012-09},
  journaltitle = {Nature protocols},
  shortjournal = {Nat Protoc},
  volume = {7},
  issn = {1754-2189},
  doi = {10.1038/nprot.2012.101},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3868217/},
  urldate = {2018-12-03},
  abstract = {Model-based Analysis of ChIP-seq (MACS) is a computational algorithm that identifies genome-wide locations of transcription/chromatin factor binding or histone modification from ChIP-seq data. MACS consists of four steps: removing redundant reads, adjusting read position, calculating peak enrichment, and estimating the empirical false discovery rate. In this protocol, we provide a detailed demonstration of how to install MACS and how to use it to analyze three common types of ChIP-seq datasets with different characteristics: the sequence-specific transcription factor FoxA1, the histone modification mark H3K4me3 with sharp enrichment, and the H3K36me3 mark with broad enrichment. We also explain how to interpret and visualize the results of MACS analyses. The algorithm requires approximately 3 GB of RAM and 1.5 hours of computing time to analyze a ChIP-seq dataset containing 30 million reads, an estimate that increases with sequence coverage. MACS is open-source and is available from http://liulab.dfci.harvard.edu/MACS.},
  eprint = {22936215},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/2XB2N2JN/Feng et al. - 2012 - Identifying ChIP-seq enrichment using MACS.pdf},
  number = {9},
  pmcid = {PMC3868217}
}

@article{fergusonImpactNonpharmaceuticalInterventions2020,
  title = {Impact of Non-Pharmaceutical Interventions ({{NPIs}}) to Reduce {{COVID}}- 19 Mortality and Healthcare Demand},
  author = {Ferguson, Neil M and Laydon, Daniel and Nedjati-Gilani, Gemma and Imai, Natsuko and Ainslie, Kylie and Baguelin, Marc and Bhatia, Sangeeta and Boonyasiri, Adhiratha and Cucunubá, Zulma and Cuomo-Dannenburg, Gina and Dighe, Amy and Fu, Han and Gaythorpe, Katy and Thompson, Hayley and Verity, Robert and Volz, Erik and Wang, Haowei and Wang, Yuanrong and Walker, Patrick GT and Walters, Caroline and Winskill, Peter and Whittaker, Charles and Donnelly, Christl A and Riley, Steven and Ghani, Azra C},
  date = {2020},
  pages = {20},
  abstract = {The global impact of COVID-19 has been profound, and the public health threat it represents is the most serious seen in a respiratory virus since the 1918 H1N1 influenza pandemic. Here we present the results of epidemiological modelling which has informed policymaking in the UK and other countries in recent weeks. In the absence of a COVID-19 vaccine, we assess the potential role of a number of public health measures – so-called non-pharmaceutical interventions (NPIs) – aimed at reducing contact rates in the population and thereby reducing transmission of the virus. In the results presented here, we apply a previously published microsimulation model to two countries: the UK (Great Britain specifically) and the US. We conclude that the effectiveness of any one intervention in isolation is likely to be limited, requiring multiple interventions to be combined to have a substantial impact on transmission.},
  file = {/mnt/data/Google Drive/Zotero/storage/794A9ZNP/Ferguson et al. - 2020 - Impact of non-pharmaceutical interventions (NPIs) .pdf},
  langid = {english}
}

@report{fergusonReportImpactNonpharmaceutical2020,
  title = {Report 9: {{Impact}} of Non-Pharmaceutical Interventions ({{NPIs}}) to Reduce {{COVID19}} Mortality and Healthcare Demand},
  shorttitle = {Report 9},
  author = {Ferguson, N. and Laydon, D. and Nedjati Gilani, G. and Imai, N. and Ainslie, K. and Baguelin, M. and Bhatia, S. and Boonyasiri, A. and Cucunuba Perez, Z. and Cuomo-Dannenburg, G. and Dighe, A. and Dorigatti, I. and Fu, H. and Gaythorpe, K. and Green, W. and Hamlet, A. and Hinsley, W. and Okell, L. and Van Elsland, S. and Thompson, H. and Verity, R. and Volz, E. and Wang, H. and Wang, Y. and Walker, P. and Walters, C. and Winskill, P. and Whittaker, C. and Donnelly, C. and Riley, S. and Ghani, A.},
  date = {2020-03-16},
  journaltitle = {20},
  doi = {10.25561/77482},
  url = {http://spiral.imperial.ac.uk/handle/10044/1/77482},
  urldate = {2021-05-29},
  abstract = {The global impact of COVID-19 has been profound, and the public health threat it represents is the most serious seen in a respiratory virus since the 1918 H1N1 influenza pandemic. Here we present the results of epidemiological modelling which has informed policymaking in the UK and other countries in recent weeks. In the absence of a COVID-19 vaccine, we assess the potential role of a number of public health measures – so-called non-pharmaceutical interventions (NPIs) – aimed at reducing contact rates in the population and thereby reducing transmission of the virus. In the results presented here, we apply a previously published microsimulation model to two countries: the UK (Great Britain specifically) and the US. We conclude that the effectiveness of any one intervention in isolation is likely to be limited, requiring multiple interventions to be combined to have a substantial impact on transmission. Two fundamental strategies are possible: (a) mitigation, which focuses on slowing but not necessarily stopping epidemic spread – reducing peak healthcare demand while protecting those most at risk of severe disease from infection, and (b) suppression, which aims to reverse epidemic growth, reducing case numbers to low levels and maintaining that situation indefinitely. Each policy has major challenges. We find that that optimal mitigation policies (combining home isolation of suspect cases, home quarantine of those living in the same household as suspect cases, and social distancing of the elderly and others at most risk of severe disease) might reduce peak healthcare demand by 2/3 and deaths by half. However, the resulting mitigated epidemic would still likely result in hundreds of thousands of deaths and health systems (most notably intensive care units) being overwhelmed many times over. For countries able to achieve it, this leaves suppression as the preferred policy option. We show that in the UK and US context, suppression will minimally require a combination of social distancing of the entire population, home isolation of cases and household quarantine of their family members. This may need to be supplemented by school and university closures, though it should be recognised that such closures may have negative impacts on health systems due to increased absenteeism. The major challenge of suppression is that this type of intensive intervention package – or something equivalently effective at reducing transmission – will need to be maintained until a vaccine becomes available (potentially 18 months or more) – given that we predict that transmission will quickly rebound if interventions are relaxed. We show that intermittent social distancing – triggered by trends in disease surveillance – may allow interventions to be relaxed temporarily in relative short time windows, but measures will need to be reintroduced if or when case numbers rebound. Last, while experience in China and now South Korea show that suppression is possible in the short term, it remains to be seen whether it is possible long-term, and whether the social and economic costs of the interventions adopted thus far can be reduced.},
  annotation = {Accepted: 2020-03-17T09:57:15Z},
  file = {/mnt/data/Google Drive/Zotero/storage/HQC2XC9J/Ferguson et al. - 2020 - Report 9 Impact of non-pharmaceutical interventio.pdf;/mnt/data/Google Drive/Zotero/storage/MEY92F4G/77482.html},
  langid = {american},
  type = {Report}
}

@article{ferroMeasuringForecastPerformance2017,
  title = {Measuring Forecast Performance in the Presence of Observation Error},
  author = {Ferro, Christopher A. T.},
  date = {2017},
  journaltitle = {Quarterly Journal of the Royal Meteorological Society},
  volume = {143},
  pages = {2665--2676},
  issn = {1477-870X},
  doi = {10.1002/qj.3115},
  url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.3115},
  urldate = {2021-03-26},
  abstract = {A new framework is introduced for measuring the performance of probability forecasts when the true value of the predictand is observed with error. In these circumstances, proper scoring rules favour good forecasts of observations rather than of truth and yield scores that vary with the quality of the observations. Proper scoring rules thus can favour forecasters who issue worse forecasts of the truth and can mask real changes in forecast performance if observation quality varies over time. Existing approaches to accounting for observation error provide unsatisfactory solutions to these two problems. A new class of ‘error-corrected’ proper scoring rules is defined that solves both problems by producing unbiased estimates of the scores that would be obtained if the forecasts could be verified against the truth. A general method for constructing error-corrected proper scoring rules is given for the case of categorical predictands, and error-corrected versions of the Dawid–Sebastiani scoring rule are proposed for numerical predictands. The benefits of accounting for observation error in ensemble post-processing and in forecast verification are illustrated in three data examples that include forecasts for the occurrence of tornadoes and of aircraft icing.},
  annotation = {\_eprint: https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3115},
  file = {/mnt/data/Google Drive/Zotero/storage/QHDIQDPK/Ferro - 2017 - Measuring forecast performance in the presence of .pdf;/mnt/data/Google Drive/Zotero/storage/LZIHYDDQ/qj.html},
  keywords = {observation errors,probability forecasts,proper,scores,scoring rules,verification},
  langid = {english},
  number = {708}
}

@article{fingerRealtimeAnalysisDiphtheria2019,
  title = {Real-Time Analysis of the Diphtheria Outbreak in Forcibly Displaced {{Myanmar}} Nationals in {{Bangladesh}}},
  author = {Finger, Flavio and Funk, Sebastian and White, Kate and Siddiqui, M. Ruby and Edmunds, W. John and Kucharski, Adam J.},
  date = {2019-03-12},
  journaltitle = {BMC Medicine},
  shortjournal = {BMC Medicine},
  volume = {17},
  pages = {58},
  issn = {1741-7015},
  doi = {10.1186/s12916-019-1288-7},
  url = {https://doi.org/10.1186/s12916-019-1288-7},
  urldate = {2020-04-12},
  abstract = {Between August and December 2017, more than 625,000 Rohingya from Myanmar fled into Bangladesh, settling in informal makeshift camps in Cox’s Bazar district and joining 212,000 Rohingya already present. In early November, a diphtheria outbreak hit the camps, with 440 reported cases during the first month. A rise in cases during early December led to a collaboration between teams from Médecins sans Frontières—who were running a provisional diphtheria treatment centre—and the London School of Hygiene and Tropical Medicine with the goal to use transmission dynamic models to forecast the potential scale of the outbreak and the resulting resource needs.},
  file = {/mnt/data/Google Drive/Zotero/storage/RZM5I6TE/Finger et al. - 2019 - Real-time analysis of the diphtheria outbreak in f.pdf;/mnt/data/Google Drive/Zotero/storage/UPFFPNMC/s12916-019-1288-7.html},
  number = {1}
}

@online{FK83Bvarsv,
  title = {{{FK83}}/Bvarsv},
  url = {https://github.com/FK83/bvarsv},
  urldate = {2020-04-13},
  abstract = {Analysis of the Primiceri (REStud, 2005) model. Contribute to FK83/bvarsv development by creating an account on GitHub.},
  file = {/mnt/data/Google Drive/Zotero/storage/34QH4CE5/bvarsv_Nov2015_website.html},
  langid = {english},
  organization = {{GitHub}}
}

@article{flaxmanEstimatingEffectsNonpharmaceutical2020,
  title = {Estimating the Effects of Non-Pharmaceutical Interventions on {{COVID}}-19 in {{Europe}}},
  author = {Flaxman, Seth and Mishra, Swapnil and Gandy, Axel and Unwin, H. Juliette T. and Mellan, Thomas A. and Coupland, Helen and Whittaker, Charles and Zhu, Harrison and Berah, Tresnia and Eaton, Jeffrey W. and Monod, Mélodie and Ghani, Azra C. and Donnelly, Christl A. and Riley, Steven M. and Vollmer, Michaela A. C. and Ferguson, Neil M. and Okell, Lucy C. and Bhatt, Samir},
  date = {2020-06-08},
  journaltitle = {Nature},
  pages = {1--8},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2405-7},
  url = {https://www.nature.com/articles/s41586-020-2405-7},
  urldate = {2020-06-18},
  abstract = {Following the emergence of a novel coronavirus1 (SARS-CoV-2) and its spread outside of China, Europe has experienced large epidemics. In response, many European countries have implemented unprecedented non-pharmaceutical interventions such as closure of schools and national lockdowns. We study the impact of major interventions across 11 European countries for the period from the start of COVID-19 until the 4th of May 2020 when lockdowns started to be lifted. Our model calculates backwards from observed deaths to estimate transmission that occurred several weeks prior, allowing for the time lag between infection and death. We use partial pooling of information between countries with both individual and shared effects on the reproduction number. Pooling allows more information to be used, helps overcome data idiosyncrasies, and enables more timely estimates. Our model relies on fixed estimates of some epidemiological parameters such as the infection fatality rate, does not include importation or subnational variation and assumes that changes in the reproduction number are an immediate response to interventions rather than gradual changes in behavior. Amidst the ongoing pandemic, we rely on death data that is incomplete, with systematic biases in reporting, and subject to future consolidation. We estimate that, for all the countries we consider, current interventions have been sufficient to drive the reproduction number \$\$\{R\}\_\{t\}\$\$Rt below 1 (probability \$\$\{R\}\_\{t\}\textbackslash,\$\$Rt{$<$} 1.0 is 99.9\%) and achieve epidemic control. We estimate that, across all 11 countries, between 12 and 15 million individuals have been infected with SARS-CoV-2 up to 4th May, representing between 3.2\% and 4.0\% of the population. Our results show that major non-pharmaceutical interventions and lockdown in particular have had a large effect on reducing transmission. Continued intervention should be considered to keep transmission of SARS-CoV-2 under control.},
  file = {/mnt/data/Google Drive/Zotero/storage/7BXLI453/Flaxman et al. - 2020 - Estimating the effects of non-pharmaceutical inter.pdf;/mnt/data/Google Drive/Zotero/storage/69J4UAFM/s41586-020-2405-7.html},
  langid = {english}
}

@article{floresDifferentialRISCAssociation2014,
  title = {Differential {{RISC}} Association of Endogenous Human {{microRNAs}} Predicts Their Inhibitory Potential},
  author = {Flores, Omar and Kennedy, Edward M. and Skalsky, Rebecca L. and Cullen, Bryan R.},
  date = {2014-04},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Res},
  volume = {42},
  pages = {4629--4639},
  issn = {0305-1048},
  doi = {10.1093/nar/gkt1393},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3985621/},
  urldate = {2019-05-22},
  abstract = {It has previously been assumed that the generally high stability of microRNAs (miRNAs) reflects their tight association with Argonaute (Ago) proteins, essential components of the RNA-induced silencing complex (RISC). However, recent data have suggested that the majority of mature miRNAs are not, in fact, Ago associated. Here, we demonstrate that endogenous human miRNAs vary widely, by {$>$}100-fold, in their level of RISC association and show that the level of Ago binding is a better indicator of inhibitory potential than is the total level of miRNA expression. While miRNAs of closely similar sequence showed comparable levels of RISC association in the same cell line, these varied between different cell types. Moreover, the level of RISC association could be modulated by overexpression of complementary target mRNAs. Together, these data indicate that the level of RISC association of a given endogenous miRNA is regulated by the available RNA targetome and predicts miRNA function.},
  eprint = {24464996},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/R2LLUIRP/Flores et al. - 2014 - Differential RISC association of endogenous human .pdf},
  number = {7},
  pmcid = {PMC3985621}
}

@online{ForecastVisualisation,
  title = {Forecast Visualisation},
  url = {https://covid19forecasthub.eu/visualisation.html},
  urldate = {2021-05-30}
}

@article{fragosoBayesianModelAveraging2018,
  title = {Bayesian {{Model Averaging}}: {{A Systematic Review}} and {{Conceptual Classification}}},
  shorttitle = {Bayesian {{Model Averaging}}},
  author = {Fragoso, Tiago M. and Bertoli, Wesley and Louzada, Francisco},
  date = {2018},
  journaltitle = {International Statistical Review},
  volume = {86},
  pages = {1--28},
  issn = {1751-5823},
  doi = {10.1111/insr.12243},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12243},
  urldate = {2020-06-02},
  abstract = {Bayesian model averaging (BMA) provides a coherent and systematic mechanism for accounting for model uncertainty. It can be regarded as an direct application of Bayesian inference to the problem of model selection, combined estimation and prediction. BMA produces a straightforward model choice criterion and less risky predictions. However, the application of BMA is not always straightforward, leading to diverse assumptions and situational choices on its different aspects. Despite the widespread application of BMA in the literature, there were not many accounts of these differences and trends besides a few landmark revisions in the late 1990s and early 2000s, therefore not accounting for advancements made in the last decades. In this work, we present an account of these developments through a careful content analysis of 820 articles in BMA published between 1996 and 2016. We also develop a conceptual classification scheme to better describe this vast literature, understand its trends and future directions and provide guidance for the researcher interested in both the application and development of the methodology. The results of the classification scheme and content review are then used to discuss the present and future of the BMA literature.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12243},
  file = {/mnt/data/Google Drive/Zotero/storage/6M4VGYYN/Fragoso et al. - 2018 - Bayesian Model Averaging A Systematic Review and .pdf;/mnt/data/Google Drive/Zotero/storage/XG55P67G/insr.html},
  keywords = {Bayesian model averaging,conceptual classification scheme,qualitative content analysis,systematic review},
  langid = {english},
  number = {1}
}

@article{fraleyEnsembleBMAPackageProbabilistic,
  title = {{{ensembleBMA}}: {{An R Package}} for {{Probabilistic Forecasting}} Using {{Ensembles}} and {{Bayesian Model Averaging}}},
  author = {Fraley, Chris and Raftery, Adrian E and Gneiting, Tilmann and Sloughter, J McLean},
  pages = {19},
  file = {/mnt/data/Google Drive/Zotero/storage/396E78V2/Fraley et al. - ensembleBMA An R Package for Probabilistic Foreca.pdf},
  langid = {english}
}

@article{fuGeneExpressionRegulation2014,
  title = {Gene Expression Regulation Mediated through Reversible M{\textsuperscript{6}}{{A RNA}} Methylation},
  author = {Fu, Ye and Dominissini, Dan and Rechavi, Gideon and He, Chuan},
  date = {2014-05},
  journaltitle = {Nature Reviews Genetics},
  volume = {15},
  pages = {293--306},
  issn = {1471-0064},
  doi = {10.1038/nrg3724},
  url = {https://www.nature.com/articles/nrg3724},
  urldate = {2018-12-03},
  abstract = {Cellular RNAs carry diverse chemical modifications that used to be regarded as static and having minor roles in 'fine-tuning' structural and functional properties of RNAs. In this Review, we focus on reversible methylation through the most prevalent mammalian mRNA internal modification, N6-methyladenosine (m6A). Recent studies have discovered protein 'writers', 'erasers' and 'readers' of this RNA chemical mark, as well as its dynamic deposition on mRNA and other types of nuclear RNA. These findings strongly indicate dynamic regulatory roles that are analogous to the well-known reversible epigenetic modifications of DNA and histone proteins. This reversible RNA methylation adds a new dimension to the developing picture of post-transcriptional regulation of gene expression.},
  file = {/mnt/data/Google Drive/Zotero/storage/TF47HCJS/Fu et al. - 2014 - Gene expression regulation mediated through revers.pdf;/mnt/data/Google Drive/Zotero/storage/XMWA2EI9/nrg3724.html},
  langid = {english},
  number = {5}
}

@article{funkAssessingPerformanceRealtime2019,
  title = {Assessing the Performance of Real-Time Epidemic Forecasts: {{A}} Case Study of {{Ebola}} in the {{Western Area}} Region of {{Sierra Leone}}, 2014-15},
  shorttitle = {Assessing the Performance of Real-Time Epidemic Forecasts},
  author = {Funk, Sebastian and Camacho, Anton and Kucharski, Adam J. and Lowe, Rachel and Eggo, Rosalind M. and Edmunds, W. John},
  date = {2019-02-11},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {15},
  pages = {e1006785},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006785},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006785},
  urldate = {2019-09-16},
  abstract = {Real-time forecasts based on mathematical models can inform critical decision-making during infectious disease outbreaks. Yet, epidemic forecasts are rarely evaluated during or after the event, and there is little guidance on the best metrics for assessment. Here, we propose an evaluation approach that disentangles different components of forecasting ability using metrics that separately assess the calibration, sharpness and bias of forecasts. This makes it possible to assess not just how close a forecast was to reality but also how well uncertainty has been quantified. We used this approach to analyse the performance of weekly forecasts we generated in real time for Western Area, Sierra Leone, during the 2013–16 Ebola epidemic in West Africa. We investigated a range of forecast model variants based on the model fits generated at the time with a semi-mechanistic model, and found that good probabilistic calibration was achievable at short time horizons of one or two weeks ahead but model predictions were increasingly unreliable at longer forecasting horizons. This suggests that forecasts may have been of good enough quality to inform decision making based on predictions a few weeks ahead of time but not longer, reflecting the high level of uncertainty in the processes driving the trajectory of the epidemic. Comparing forecasts based on the semi-mechanistic model to simpler null models showed that the best semi-mechanistic model variant performed better than the null models with respect to probabilistic calibration, and that this would have been identified from the earliest stages of the outbreak. As forecasts become a routine part of the toolkit in public health, standards for evaluation of performance will be important for assessing quality and improving credibility of mathematical models, and for elucidating difficulties and trade-offs when aiming to make the most useful and reliable forecasts.},
  file = {/mnt/data/Google Drive/Zotero/storage/X6Z9PIFT/Funk et al. - 2019 - Assessing the performance of real-time epidemic fo.pdf;/mnt/data/Google Drive/Zotero/storage/JN28VVKF/article.html},
  keywords = {Epidemiology,Forecasting,Infectious disease epidemiology,Infectious diseases,Mathematical models,Probability distribution,Public and occupational health,Sierra Leone},
  langid = {english},
  number = {2}
}

@article{funkRealtimeForecastingInfectious2018,
  title = {Real-Time Forecasting of Infectious Disease Dynamics with a Stochastic Semi-Mechanistic Model},
  author = {Funk, Sebastian and Camacho, Anton and Kucharski, Adam J. and Eggo, Rosalind M. and Edmunds, W. John},
  date = {2018-03},
  journaltitle = {Epidemics},
  shortjournal = {Epidemics},
  volume = {22},
  pages = {56--61},
  issn = {1755-4365},
  doi = {10.1016/j.epidem.2016.11.003},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5871642/},
  urldate = {2019-11-01},
  abstract = {•               A Bayesian semi-mechanistic model was applied to the Ebola Forecasting Challenge.                                         •               Model fits to simulated data were obtained from particle Markov-chain Monte Carlo.                                         •               Posterior samples of model parameters were used to generate forecast trajectories.                                         •               The forecasts were assessed using subsequently released simulation points.                                 , Real-time forecasts of infectious diseases can help public health planning, especially during outbreaks. If forecasts are generated from mechanistic models, they can be further used to target resources or to compare the impact of possible interventions. However, paremeterising such models is often difficult in real time, when information on behavioural changes, interventions and routes of transmission are not readily available. Here, we present a semi-mechanistic model of infectious disease dynamics that was used in real time during the 2013–2016 West African Ebola epidemic, and show fits to a Ebola Forecasting Challenge conducted in late 2015 with simulated data mimicking the true epidemic. We assess the performance of the model in different situations and identify strengths and shortcomings of our approach. Models such as the one presented here which combine the power of mechanistic models with the flexibility to include uncertainty about the precise outbreak dynamics may be an important tool in combating future outbreaks.},
  eprint = {28038870},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/E3U9WS5C/Funk et al. - 2018 - Real-time forecasting of infectious disease dynami.pdf},
  pmcid = {PMC5871642}
}

@software{funkSbfnkEbolaForecast2020,
  title = {Sbfnk/Ebola.Forecast.Wa.Sl},
  author = {Funk, Sebastian},
  date = {2020-01-09T14:59:01Z},
  origdate = {2018-10-05T06:34:34Z},
  url = {https://github.com/sbfnk/ebola.forecast.wa.sl},
  urldate = {2020-02-12},
  abstract = {Code accompanying the manuscript \&quot;Assessing the performance of real-time epidemic forecasts: A case study of Ebola  in the Western Area region of Sierra Leone, 2014-15\&quot; (doi:10.1371/journ...}
}

@article{funkShorttermForecastsInform2020,
  title = {Short-Term Forecasts to Inform the Response to the {{Covid}}-19 Epidemic in the {{UK}}},
  author = {Funk, Sebastian and Abbott, Sam and Atkins, B. D. and Baguelin, M. and Baillie, J. K. and Birrell, P. and Blake, J. and Bosse, N. I. and Burton, J. and Carruthers, J. and Davies, N. G. and Angelis, D. De and Dyson, L. and Edmunds, W. J. and Eggo, R. M. and Ferguson, N. M. and Gaythorpe, K. and Gorsich, E. and Guyver-Fletcher, G. and Hellewell, J. and Hill, E. M. and Holmes, A. and House, T. A. and Jewell, C. and Jit, M. and Jombart, T. and Joshi, I. and Keeling, M. J. and Kendall, E. and Knock, E. S. and Kucharski, A. J. and Lythgoe, K. A. and Meakin, S. R. and Munday, J. D. and Openshaw, P. J. M. and Overton, C. E. and Pagani, F. and Pearson, J. and Perez-Guzman, P. N. and Pellis, L. and Scarabel, F. and Semple, M. G. and Sherratt, K. and Tang, M. and Tildesley, M. J. and Leeuwen, E. Van and Whittles, L. K. and Group, CMMID COVID-19 Working and Team, Imperial College COVID-19 Response and Investigators, Isaric4c},
  date = {2020-11-13},
  journaltitle = {medRxiv},
  pages = {2020.11.11.20220962},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2020.11.11.20220962},
  url = {https://www.medrxiv.org/content/10.1101/2020.11.11.20220962v1},
  urldate = {2020-11-28},
  abstract = {{$<$}p{$>$}Background: Short-term forecasts of infectious disease can create situational awareness and inform planning for outbreak response. Here, we report on multi-model forecasts of Covid-19 in the UK that were generated at regular intervals starting at the end of March 2020, in order to monitor expected healthcare utilisation and population impacts in real time. Methods: We evaluated the performance of individual model forecasts generated between 24 March and 14 July 2020, using a variety of metrics including the weighted interval score as well as metrics that assess the calibration, sharpness, bias and absolute error of forecasts separately. We further combined the predictions from individual models to ensemble forecasts using a simple mean as well as a quantile regression average that aimed to maximise performance. We further compared model performance to a null model of no change. Results: In most cases, individual models performed better than the null model, and ensembles models were well calibrated and performed comparatively to the best individual models. The quantile regression average did not noticeably outperform the mean ensemble. Conclusions: Ensembles of multi-model forecasts can inform the policy response to the Covid-19 pandemic by assessing future resource needs and expected population impact of morbidity and mortality.{$<$}/p{$>$}},
  file = {/mnt/data/Google Drive/Zotero/storage/9RK57885/Funk et al. - 2020 - Short-term forecasts to inform the response to the.pdf;/mnt/data/Google Drive/Zotero/storage/AKDY6PAQ/2020.11.11.20220962v1.full.html},
  langid = {english}
}

@article{gabryVisualizationBayesianWorkflow2019,
  title = {Visualization in {{Bayesian}} Workflow},
  author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  date = {2019-02},
  journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  shortjournal = {J. R. Stat. Soc. A},
  volume = {182},
  pages = {389--402},
  issn = {09641998},
  doi = {10.1111/rssa.12378},
  url = {http://arxiv.org/abs/1709.01449},
  urldate = {2020-01-13},
  abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers.},
  archiveprefix = {arXiv},
  eprint = {1709.01449},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/B6BPIY9I/Gabry et al. - 2019 - Visualization in Bayesian workflow.pdf},
  keywords = {Statistics - Applications,Statistics - Methodology},
  langid = {english},
  number = {2}
}

@online{ganapathyvidyamurthyPairsTradingQuantitative2004,
  title = {Pairs {{Trading}}: {{Quantitative Methods}} and {{Analysis}} | {{Finance}} \& {{Investments Special Topics}} | {{General Finance}} \& {{Investments}} | {{Subjects}} | {{Wiley}}},
  shorttitle = {Pairs {{Trading}}},
  author = {{Ganapathy Vidyamurthy}},
  date = {2004},
  url = {https://www.wiley.com/en-us/Pairs+Trading%3A+Quantitative+Methods+and+Analysis-p-9780471460671},
  urldate = {2019-09-14},
  abstract = {The first in-depth analysis of pairs trading Pairs trading is a market-neutral strategy in its most simple form. The strategy involves being long (or bullish) one asset and short (or bearish) another. If properly performed, the investor will gain if the market rises or falls. Pairs Trading reveals the secrets of this rigorous quantitative analysis program to provide individuals and investment houses with the tools they need to successfully implement and profit from this proven trading methodology. Pairs Trading contains specific and tested formulas for identifying and investing in pairs, and answers important questions such as what ratio should be used to construct the pairs properly. Ganapathy Vidyamurthy (Stamford, CT) is currently a quantitative software analyst and developer at a major New York City hedge fund.},
  file = {/mnt/data/Google Drive/Zotero/storage/MEJKQP2I/Ganapathy Vidyamurthy - 2004 - Pairs Trading Quantitative Methods and Analysis .pdf;/mnt/data/Google Drive/Zotero/storage/GNX2X9ZK/Pairs+Trading+Quantitative+Methods+and+Analysis-p-9780471460671.html},
  langid = {american},
  organization = {{Wiley.com}}
}

@article{gasparriniDistributedLagLinear,
  title = {Distributed Lag Linear and Non-Linear Models for Time Series Data},
  author = {Gasparrini, Antonio},
  pages = {12},
  file = {/mnt/data/Google Drive/Zotero/storage/HAUGWLZV/Gasparrini - Distributed lag linear and non-linear models for t.pdf},
  langid = {english}
}

@article{gasparriniDistributedLagLinear2011,
  title = {Distributed {{Lag Linear}} and {{Non}}-{{Linear Models}} in {{{\emph{R}}}} : {{The Package}} {\textbf{Dlnm}}},
  shorttitle = {Distributed {{Lag Linear}} and {{Non}}-{{Linear Models}} in {{{\emph{R}}}}},
  author = {Gasparrini, Antonio},
  date = {2011},
  journaltitle = {Journal of Statistical Software},
  shortjournal = {J. Stat. Soft.},
  volume = {43},
  issn = {1548-7660},
  doi = {10.18637/jss.v043.i08},
  url = {http://www.jstatsoft.org/v43/i08/},
  urldate = {2019-10-31},
  abstract = {Distributed lag non-linear models (DLNMs) represent a modeling framework to flexibly describe associations showing potentially non-linear and delayed effects in time series data. This methodology rests on the definition of a crossbasis, a bi-dimensional functional space expressed by the combination of two sets of basis functions, which specify the relationships in the dimensions of predictor and lags, respectively. This framework is implemented in the R package dlnm, which provides functions to perform the broad range of models within the DLNM family and then to help interpret the results, with an emphasis on graphical representation. This paper offers an overview of the capabilities of the package, describing the conceptual and practical steps to specify and interpret DLNMs with an example of application to real data.},
  file = {/mnt/data/Google Drive/Zotero/storage/J4FFTYLL/Gasparrini - 2011 - Distributed Lag Linear and Non-Linear Models in i.pdf},
  langid = {english},
  number = {8}
}

@article{gasparriniPenalizedFrameworkDistributed2017,
  title = {A Penalized Framework for Distributed Lag Non-Linear Models},
  author = {Gasparrini, Antonio and Scheipl, Fabian and Armstrong, Ben and Kenward, Michael G.},
  date = {2017-09},
  journaltitle = {Biometrics},
  shortjournal = {Biometrics},
  volume = {73},
  pages = {938--948},
  issn = {1541-0420},
  doi = {10.1111/biom.12645},
  abstract = {Distributed lag non-linear models (DLNMs) are a modelling tool for describing potentially non-linear and delayed dependencies. Here, we illustrate an extension of the DLNM framework through the use of penalized splines within generalized additive models (GAM). This extension offers built-in model selection procedures and the possibility of accommodating assumptions on the shape of the lag structure through specific penalties. In addition, this framework includes, as special cases, simpler models previously proposed for linear relationships (DLMs). Alternative versions of penalized DLNMs are compared with each other and with the standard unpenalized version in a simulation study. Results show that this penalized extension to the DLNM class provides greater flexibility and improved inferential properties. The framework exploits recent theoretical developments of GAMs and is implemented using efficient routines within freely available software. Real-data applications are illustrated through two reproducible examples in time series and survival analysis.},
  eprint = {28134978},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/2MRP9PLR/Gasparrini et al. - 2017 - A penalized framework for distributed lag non-line.pdf},
  keywords = {Distributed lag,Exposure-lag-response,Generalized additive models,Latency,Nonlinear Dynamics,Penalized splines,Software},
  langid = {english},
  number = {3}
}

@inproceedings{gatevPairsTradingPerformance1999,
  title = {Pairs {{Trading}}: {{Performance}} of a {{Relative Value Arbitrage Rule}}},
  shorttitle = {Pairs {{Trading}}},
  author = {Gatev, Evan and Goetzmann, William N. and Rouwenhorst, K. Geert},
  date = {1999},
  doi = {10.1093/rfs/hhj020},
  abstract = {We test a Wall Street investment strategy known as pairs trading' with daily data over the period 1962 through 1997. Stocks are matched into pairs according to minimum distance in historical normalized price space. We test the profitability of several trading rules with six-month trading periods over the 1962-1997 period, and find average annualized excess returns of up to 12 percent for a number of self-financing portfolios of top pairs. Part of these profits may be due to market microstructure effects. Nevertheless, our historical trading profits exceed a conservative estimate of transaction costs through most of the period. We bootstrap random pairs in order to distinguish pairs trading from pure mean-reversion strategies. The bootstrap results suggest that the pairs' effect differs from previously documented mean reversion profits.},
  file = {/mnt/data/Google Drive/Zotero/storage/Z48GWP4T/Gatev et al. - 1999 - Pairs Trading Performance of a Relative Value Arb.pdf},
  keywords = {Estimated,Offset binary,Risk measure}
}

@article{gelmanPOSTERIORPREDICTIVEASSESSMENT,
  title = {{{POSTERIOR PREDICTIVE ASSESSMENT OF MODEL FITNESS VIA REALIZED DISCREPANCIES}}},
  author = {Gelman, Andrew and Meng, Xiao-Li and Stern, Hal},
  pages = {76},
  abstract = {This paper considers Bayesian counterparts of the classical tests for goodness of fit and their use in judging the fit of a single Bayesian model to the observed data. We focus on posterior predictive assessment, in a framework that also includes conditioning on auxiliary statistics. The Bayesian formulation facilitates the construction and calculation of a meaningful reference distribution not only for any (classical) statistic, but also for any parameter-dependent “statistic” or discrepancy. The latter allows us to propose the realized discrepancy assessment of model fitness, which directly measures the true discrepancy between data and the posited model, for any aspect of the model which we want to explore. The computation required for the realized discrepancy assessment is a straightforward byproduct of the posterior simulation used for the original Bayesian analysis.},
  file = {/mnt/data/Google Drive/Zotero/storage/VX43QDUG/Gelman et al. - POSTERIOR PREDICTIVE ASSESSMENT OF MODEL FITNESS V.pdf},
  langid = {english}
}

@article{gelmanUnderstandingPredictiveInformation2014,
  title = {Understanding Predictive Information Criteria for {{Bayesian}} Models},
  author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
  date = {2014-11-01},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {24},
  pages = {997--1016},
  issn = {1573-1375},
  doi = {10.1007/s11222-013-9416-2},
  url = {https://doi.org/10.1007/s11222-013-9416-2},
  urldate = {2019-02-28},
  abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a bias-corrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this paper is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
  file = {/mnt/data/Google Drive/Zotero/storage/B2VVZDAP/Gelman et al. - 2014 - Understanding predictive information criteria for .pdf},
  keywords = {AIC,Bayes,Cross-validation,DIC,Prediction,WAIC},
  langid = {english},
  number = {6}
}

@article{gelmanUnderstandingPredictiveInformation2014a,
  title = {Understanding Predictive Information Criteria for {{Bayesian}} Models},
  author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
  date = {2014-11},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {24},
  pages = {997--1016},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-013-9416-2},
  url = {http://link.springer.com/10.1007/s11222-013-9416-2},
  urldate = {2020-08-13},
  abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
  file = {/mnt/data/Google Drive/Zotero/storage/U9RQUUR4/Gelman et al. - 2014 - Understanding predictive information criteria for .pdf},
  langid = {english},
  number = {6}
}

@online{gerardUnifyingGeneralizingMethods2017,
  title = {Unifying and {{Generalizing Methods}} for {{Removing Unwanted Variation Based}} on {{Negative Controls}}},
  author = {Gerard, David and Stephens, Matthew},
  date = {2017-05-23},
  url = {http://arxiv.org/abs/1705.08393},
  urldate = {2019-04-16},
  abstract = {Unwanted variation, including hidden confounding, is a well-known problem in many fields, particularly large-scale gene expression studies. Recent proposals to use control genes --- genes assumed to be unassociated with the covariates of interest --- have led to new methods to deal with this problem. Going by the moniker Removing Unwanted Variation (RUV), there are many versions --- RUV1, RUV2, RUV4, RUVinv, RUVrinv, RUVfun. In this paper, we introduce a general framework, RUV*, that both unites and generalizes these approaches. This unifying framework helps clarify connections between existing methods. In particular we provide conditions under which RUV2 and RUV4 are equivalent. The RUV* framework also preserves an advantage of RUV approaches --- their modularity --- which facilitates the development of novel methods based on existing matrix imputation algorithms. We illustrate this by implementing RUVB, a version of RUV* based on Bayesian factor analysis. In realistic simulations based on real data we found that RUVB is competitive with existing methods in terms of both power and calibration, although we also highlight the challenges of providing consistently reliable calibration among data sets.},
  archiveprefix = {arXiv},
  eprint = {1705.08393},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/LFR8VJ4F/Gerard and Stephens - 2017 - Unifying and Generalizing Methods for Removing Unw.pdf;/mnt/data/Google Drive/Zotero/storage/8PDIFYVI/1705.html},
  keywords = {62J15 (Primary) 62F15; 62H25; 62P10 (Secondary),Mathematics - Statistics Theory,Statistics - Methodology},
  primaryclass = {math, stat}
}

@software{gerardVariousIdeasConfounder2019,
  title = {Various {{Ideas}} for {{Confounder Adjustment}} in {{Regression}}: Dcgerard/Vicar},
  shorttitle = {Various {{Ideas}} for {{Confounder Adjustment}} in {{Regression}}},
  author = {Gerard, David},
  date = {2019-04-30T17:41:59Z},
  origdate = {2016-06-07T16:40:43Z},
  url = {https://github.com/dcgerard/vicar},
  urldate = {2019-05-14}
}

@report{gibsonImprovingProbabilisticInfectious2019,
  title = {Improving {{Probabilistic Infectious Disease Forecasting Through Coherence}}},
  author = {Gibson, Graham Casey and Moran, Kelly R. and Reich, Nicholas G. and Osthus, Dave},
  date = {2019-12-27},
  institution = {{Bioinformatics}},
  doi = {10.1101/2019.12.27.889212},
  url = {http://biorxiv.org/lookup/doi/10.1101/2019.12.27.889212},
  urldate = {2020-04-06},
  abstract = {With an estimated \$10.4 billion in medical costs and 31.4 million outpatient visits each year, influenza poses a serious burden of disease in the United States. To provide insights and advance warning into the spread of influenza, the U.S. Centers for Disease Control and Prevention (CDC) runs a challenge for forecasting weighted influenza-like illness (wILI) at the national and regional level. Many models produce independent forecasts for each geographical unit, ignoring the constraint that the national wILI is a weighted sum of regional wILI, where the weights correspond to the population size of the region. We propose a novel algorithm that transforms a set of independent forecast distributions to obey this constraint, which we refer to as probabilistically coherent. Enforcing probabilistic coherence led to an increase in forecast skill for 90\% of the models we tested over multiple flu seasons, highlighting the importance of respecting the forecasting system’s geographical hierarchy.},
  file = {/mnt/data/Google Drive/Zotero/storage/J22WMETR/Gibson et al. - 2019 - Improving Probabilistic Infectious Disease Forecas.pdf},
  langid = {english},
  type = {preprint}
}

@report{gibsonImprovingProbabilisticInfectious2019a,
  title = {Improving {{Probabilistic Infectious Disease Forecasting Through Coherence}}},
  author = {Gibson, Graham Casey and Moran, Kelly R. and Reich, Nicholas G. and Osthus, Dave},
  date = {2019-12-27},
  institution = {{Bioinformatics}},
  doi = {10.1101/2019.12.27.889212},
  url = {http://biorxiv.org/lookup/doi/10.1101/2019.12.27.889212},
  urldate = {2020-05-16},
  abstract = {With an estimated \$10.4 billion in medical costs and 31.4 million outpatient visits each year, influenza poses a serious burden of disease in the United States. To provide insights and advance warning into the spread of influenza, the U.S. Centers for Disease Control and Prevention (CDC) runs a challenge for forecasting weighted influenza-like illness (wILI) at the national and regional level. Many models produce independent forecasts for each geographical unit, ignoring the constraint that the national wILI is a weighted sum of regional wILI, where the weights correspond to the population size of the region. We propose a novel algorithm that transforms a set of independent forecast distributions to obey this constraint, which we refer to as probabilistically coherent. Enforcing probabilistic coherence led to an increase in forecast skill for 90\% of the models we tested over multiple flu seasons, highlighting the importance of respecting the forecasting system’s geographical hierarchy.},
  file = {/mnt/data/Google Drive/Zotero/storage/LRMDHF65/Gibson et al. - 2019 - Improving Probabilistic Infectious Disease Forecas.pdf},
  langid = {english},
  type = {preprint}
}

@article{glostenRelationExpectedValue1993,
  title = {On the {{Relation}} between the {{Expected Value}} and the {{Volatility}} of the {{Nominal Excess Return}} on {{Stocks}}},
  author = {Glosten, Lawrence R. and Jagannathan, Ravi and Runkle, David E.},
  date = {1993-12},
  journaltitle = {The Journal of Finance},
  shortjournal = {The Journal of Finance},
  volume = {48},
  pages = {1779--1801},
  issn = {00221082},
  doi = {10.1111/j.1540-6261.1993.tb05128.x},
  url = {http://doi.wiley.com/10.1111/j.1540-6261.1993.tb05128.x},
  urldate = {2019-09-12},
  abstract = {We find support for a negative relation between conditional expected monthly return and conditional variance of monthly return, using a GARCH-Mmodel modified by allowing (1) seasonal patterns in volatility, (2) positive and negative innovations to returns having different impacts on conditional volatility, and (3) nominalinterest rates to predictconditionalvariance.Using the modifiedGARCH-M model, we also show that monthly conditionalvolatility may not be as persistent as was thought. Positive unanticipated returns appear to result in a downward revision of the conditional volatility whereas negative unanticipated returns result in an upward revision of conditional volatility.},
  file = {/mnt/data/Google Drive/Zotero/storage/8WA5HJU7/Glosten et al. - 1993 - On the Relation between the Expected Value and the.pdf},
  langid = {english},
  number = {5}
}

@online{glownyurzadstatystycznyLudnoscStanStruktura2020,
  title = {Ludność. Stan i struktura ludności oraz ruch naturalny w przekroju terytorialnym (stan w dniu 30.06.2020)},
  author = {Główny Urząd Statystyczny},
  date = {2020-06-30},
  url = {https://stat.gov.pl/obszary-tematyczne/ludnosc/ludnosc/ludnosc-stan-i-struktura-ludnosci-oraz-ruch-naturalny-w-przekroju-terytorialnym-stan-w-dniu-30-06-2020,6,28.html},
  urldate = {2021-06-10},
  abstract = {Główny Urząd Statystyczny - Portal Statystyki Publicznej},
  file = {/mnt/data/Google Drive/Zotero/storage/IDHSTLBK/ludnosc-stan-i-struktura-ludnosci-oraz-ruch-naturalny-w-przekroju-terytorialnym-stan-w-dniu-30-.html},
  langid = {polish},
  organization = {{stat.gov.pl}}
}

@article{gneitingCalibratedProbabilisticForecasting2005,
  title = {Calibrated {{Probabilistic Forecasting Using Ensemble Model Output Statistics}} and {{Minimum CRPS Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E. and Westveld, Anton H. and Goldman, Tom},
  date = {2005-05-01},
  journaltitle = {Monthly Weather Review},
  shortjournal = {Mon. Wea. Rev.},
  volume = {133},
  pages = {1098--1118},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/MWR2904.1},
  url = {https://journals.ametsoc.org/mwr/article/133/5/1098/67504/Calibrated-Probabilistic-Forecasting-Using},
  urldate = {2020-08-12},
  file = {/mnt/data/Google Drive/Zotero/storage/DWIEDUKB/Gneiting et al. - 2005 - Calibrated Probabilistic Forecasting Using Ensembl.pdf;/mnt/data/Google Drive/Zotero/storage/UKQZH4WN/Calibrated-Probabilistic-Forecasting-Using.html},
  langid = {english},
  number = {5}
}

@online{gneitingMakingEvaluatingPoint2010,
  title = {Making and {{Evaluating Point Forecasts}}},
  author = {Gneiting, Tilmann},
  date = {2010-03-07},
  url = {http://arxiv.org/abs/0912.0902},
  urldate = {2020-08-09},
  abstract = {Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, such as the absolute error or the squared error. The individual scores are then averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the (root) mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance.},
  archiveprefix = {arXiv},
  eprint = {0912.0902},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/EG5KRUWL/Gneiting - 2010 - Making and Evaluating Point Forecasts.pdf;/mnt/data/Google Drive/Zotero/storage/8VSGH5LC/0912.html},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Methodology},
  primaryclass = {math, stat}
}

@article{gneitingProbabilisticForecastsCalibration2007,
  title = {Probabilistic Forecasts, Calibration and Sharpness},
  author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
  date = {2007},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {69},
  pages = {243--268},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2007.00587.x},
  url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2007.00587.x},
  urldate = {2020-02-17},
  abstract = {Summary. Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
  file = {/mnt/data/Google Drive/Zotero/storage/BUWD6CGT/Gneiting et al. - 2007 - Probabilistic forecasts, calibration and sharpness.pdf;/mnt/data/Google Drive/Zotero/storage/EUCMSBKN/j.1467-9868.2007.00587.html},
  keywords = {Cross-validation,Density forecast,Ensemble prediction system,Ex post evaluation,Forecast verification,Model diagnostics,Posterior predictive assessment,Predictive distribution,Prequential principle,Probability integral transform,Proper scoring rule},
  langid = {english},
  number = {2}
}

@article{gneitingStrictlyProperScoring2007,
  title = {Strictly {{Proper Scoring Rules}}, {{Prediction}}, and {{Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  date = {2007-03},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {102},
  pages = {359--378},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214506000001437},
  url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
  urldate = {2020-03-22},
  file = {/mnt/data/Google Drive/Zotero/storage/P599P5ZY/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf},
  langid = {english},
  number = {477}
}

@article{gneitingWeatherForecastingEnsemble2005,
  title = {Weather {{Forecasting}} with {{Ensemble Methods}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E.},
  date = {2005-10-14},
  journaltitle = {Science},
  volume = {310},
  pages = {248--249},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1115255},
  url = {https://science.sciencemag.org/content/310/5746/248},
  urldate = {2021-05-30},
  abstract = {{$<$}p{$>$} Traditional weather forecasting has been built on a foundation of deterministic modeling--start with initial conditions, put them into a supercomputer model, and end up with a prediction about future weather. But as Gneiting and Raftery discuss in their Perspective, a new approach--ensemble forecasting--was introduced in the early 1990s. In this method, up to 100 different computer runs, each with slightly different starting conditions or model assumptions, are combined into a weather forecast. In concert with statistical techniques, ensembles can provide accurate statements about the uncertainty in daily and seasonal forecasting. The challenge now is to improve the modeling, statistical analysis, and visualization technologies for disseminating the ensemble results. {$<$}/p{$>$}},
  eprint = {16224011},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/VRJMN77J/Gneiting and Raftery - 2005 - Weather Forecasting with Ensemble Methods.pdf;/mnt/data/Google Drive/Zotero/storage/8Q5UA2FU/248.html},
  langid = {english},
  number = {5746}
}

@article{goodRationalDecisions1952,
  title = {Rational {{Decisions}}},
  author = {Good, I. J.},
  date = {1952},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {14},
  pages = {107--114},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {This paper deals first with the relationship between the theory of probability and the theory of rational behaviour. A method is then suggested for encouraging people to make accurate probability estimates, a connection with the theory of information being mentioned. Finally Wald's theory of statistical decision functions is summarised and generalised and its relation to the theory of rational behaviour is discussed.},
  eprint = {2984087},
  eprinttype = {jstor},
  file = {/mnt/data/Google Drive/Zotero/storage/23458422/2020 - Rational Decisions.pdf},
  keywords = {Log Score,LogS},
  number = {1}
}

@article{gosticPracticalConsiderationsMeasuring2020,
  title = {Practical Considerations for Measuring the Effective Reproductive Number, {{Rt}}},
  author = {Gostic, Katelyn M. and McGough, Lauren and Baskerville, Ed and Abbott, Sam and Joshi, Keya and Tedijanto, Christine and Kahn, Rebecca and Niehus, Rene and Hay, James and de Salazar, Pablo and Hellewell, Joel and Meakin, Sophie and Munday, James and Bosse, Nikos I. and Sherrat, Katharine and Thompson, Robin N. and White, Laura F. and Huisman, Jana S. and Scire, Jérémie and Bonhoeffer, Sebastian and Stadler, Tanja and Wallinga, Jacco and Funk, Sebastian and Lipsitch, Marc and Cobey, Sarah},
  date = {2020-06-20},
  journaltitle = {medRxiv},
  shortjournal = {medRxiv},
  doi = {10.1101/2020.06.18.20134858},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7325187/},
  urldate = {2020-08-12},
  abstract = {Estimation of the effective reproductive number, Rt, is important for detecting changes in disease transmission over time. During the COVID-19 pandemic, policymakers and public health officials are using Rt to assess the effectiveness of interventions and to inform policy. However, estimation of Rt from available data presents several challenges, with critical implications for the interpretation of the course of the pandemic. The purpose of this document is to summarize these challenges, illustrate them with examples from synthetic data, and, where possible, make methodological recommendations. For near real-time estimation of Rt, we recommend the approach of Cori et al. (2013), which uses data from before time t and empirical estimates of the distribution of time between infections. Methods that require data from after time t, such as Wallinga and Teunis (2004), are conceptually and methodologically less suited for near real-time estimation, but may be appropriate for some retrospective analyses. We advise against using methods derived from Bettencourt and Ribeiro (2008), as the resulting Rt estimates may be biased if the underlying structural assumptions are not met. A challenge common to all approaches is reconstruction of the time series of new infections from observations occurring long after the moment of transmission. Naive approaches for dealing with observation delays, such as subtracting delays sampled from a distribution, can introduce bias. We provide suggestions for how to mitigate this and other technical challenges and highlight open problems in Rt estimation.},
  eprint = {32607522},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/2DQ39CTT/Gostic et al. - 2020 - Practical considerations for measuring the effecti.pdf},
  options = {useprefix=true},
  pmcid = {PMC7325187}
}

@article{graefeCombiningForecastsApplication2014,
  title = {Combining Forecasts: {{An}} Application to Elections},
  shorttitle = {Combining Forecasts},
  author = {Graefe, Andreas and Armstrong, J. Scott and Jones, Randall J. and Cuzán, Alfred G.},
  date = {2014-01-01},
  journaltitle = {International Journal of Forecasting},
  shortjournal = {International Journal of Forecasting},
  volume = {30},
  pages = {43--54},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2013.02.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0169207013000423},
  urldate = {2021-07-07},
  abstract = {We summarize the literature on the effectiveness of combining forecasts by assessing the conditions under which combining is most valuable. Using data on the six US presidential elections from 1992 to 2012, we report the reductions in error obtained by averaging forecasts within and across four election forecasting methods: poll projections, expert judgment, quantitative models, and the Iowa Electronic Markets. Across the six elections, the resulting combined forecasts were more accurate than any individual component method, on average. The gains in accuracy from combining increased with the numbers of forecasts used, especially when these forecasts were based on different methods and different data, and in situations involving high levels of uncertainty. Such combining yielded error reductions of between 16\% and 59\%, compared to the average errors of the individual forecasts. This improvement is substantially greater than the 12\% reduction in error that had been reported previously for combining forecasts.},
  file = {/mnt/data/Google Drive/Zotero/storage/RV4HB4EK/S0169207013000423.html},
  keywords = {Combining,Econometric models,Election forecasting,Expert judgment,Polls,Prediction markets},
  langid = {english},
  number = {1}
}

@article{graefeCombiningForecastsApplication2014a,
  title = {Combining Forecasts: {{An}} Application to Elections},
  shorttitle = {Combining Forecasts},
  author = {Graefe, Andreas and Armstrong, J. Scott and Jones, Randall J. and Cuzán, Alfred G.},
  date = {2014-01},
  journaltitle = {International Journal of Forecasting},
  shortjournal = {International Journal of Forecasting},
  volume = {30},
  pages = {43--54},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2013.02.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207013000423},
  urldate = {2021-07-07},
  abstract = {We summarize the literature on the effectiveness of combining forecasts by assessing the conditions under which combining is most valuable. Using data on the six US presidential elections from 1992 to 2012, we report the reductions in error obtained by averaging forecasts within and across four election forecasting methods: poll projections, expert judgment, quantitative models, and the Iowa Electronic Markets. Across the six elections, the resulting combined forecasts were more accurate than any individual component method, on average. The gains in accuracy from combining increased with the numbers of forecasts used, especially when these forecasts were based on different methods and different data, and in situations involving high levels of uncertainty. Such combining yielded error reductions of between 16\% and 59\%, compared to the average errors of the individual forecasts. This improvement is substantially greater than the 12\% reduction in error that had been reported previously for combining forecasts.},
  file = {/mnt/data/Google Drive/Zotero/storage/T4B8B8EI/Graefe et al. - 2014 - Combining forecasts An application to elections.pdf},
  langid = {english},
  number = {1}
}

@article{graefeCombiningForecastsApplication2014b,
  title = {Combining {{Forecasts}}: {{An Application}} to {{Elections}}},
  shorttitle = {Combining {{Forecasts}}},
  author = {Graefe, Andreas and Armstrong, J. and Jones, Randall and Cuzán, Alfred},
  date = {2014-03-31},
  journaltitle = {International Journal of Forecasting},
  shortjournal = {International Journal of Forecasting},
  volume = {30},
  pages = {43--54},
  doi = {10.1016/j.ijforecast.2013.02.005},
  abstract = {We summarize the literature on the effectiveness of combining forecasts by assessing the conditions under which combining is most valuable. Using data on the six US presidential elections from 1992 to 2012, we report the reductions in error obtained by averaging forecasts within and across four election forecasting methods: poll projections, expert judgment, quantitative models, and the Iowa Electronic Markets. Across the six elections, the resulting combined forecasts were more accurate than any individual component method, on average. The gains in accuracy from combining increased with the numbers of forecasts used, especially when these forecasts were based on different methods and different data, and in situations involving high levels of uncertainty. Such combining yielded error reductions of between 16\% and 59\%, compared to the average errors of the individual forecasts. This improvement is substantially greater than the 12\% reduction in error that had been reported previously for combining forecasts.},
  file = {/mnt/data/Google Drive/Zotero/storage/2K2CAK26/Graefe et al. - 2014 - Combining Forecasts An Application to Elections.pdf}
}

@article{grazianiProbabilisticRecalibrationForecasts2021,
  title = {Probabilistic Recalibration of Forecasts},
  author = {Graziani, Carlo and Rosner, Robert and Adams, Jennifer M. and Machete, Reason L.},
  date = {2021-01-01},
  journaltitle = {International Journal of Forecasting},
  shortjournal = {International Journal of Forecasting},
  volume = {37},
  pages = {1--27},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.04.019},
  url = {https://www.sciencedirect.com/science/article/pii/S016920701930158X},
  urldate = {2021-03-28},
  abstract = {We present a scheme by which a probabilistic forecasting system whose predictions have a poor probabilistic calibration may be recalibrated through the incorporation of past performance information in order to produce a new forecasting system that is demonstrably superior to the original, inasmuch as one may use it to win wagers consistently against someone who is using the original system. The scheme utilizes Gaussian process (GP) modeling to estimate a probability distribution over the probability integral transform (PIT) of a scalar predictand. The GP density estimate gives closed-form access to information entropy measures that are associated with the estimated distribution, which allows the prediction of winnings in wagers against the base forecasting system. A separate consequence of the procedure is that the recalibrated forecast has a uniform expected PIT distribution. One distinguishing feature of the procedure is that it is appropriate even if the PIT values are not i.i.d. The recalibration scheme is formulated in a framework that exploits the deep connections among information theory, forecasting, and betting. We demonstrate the effectiveness of the scheme in two case studies: a laboratory experiment with a nonlinear circuit and seasonal forecasts of the intensity of the El Niño-Southern Oscillation phenomenon.},
  file = {/mnt/data/Google Drive/Zotero/storage/V77RMXS5/Graziani et al. - 2021 - Probabilistic recalibration of forecasts.pdf;/mnt/data/Google Drive/Zotero/storage/85SD6JMG/S016920701930158X.html;/mnt/data/Google Drive/Zotero/storage/ZFACSVML/S016920701930158X.html},
  keywords = {Gaussian processes,Information theory,Machine learning,Probabilistic calibration,Sharpness,Skill scoring},
  langid = {english},
  number = {1}
}

@article{guhaBayesianHiddenMarkov2008,
  title = {Bayesian {{Hidden Markov Modeling}} of {{Array CGH Data}}},
  author = {Guha, Subharup and Li, Yi and Neuberg, Donna},
  date = {2008-06-01},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {J Am Stat Assoc},
  volume = {103},
  pages = {485--497},
  issn = {0162-1459},
  doi = {10.1198/016214507000000923},
  abstract = {Genomic alterations have been linked to the development and progression of cancer. The technique of comparative genomic hybridization (CGH) yields data consisting of fluorescence intensity ratios of test and reference DNA samples. The intensity ratios provide information about the number of copies in DNA. Practical issues such as the contamination of tumor cells in tissue specimens and normalization errors necessitate the use of statistics for learning about the genomic alterations from array CGH data. As increasing amounts of array CGH data become available, there is a growing need for automated algorithms for characterizing genomic profiles. Specifically, there is a need for algorithms that can identify gains and losses in the number of copies based on statistical considerations, rather than merely detect trends in the data.We adopt a Bayesian approach, relying on the hidden Markov model to account for the inherent dependence in the intensity ratios. Posterior inferences are made about gains and losses in copy number. Localized amplifications (associated with oncogene mutations) and deletions (associated with mutations of tumor suppressors) are identified using posterior probabilities. Global trends such as extended regions of altered copy number are detected. Because the posterior distribution is analytically intractable, we implement a Metropolis-within-Gibbs algorithm for efficient simulation-based inference. Publicly available data on pancreatic adenocarcinoma, glioblastoma multiforme, and breast cancer are analyzed, and comparisons are made with some widely used algorithms to illustrate the reliability and success of the technique.},
  eprint = {22375091},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/39BWIRR9/Guha et al. - 2008 - Bayesian Hidden Markov Modeling of Array CGH Data.pdf},
  langid = {english},
  number = {482},
  pmcid = {PMC3286622}
}

@article{haiderPredictionMixtureModels,
  title = {Prediction with Mixture Models},
  author = {Haider, Peter},
  pages = {58},
  file = {/mnt/data/Google Drive/Zotero/storage/UXIZ5K8I/Haider - Prediction with Mixture Models.pdf},
  langid = {german}
}

@article{hamillInterpretationRankHistograms2001,
  title = {Interpretation of {{Rank Histograms}} for {{Verifying Ensemble Forecasts}}},
  author = {Hamill, Thomas M.},
  date = {2001-03-01},
  journaltitle = {Monthly Weather Review},
  shortjournal = {Mon. Wea. Rev.},
  volume = {129},
  pages = {550--560},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/1520-0493(2001)129<0550:IORHFV>2.0.CO;2},
  url = {https://journals.ametsoc.org/mwr/article/129/3/550/66423/Interpretation-of-Rank-Histograms-for-Verifying},
  urldate = {2020-08-12},
  file = {/mnt/data/Google Drive/Zotero/storage/TFD3LRCD/Hamill - 2001 - Interpretation of Rank Histograms for Verifying En.pdf;/mnt/data/Google Drive/Zotero/storage/AQIRNSKE/Interpretation-of-Rank-Histograms-for-Verifying.html},
  langid = {english},
  number = {3}
}

@article{hansenForecastComparisonVolatility2005,
  title = {A Forecast Comparison of Volatility Models: Does Anything Beat a {{GARCH}}(1,1)?},
  shorttitle = {A Forecast Comparison of Volatility Models},
  author = {Hansen, Peter R. and Lunde, Asger},
  date = {2005},
  journaltitle = {Journal of Applied Econometrics},
  volume = {20},
  pages = {873--889},
  issn = {1099-1255},
  doi = {10.1002/jae.800},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.800},
  urldate = {2019-09-13},
  abstract = {We compare 330 ARCH-type models in terms of their ability to describe the conditional variance. The models are compared out-of-sample using DM–\$ exchange rate data and IBM return data, where the latter is based on a new data set of realized variance. We find no evidence that a GARCH(1,1) is outperformed by more sophisticated models in our analysis of exchange rates, whereas the GARCH(1,1) is clearly inferior to models that can accommodate a leverage effect in our analysis of IBM returns. The models are compared with the test for superior predictive ability (SPA) and the reality check for data snooping (RC). Our empirical results show that the RC lacks power to an extent that makes it unable to distinguish ‘good’ and ‘bad’ models in our analysis. Copyright © 2005 John Wiley \& Sons, Ltd.},
  file = {/mnt/data/Google Drive/Zotero/storage/K7BM4QPB/Hansen and Lunde - 2005 - A forecast comparison of volatility models does a.pdf;/mnt/data/Google Drive/Zotero/storage/6UVZC85E/jae.html},
  langid = {english},
  number = {7}
}

@article{hardcastleBaySeqEmpiricalBayesian2010,
  title = {{{baySeq}}: Empirical {{Bayesian}} Methods for Identifying Differential Expression in Sequence Count Data},
  shorttitle = {{{baySeq}}},
  author = {Hardcastle, Thomas J. and Kelly, Krystyna A.},
  date = {2010-08-10},
  journaltitle = {BMC bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {11},
  pages = {422},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-11-422},
  abstract = {BACKGROUND: High throughput sequencing has become an important technology for studying expression levels in many types of genomic, and particularly transcriptomic, data. One key way of analysing such data is to look for elements of the data which display particular patterns of differential expression in order to take these forward for further analysis and validation. RESULTS: We propose a framework for defining patterns of differential expression and develop a novel algorithm, baySeq, which uses an empirical Bayes approach to detect these patterns of differential expression within a set of sequencing samples. The method assumes a negative binomial distribution for the data and derives an empirically determined prior distribution from the entire dataset. We examine the performance of the method on real and simulated data. CONCLUSIONS: Our method performs at least as well, and often better, than existing methods for analyses of pairwise differential expression in both real and simulated data. When we compare methods for the analysis of data from experimental designs involving multiple sample groups, our method again shows substantial gains in performance. We believe that this approach thus represents an important step forward for the analysis of count data from sequencing experiments.},
  eprint = {20698981},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/KCL69KYW/Hardcastle and Kelly - 2010 - baySeq empirical Bayesian methods for identifying.pdf},
  keywords = {Algorithms,Arabidopsis,Base Sequence,Bayes Theorem,Gene Expression Profiling,Research Design,RNA; Plant},
  langid = {english},
  pmcid = {PMC2928208}
}

@article{hardcastleEmpiricalBayesianAnalysis2013,
  title = {Empirical {{Bayesian}} Analysis of Paired High-Throughput Sequencing Data with a Beta-Binomial Distribution},
  author = {Hardcastle, Thomas J. and Kelly, Krystyna A.},
  date = {2013-04-23},
  journaltitle = {BMC bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {14},
  pages = {135},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-14-135},
  abstract = {BACKGROUND: Pairing of samples arises naturally in many genomic experiments; for example, gene expression in tumour and normal tissue from the same patients. Methods for analysing high-throughput sequencing data from such experiments are required to identify differential expression, both within paired samples and between pairs under different experimental conditions. RESULTS: We develop an empirical Bayesian method based on the beta-binomial distribution to model paired data from high-throughput sequencing experiments. We examine the performance of this method on simulated and real data in a variety of scenarios. Our methods are implemented as part of the RbaySeq package (versions 1.11.6 and greater) available from Bioconductor (http://www.bioconductor.org). CONCLUSIONS: We compare our approach to alternatives based on generalised linear modelling approaches and show that our method offers significant gains in performance on simulated data. In testing on real data from oral squamous cell carcinoma patients, we discover greater enrichment of previously identified head and neck squamous cell carcinoma associated gene sets than has previously been achieved through a generalised linear modelling approach, suggesting that similar gains in performance may be found in real data. Our methods thus show real and substantial improvements in analyses of high-throughput sequencing data from paired samples.},
  eprint = {23617841},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/HQLEM974/Hardcastle and Kelly - 2013 - Empirical Bayesian analysis of paired high-through.pdf},
  keywords = {Bayes Theorem,Binomial Distribution,Carcinoma; Squamous Cell,Gene Expression Profiling,Genomics,Head and Neck Neoplasms,High-Throughput Nucleotide Sequencing,Humans,Linear Models,Squamous Cell Carcinoma of Head and Neck},
  langid = {english},
  pmcid = {PMC3658937}
}

@article{hardcastleGeneralizedEmpiricalBayesian2016,
  title = {Generalized Empirical {{Bayesian}} Methods for Discovery of Differential Data in High-Throughput Biology},
  author = {Hardcastle, Thomas J.},
  date = {2016-01-15},
  journaltitle = {Bioinformatics (Oxford, England)},
  shortjournal = {Bioinformatics},
  volume = {32},
  pages = {195--202},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/btv569},
  abstract = {MOTIVATION: High-throughput data are now commonplace in biological research. Rapidly changing technologies and application mean that novel methods for detecting differential behaviour that account for a 'large P, small n' setting are required at an increasing rate. The development of such methods is, in general, being done on an ad hoc basis, requiring further development cycles and a lack of standardization between analyses. RESULTS: We present here a generalized method for identifying differential behaviour within high-throughput biological data through empirical Bayesian methods. This approach is based on our baySeq algorithm for identification of differential expression in RNA-seq data based on a negative binomial distribution, and in paired data based on a beta-binomial distribution. Here we show how the same empirical Bayesian approach can be applied to any parametric distribution, removing the need for lengthy development of novel methods for differently distributed data. Comparisons with existing methods developed to address specific problems in high-throughput biological data show that these generic methods can achieve equivalent or better performance. A number of enhancements to the basic algorithm are also presented to increase flexibility and reduce computational costs. AVAILABILITY AND IMPLEMENTATION: The methods are implemented in the R baySeq (v2) package, available on Bioconductor http://www.bioconductor.org/packages/release/bioc/html/baySeq.html. CONTACT: tjh48@cam.ac.uk SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
  eprint = {26428289},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/CR3W92VA/1744387.html},
  keywords = {Algorithms,Animals,Bayes Theorem,Binomial Distribution,Gene Expression Profiling,Rats,Sequence Analysis; RNA,Software},
  langid = {english},
  number = {2}
}

@article{heldProbabilisticForecastingInfectious2017,
  title = {Probabilistic Forecasting in Infectious Disease Epidemiology: The 13th {{Armitage}} Lecture},
  shorttitle = {Probabilistic Forecasting in Infectious Disease Epidemiology},
  author = {Held, Leonhard and Meyer, Sebastian and Bracher, Johannes},
  date = {2017},
  journaltitle = {Statistics in Medicine},
  volume = {36},
  pages = {3443--3460},
  issn = {1097-0258},
  doi = {10.1002/sim.7363},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7363},
  urldate = {2019-09-16},
  abstract = {Routine surveillance of notifiable infectious diseases gives rise to daily or weekly counts of reported cases stratified by region and age group. From a public health perspective, forecasts of infectious disease spread are of central importance. We argue that such forecasts need to properly incorporate the attached uncertainty, so they should be probabilistic in nature. However, forecasts also need to take into account temporal dependencies inherent to communicable diseases, spatial dynamics through human travel and social contact patterns between age groups. We describe a multivariate time series model for weekly surveillance counts on norovirus gastroenteritis from the 12 city districts of Berlin, in six age groups, from week 2011/27 to week 2015/26. The following year (2015/27 to 2016/26) is used to assess the quality of the predictions. Probabilistic forecasts of the total number of cases can be derived through Monte Carlo simulation, but first and second moments are also available analytically. Final size forecasts as well as multivariate forecasts of the total number of cases by age group, by district and by week are compared across different models of varying complexity. This leads to a more general discussion of issues regarding modelling, prediction and evaluation of public health surveillance data. Copyright © 2017 John Wiley \& Sons, Ltd.},
  file = {/mnt/data/Google Drive/Zotero/storage/DIJH7TNP/Held et al. - 2017 - Probabilistic forecasting in infectious disease ep.pdf},
  keywords = {age-structured contact matrix,endemic–epidemic modelling,multivariate probabilistic forecasting,proper scoring rules,spatio-temporal surveillance data},
  langid = {english},
  number = {22}
}

@article{heldProbabilisticForecastingInfectious2017a,
  title = {Probabilistic Forecasting in Infectious Disease Epidemiology: The 13th {{Armitage}} Lecture},
  shorttitle = {Probabilistic Forecasting in Infectious Disease Epidemiology},
  author = {Held, Leonhard and Meyer, Sebastian and Bracher, Johannes},
  date = {2017},
  journaltitle = {Statistics in Medicine},
  volume = {36},
  pages = {3443--3460},
  issn = {1097-0258},
  doi = {10.1002/sim.7363},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7363},
  urldate = {2019-09-16},
  abstract = {Routine surveillance of notifiable infectious diseases gives rise to daily or weekly counts of reported cases stratified by region and age group. From a public health perspective, forecasts of infectious disease spread are of central importance. We argue that such forecasts need to properly incorporate the attached uncertainty, so they should be probabilistic in nature. However, forecasts also need to take into account temporal dependencies inherent to communicable diseases, spatial dynamics through human travel and social contact patterns between age groups. We describe a multivariate time series model for weekly surveillance counts on norovirus gastroenteritis from the 12 city districts of Berlin, in six age groups, from week 2011/27 to week 2015/26. The following year (2015/27 to 2016/26) is used to assess the quality of the predictions. Probabilistic forecasts of the total number of cases can be derived through Monte Carlo simulation, but first and second moments are also available analytically. Final size forecasts as well as multivariate forecasts of the total number of cases by age group, by district and by week are compared across different models of varying complexity. This leads to a more general discussion of issues regarding modelling, prediction and evaluation of public health surveillance data. Copyright © 2017 John Wiley \& Sons, Ltd.},
  file = {/mnt/data/Google Drive/Zotero/storage/TR4PHLGF/sim.html},
  keywords = {age-structured contact matrix,endemic–epidemic modelling,multivariate probabilistic forecasting,proper scoring rules,spatio-temporal surveillance data},
  langid = {english},
  number = {22}
}

@article{heldProbabilisticForecastingInfectious2017b,
  title = {Probabilistic Forecasting in Infectious Disease Epidemiology: The 13th {{Armitage}} Lecture},
  shorttitle = {Probabilistic Forecasting in Infectious Disease Epidemiology},
  author = {Held, Leonhard and Meyer, Sebastian and Bracher, Johannes},
  date = {2017-09-30},
  journaltitle = {Statistics in Medicine},
  volume = {36},
  pages = {3443--3460},
  issn = {1097-0258},
  doi = {10.1002/sim.7363},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.7363},
  urldate = {2019-09-16},
  abstract = {Routine surveillance of notifiable infectious diseases gives rise to daily or weekly counts of reported cases stratified by region and age group. From a public health perspective, forecasts of infect...},
  file = {/mnt/data/Google Drive/Zotero/storage/WZARB3AM/sim.html},
  langid = {english},
  number = {22}
}

@article{heldProbabilisticForecastingInfectious2017c,
  title = {Probabilistic Forecasting in Infectious Disease Epidemiology: The 13th {{Armitage}} Lecture: {{L}}. {{HELD}}, {{S}}. {{MEYER AND J}}. {{BRACHER}}},
  shorttitle = {Probabilistic Forecasting in Infectious Disease Epidemiology},
  author = {Held, Leonhard and Meyer, Sebastian and Bracher, Johannes},
  date = {2017-09-30},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Statist. Med.},
  volume = {36},
  pages = {3443--3460},
  issn = {02776715},
  doi = {10.1002/sim.7363},
  url = {http://doi.wiley.com/10.1002/sim.7363},
  urldate = {2019-09-21},
  file = {/mnt/data/Google Drive/Zotero/storage/4GBAAE6J/Held et al. - 2017 - Probabilistic forecasting in infectious disease ep.pdf},
  langid = {english},
  number = {22}
}

@article{hellewellFeasibilityControllingCOVID192020,
  title = {Feasibility of Controlling {{COVID}}-19 Outbreaks by Isolation of Cases and Contacts},
  author = {Hellewell, Joel and Abbott, Sam and Gimma, Amy and Bosse, Nikos I. and Jarvis, Christopher I. and Russell, Timothy W. and Munday, James D. and Kucharski, Adam J. and Edmunds, W. John and Sun, Fiona and Flasche, Stefan and Quilty, Billy J. and Davies, Nicholas and Liu, Yang and Clifford, Samuel and Klepac, Petra and Jit, Mark and Diamond, Charlie and Gibbs, Hamish and van Zandvoort, Kevin and Funk, Sebastian and Eggo, Rosalind M.},
  date = {2020-04-01},
  journaltitle = {The Lancet Global Health},
  shortjournal = {The Lancet Global Health},
  volume = {8},
  pages = {e488-e496},
  issn = {2214-109X},
  doi = {10.1016/S2214-109X(20)30074-7},
  url = {https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(20)30074-7/abstract},
  urldate = {2020-04-12},
  abstract = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}h3{$>$}Background{$<$}/h3{$><$}p{$>$}Isolation of cases and contact tracing is used to control outbreaks of infectious diseases, and has been used for coronavirus disease 2019 (COVID-19). Whether this strategy will achieve control depends on characteristics of both the pathogen and the response. Here we use a mathematical model to assess if isolation and contact tracing are able to control onwards transmission from imported cases of COVID-19.{$<$}/p{$><$}h3{$>$}Methods{$<$}/h3{$><$}p{$>$}We developed a stochastic transmission model, parameterised to the COVID-19 outbreak. We used the model to quantify the potential effectiveness of contact tracing and isolation of cases at controlling a severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)-like pathogen. We considered scenarios that varied in the number of initial cases, the basic reproduction number (\emph{R}\textsubscript{0}), the delay from symptom onset to isolation, the probability that contacts were traced, the proportion of transmission that occurred before symptom onset, and the proportion of subclinical infections. We assumed isolation prevented all further transmission in the model. Outbreaks were deemed controlled if transmission ended within 12 weeks or before 5000 cases in total. We measured the success of controlling outbreaks using isolation and contact tracing, and quantified the weekly maximum number of cases traced to measure feasibility of public health effort.{$<$}/p{$><$}h3{$>$}Findings{$<$}/h3{$><$}p{$>$}Simulated outbreaks starting with five initial cases, an \emph{R}\textsubscript{0} of 1·5, and 0\% transmission before symptom onset could be controlled even with low contact tracing probability; however, the probability of controlling an outbreak decreased with the number of initial cases, when \emph{R}\textsubscript{0} was 2·5 or 3·5 and with more transmission before symptom onset. Across different initial numbers of cases, the majority of scenarios with an \emph{R}\textsubscript{0} of 1·5 were controllable with less than 50\% of contacts successfully traced. To control the majority of outbreaks, for \emph{R}\textsubscript{0} of 2·5 more than 70\% of contacts had to be traced, and for an \emph{R}\textsubscript{0} of 3·5 more than 90\% of contacts had to be traced. The delay between symptom onset and isolation had the largest role in determining whether an outbreak was controllable when \emph{R}\textsubscript{0} was 1·5. For \emph{R}\textsubscript{0} values of 2·5 or 3·5, if there were 40 initial cases, contact tracing and isolation were only potentially feasible when less than 1\% of transmission occurred before symptom onset.{$<$}/p{$><$}h3{$>$}Interpretation{$<$}/h3{$><$}p{$>$}In most scenarios, highly effective contact tracing and case isolation is enough to control a new outbreak of COVID-19 within 3 months. The probability of control decreases with long delays from symptom onset to isolation, fewer cases ascertained by contact tracing, and increasing transmission before symptoms. This model can be modified to reflect updated transmission characteristics and more specific definitions of outbreak control to assess the potential success of local response efforts.{$<$}/p{$><$}h3{$>$}Funding{$<$}/h3{$><$}p{$>$}Wellcome Trust, Global Challenges Research Fund, and Health Data Research UK.{$<$}/p{$>$}},
  eprint = {32119825},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/NNCP658M/Hellewell et al. - 2020 - Feasibility of controlling COVID-19 outbreaks by i.pdf;/mnt/data/Google Drive/Zotero/storage/YDUTTSX3/fulltext.html},
  langid = {english},
  number = {4}
}

@article{hersbachDecompositionContinuousRanked2000,
  title = {Decomposition of the {{Continuous Ranked Probability Score}} for {{Ensemble Prediction Systems}}},
  author = {Hersbach, Hans},
  date = {2000-10-01},
  journaltitle = {Weather and Forecasting},
  shortjournal = {Wea. Forecasting},
  volume = {15},
  pages = {559--570},
  publisher = {{American Meteorological Society}},
  issn = {0882-8156},
  doi = {10.1175/1520-0434(2000)015<0559:DOTCRP>2.0.CO;2},
  url = {https://journals.ametsoc.org/doi/10.1175/1520-0434%282000%29015%3C0559%3ADOTCRP%3E2.0.CO%3B2},
  urldate = {2020-05-12},
  abstract = {Some time ago, the continuous ranked probability score (CRPS) was proposed as a new verification tool for (probabilistic) forecast systems. Its focus is on the entire permissible range of a certain (weather) parameter. The CRPS can be seen as a ranked probability score with an infinite number of classes, each of zero width. Alternatively, it can be interpreted as the integral of the Brier score over all possible threshold values for the parameter under consideration. For a deterministic forecast system the CRPS reduces to the mean absolute error. In this paper it is shown that for an ensemble prediction system the CRPS can be decomposed into a reliability part and a resolution/uncertainty part, in a way that is similar to the decomposition of the Brier score. The reliability part of the CRPS is closely connected to the rank histogram of the ensemble, while the resolution/uncertainty part can be related to the average spread within the ensemble and the behavior of its outliers. The usefulness of such a decomposition is illustrated for the ensemble prediction system running at the European Centre for Medium-Range Weather Forecasts. The evaluation of the CRPS and its decomposition proposed in this paper can be extended to systems issuing continuous probability forecasts, by realizing that these can be interpreted as the limit of ensemble forecasts with an infinite number of members.},
  file = {/mnt/data/Google Drive/Zotero/storage/HB3ZPJY3/Hersbach - 2000 - Decomposition of the Continuous Ranked Probability.pdf;/mnt/data/Google Drive/Zotero/storage/R4LMBJDM/1520-0434(2000)0150559DOTCRP2.0.html},
  number = {5}
}

@article{herwartzStockReturnPrediction2017,
  title = {Stock Return Prediction under {{GARCH}} — {{An}} Empirical Assessment},
  author = {Herwartz, Helmut},
  date = {2017-07},
  journaltitle = {International Journal of Forecasting},
  shortjournal = {International Journal of Forecasting},
  volume = {33},
  pages = {569--580},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2017.01.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207017300079},
  urldate = {2019-08-27},
  abstract = {The GARCH model and its numerous variants have been widely applied in the financial literature and practice. For purposes of quasi maximum likelihood estimation, innovations to GARCH processes are typically assumed to be identically and independently distributed with mean zero and unit variance (strong GARCH). Under less restrictive assumptions (absence of unconditional correlation, weak GARCH), higher order dependence patterns might be exploited for ex-ante forecasting of GARCH innovations and, hence, stock returns. In this paper, rolling windows of empirical stock returns are subjected to testing independence of consecutive GARCH innovations. Rolling p-values from independence testing reflect time variation of serial dependence, and entail useful information to signal one step ahead directions of stock price changes. Ex-ante forecasting gains are documented for non-parametric innovation predictions, in particular, if the sign of innovation predictors is combined with independence diagnostics (p-values) and/or the sign of linear return forecasts.},
  file = {/mnt/data/Google Drive/Zotero/storage/UBBW7DJK/Herwartz - 2017 - Stock return prediction under GARCH — An empirical.pdf},
  langid = {english},
  number = {3}
}

@article{hoetingBayesianModelAveraging1999,
  title = {Bayesian {{Model Averaging}}: {{A Tutorial}}},
  shorttitle = {Bayesian {{Model Averaging}}},
  author = {Hoeting, Jennifer A. and Madigan, David and Raftery, Adrian E. and Volinsky, Chris T.},
  date = {1999},
  journaltitle = {Statistical Science},
  volume = {14},
  pages = {382--401},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-of-sample predictive performance. We also provide a catalogue of currently available BMA software.},
  eprint = {2676803},
  eprinttype = {jstor},
  number = {4}
}

@article{hogeBayesianModelWeighting2020,
  title = {Bayesian {{Model Weighting}}: {{The Many Faces}} of {{Model Averaging}}},
  shorttitle = {Bayesian {{Model Weighting}}},
  author = {Höge, Marvin and Guthke, Anneli and Nowak, Wolfgang},
  date = {2020-01-21},
  journaltitle = {Water},
  shortjournal = {Water},
  volume = {12},
  pages = {309},
  issn = {2073-4441},
  doi = {10.3390/w12020309},
  url = {https://www.mdpi.com/2073-4441/12/2/309},
  urldate = {2020-06-20},
  abstract = {Model averaging makes it possible to use multiple models for one modelling task, like predicting a certain quantity of interest. Several Bayesian approaches exist that all yield a weighted average of predictive distributions. However, often, they are not properly applied which can lead to false conclusions. In this study, we focus on Bayesian Model Selection (BMS) and Averaging (BMA), Pseudo-BMS/BMA and Bayesian Stacking. We want to foster their proper use by, first, clarifying their theoretical background and, second, contrasting their behaviours in an applied groundwater modelling task. We show that only Bayesian Stacking has the goal of model averaging for improved predictions by model combination. The other approaches pursue the quest of finding a single best model as the ultimate goal, and use model averaging only as a preliminary stage to prevent rash model choice. Improved predictions are thereby not guaranteed. In accordance with so-called M-settings that clarify the alleged relations between models and truth, we elicit which method is most promising.},
  file = {/mnt/data/Google Drive/Zotero/storage/4J7B4XSH/Höge et al. - 2020 - Bayesian Model Weighting The Many Faces of Model .pdf},
  langid = {english},
  number = {2}
}

@article{holmesAnalysisMultivariateTime,
  title = {Analysis of Multivariate Time- Series Using the {{MARSS}} Package},
  author = {Holmes, E E and Ward, E J and Scheuerell, M D},
  pages = {284},
  file = {/mnt/data/Google Drive/Zotero/storage/DLEKJXGW/Holmes et al. - Analysis of multivariate time- series using the MA.pdf},
  langid = {english}
}

@online{HowShapeWeakly,
  title = {How the {{Shape}} of a {{Weakly Informative Prior Affects Inferences}}},
  url = {https://mc-stan.org/users/documentation/case-studies/weakly_informative_shapes.html},
  urldate = {2019-10-22},
  file = {/mnt/data/Google Drive/Zotero/storage/D49TJY8H/weakly_informative_shapes.html}
}

@article{huangBiologicalFunctionsMicroRNAs2011,
  title = {Biological Functions of {{microRNAs}}: A Review},
  shorttitle = {Biological Functions of {{microRNAs}}},
  author = {Huang, Yong and Shen, Xing Jia and Zou, Quan and Wang, Sheng Peng and Tang, Shun Ming and Zhang, Guo Zheng},
  date = {2011-03-01},
  journaltitle = {Journal of Physiology and Biochemistry},
  shortjournal = {J Physiol Biochem},
  volume = {67},
  pages = {129--139},
  issn = {1877-8755},
  doi = {10.1007/s13105-010-0050-6},
  url = {https://doi.org/10.1007/s13105-010-0050-6},
  urldate = {2019-05-22},
  abstract = {MicroRNAs (miRNAs) are a recently discovered family of endogenous, noncoding RNA molecules approximately 22 nt in length. miRNAs modulate gene expression post-transcriptionally by binding to complementary sequences in the coding or 3′ untranslated region of target messenger RNAs (mRNAs). It is now clear that the biogenesis and function of miRNAs are related to the molecular mechanisms of various clinical diseases, and that they can potentially regulate every aspect of cellular activity, including differentiation and development, metabolism, proliferation, apoptotic cell death, viral infection and tumorgenesis. Here, we review recent advances in miRNA research, and discuss the diverse roles of miRNAs in disease.},
  file = {/mnt/data/Google Drive/Zotero/storage/FXH38JHG/Huang et al. - 2011 - Biological functions of microRNAs a review.pdf},
  keywords = {Biogenesis,Disease,Expression,Function,Keywords,MicroRNA},
  langid = {english},
  number = {1}
}

@article{huntleyReportingToolsAutomatedResult2013,
  title = {{{ReportingTools}}: An Automated Result Processing and Presentation Toolkit for High-Throughput Genomic Analyses},
  shorttitle = {{{ReportingTools}}},
  author = {Huntley, Melanie A. and Larson, Jessica L. and Chaivorapol, Christina and Becker, Gabriel and Lawrence, Michael and Hackney, Jason A. and Kaminker, Joshua S.},
  date = {2013-12-15},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {29},
  pages = {3220--3221},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btt551},
  url = {https://academic.oup.com/bioinformatics/article/29/24/3220/194666},
  urldate = {2019-04-18},
  abstract = {Abstract.  Summary: It is common for computational analyses to generate large amounts of complex data that are difficult to process and share with collaborators},
  file = {/mnt/data/Google Drive/Zotero/storage/UUQESTFR/Huntley et al. - 2013 - ReportingTools an automated result processing and.pdf;/mnt/data/Google Drive/Zotero/storage/9A7HNGQV/194666.html},
  langid = {english},
  number = {24}
}

@article{hyndmanAutomaticTimeSeries2008,
  title = {Automatic {{Time Series Forecasting}}: {{The}} {\textbf{Forecast}} {{Package}} for {{{\emph{R}}}}},
  shorttitle = {Automatic {{Time Series Forecasting}}},
  author = {Hyndman, Rob J. and Khandakar, Yeasmin},
  date = {2008},
  journaltitle = {Journal of Statistical Software},
  shortjournal = {J. Stat. Soft.},
  volume = {27},
  issn = {1548-7660},
  doi = {10.18637/jss.v027.i03},
  url = {http://www.jstatsoft.org/v27/i03/},
  urldate = {2020-03-30},
  file = {/mnt/data/Google Drive/Zotero/storage/J8VLZS83/Hyndman and Khandakar - 2008 - Automatic Time Series Forecasting The bforecast.pdf},
  langid = {english},
  number = {3}
}

@book{hyndmanrobjForecastingPrinciplesPractice2019,
  title = {Forecasting: {{Principles}} and {{Practice}}},
  shorttitle = {Forecasting},
  author = {{Hyndman, Rob J} and {Athanasopoulos, George}},
  date = {2019},
  url = {https://Otexts.com/fpp3/},
  urldate = {2020-08-21},
  abstract = {3rd edition},
  file = {/mnt/data/Google Drive/Zotero/storage/ZWX4B54Y/fpp3.html}
}

@article{IBMSPSSAdvanced,
  title = {IBM SPSS Advanced Statistics 25},
  pages = {134},
  file = {/mnt/data/Google Drive/Zotero/storage/R69D2Z8Q/IBM SPSS Advanced Statistics 25.pdf},
  langid = {german}
}

@article{ibrahimPowerPriorTheory2015,
  title = {The Power Prior: Theory and Applications},
  shorttitle = {The Power Prior},
  author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Gwon, Yeongjin and Chen, Fang},
  date = {2015-12-10},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Stat Med},
  volume = {34},
  pages = {3724--3749},
  issn = {1097-0258},
  doi = {10.1002/sim.6728},
  abstract = {The power prior has been widely used in many applications covering a large number of disciplines. The power prior is intended to be an informative prior constructed from historical data. It has been used in clinical trials, genetics, health care, psychology, environmental health, engineering, economics, and business. It has also been applied for a wide variety of models and settings, both in the experimental design and analysis contexts. In this review article, we give an A-to-Z exposition of the power prior and its applications to date. We review its theoretical properties, variations in its formulation, statistical contexts for which it has been used, applications, and its advantages over other informative priors. We review models for which it has been used, including generalized linear models, survival models, and random effects models. Statistical areas where the power prior has been used include model selection, experimental design, hierarchical modeling, and conjugate priors. Frequentist properties of power priors in posterior inference are established, and a simulation study is conducted to further examine the empirical performance of the posterior estimates with power priors. Real data analyses are given illustrating the power prior as well as the use of the power prior in the Bayesian design of clinical trials.},
  eprint = {26346180},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/86JFHRSQ/Ibrahim et al. - 2015 - The power prior theory and applications.pdf},
  keywords = {Bayes Theorem,Bayesian design,borrowing,clinical trials,Clinical Trials as Topic,discounting,historical data,Historically Controlled Study,informative prior,Linear Models,Models; Statistical,Research Design,Statistics as Topic},
  langid = {english},
  number = {28},
  pmcid = {PMC4626399}
}

@article{ignatiadisDatadrivenHypothesisWeighting2016,
  title = {Data-Driven Hypothesis Weighting Increases Detection Power in Genome-Scale Multiple Testing},
  author = {Ignatiadis, Nikolaos and Klaus, Bernd and Zaugg, Judith B. and Huber, Wolfgang},
  date = {2016-07},
  journaltitle = {Nature Methods},
  volume = {13},
  pages = {577--580},
  issn = {1548-7105},
  doi = {10.1038/nmeth.3885},
  url = {https://www.nature.com/articles/nmeth.3885},
  urldate = {2019-04-16},
  abstract = {Hypothesis weighting improves the power of large-scale multiple testing. We describe independent hypothesis weighting (IHW), a method that assigns weights using covariates independent of the P-values under the null hypothesis but informative of each test's power or prior probability of the null hypothesis (http://www.bioconductor.org/packages/IHW). IHW increases power while controlling the false discovery rate and is a practical approach to discovering associations in genomics, high-throughput biology and other large data sets.},
  file = {/mnt/data/Google Drive/Zotero/storage/W4R7AQY3/Ignatiadis et al. - 2016 - Data-driven hypothesis weighting increases detecti.pdf;/mnt/data/Google Drive/Zotero/storage/VWBZVDZT/nmeth.html},
  langid = {english},
  number = {7}
}

@article{jagannathanTranslationalRegulationMitochondrial2015,
  title = {Translational {{Regulation}} of the {{Mitochondrial Genome Following Redistribution}} of {{Mitochondrial MicroRNA}} ({{MitomiR}}) in the {{Diabetic Heart}}},
  author = {Jagannathan, Rajaganapathi and Thapa, Dharendra and Nichols, Cody E. and Shepherd, Danielle L. and Stricker, Janelle C. and Croston, Tara L. and Baseler, Walter A. and Lewis, Sara E. and Martinez, Ivan and Hollander, John M.},
  date = {2015-12},
  journaltitle = {Circulation. Cardiovascular genetics},
  shortjournal = {Circ Cardiovasc Genet},
  volume = {8},
  pages = {785--802},
  issn = {1942-325X},
  doi = {10.1161/CIRCGENETICS.115.001067},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4681669/},
  urldate = {2018-09-12},
  abstract = {Background Cardiomyocytes are rich in mitochondria which are situated in spatially-distinct subcellular regions including those under the plasma membrane, subsarcolemmal mitochondria; and those between the myofibrils, interfibrillar mitochondria. We previously observed subpopulation-specific differences in mitochondrial proteomes following diabetic insult. The objective of this study was to determine whether mitochondrial genome-encoded proteins are regulated by microRNAs inside the mitochondrion and whether subcellular spatial location or diabetes mellitus influences the dynamics. Methods and Results Using microarray technology coupled with cross-linking immunoprecipitation and next generation sequencing, we identified a pool of mitochondrial microRNAs, termed mitomiRs that are redistributed in spatially-distinct mitochondrial subpopulations in an inverse manner following diabetic insult. Redistributed mitomiRs displayed distinct interactions with the mitochondrial genome requiring specific stoichiometric associations with RISC constituents argonaute-2 (Ago2) and fragile X mental retardation–related protein 1 (FXR1) for translational regulation. In the presence of Ago2 and FXR1, redistribution of mitomiR-378 to the IFM following diabetic insult led to down regulation of mitochondrially-encoded F0 component ATP6. Next generation sequencing analyses identified specific transcriptome and mitomiR sequences associated with ATP6 regulation. Overexpression of mitomiR-378 in HL-1 cells resulted in its accumulation in the mitochondrion and down-regulation of functional ATP6 protein, while antagomir blockade restored functional ATP6 protein and cardiac pump function. Conclusions We propose mitomiRs can translationally regulate mitochondrially-encoded proteins in spatially-distinct mitochondrial subpopulations during diabetes mellitus. The results reveal the requirement of RISC constituents in the mitochondrion for functional mitomiR translational regulation and provide a connecting link between diabetic insult and ATP synthase function.},
  eprint = {26377859},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/PD5SKYZT/Jagannathan et al. - 2015 - Translational Regulation of the Mitochondrial Geno.pdf},
  number = {6},
  pmcid = {PMC4681669}
}

@inproceedings{jainStructuralRNNDeepLearning2016,
  title = {Structural-{{RNN}}: {{Deep Learning}} on {{Spatio}}-{{Temporal Graphs}}},
  shorttitle = {Structural-{{RNN}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Jain, Ashesh and Zamir, Amir R. and Savarese, Silvio and Saxena, Ashutosh},
  date = {2016-06},
  pages = {5308--5317},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.573},
  url = {http://ieeexplore.ieee.org/document/7780942/},
  urldate = {2019-09-19},
  abstract = {Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can benefit from it. Spatiotemporal graphs are a popular tool for imposing such highlevel intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks (RNNs). We develop a scalable method for casting an arbitrary spatiotemporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well defined steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {/mnt/data/Google Drive/Zotero/storage/4B2UEDPF/Jain et al. - 2016 - Structural-RNN Deep Learning on Spatio-Temporal G.pdf;/mnt/data/Google Drive/Zotero/storage/PGL5S7MA/2019.12.27.889212v1.full.pdf},
  isbn = {978-1-4673-8851-1},
  langid = {english}
}

@inproceedings{jainStructuralRNNDeepLearning2016a,
  title = {Structural-{{RNN}}: {{Deep Learning}} on {{Spatio}}-{{Temporal Graphs}}},
  shorttitle = {Structural-{{RNN}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Jain, Ashesh and Zamir, Amir R. and Savarese, Silvio and Saxena, Ashutosh},
  date = {2016-06},
  pages = {5308--5317},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.573},
  url = {http://ieeexplore.ieee.org/document/7780942/},
  urldate = {2019-09-19},
  abstract = {Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can benefit from it. Spatiotemporal graphs are a popular tool for imposing such highlevel intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks (RNNs). We develop a scalable method for casting an arbitrary spatiotemporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well defined steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {/mnt/data/Google Drive/Zotero/storage/NHME8GMH/Jain et al. - 2016 - Structural-RNN Deep Learning on Spatio-Temporal G.pdf},
  isbn = {978-1-4673-8851-1},
  keywords = {not relevant},
  langid = {english}
}

@book{jamesIntroductionStatisticalLearning2013,
  title = {An Introduction to Statistical Learning: With Applications in {{R}}},
  shorttitle = {An Introduction to Statistical Learning},
  editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  date = {2013},
  publisher = {{Springer}},
  location = {{New York}},
  annotation = {OCLC: ocn828488009},
  file = {/mnt/data/Google Drive/Zotero/storage/4KUB43ZS/James et al. - 2013 - An introduction to statistical learning with appl.pdf},
  isbn = {978-1-4614-7137-0},
  keywords = {Mathematical models,Mathematical statistics,Problems; exercises; etc,R (Computer program language),Statistics},
  langid = {english},
  number = {103},
  pagetotal = {426},
  series = {Springer Texts in Statistics}
}

@book{jamesIntroductionStatisticalLearning2013a,
  title = {An Introduction to Statistical Learning: With Applications in {{R}}},
  shorttitle = {An Introduction to Statistical Learning},
  editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  date = {2013},
  publisher = {{Springer}},
  location = {{New York}},
  annotation = {OCLC: ocn828488009},
  file = {/mnt/data/Google Drive/Zotero/storage/ZYJAHZCY/James et al. - 2013 - An introduction to statistical learning with appl.pdf},
  isbn = {978-1-4614-7137-0},
  keywords = {Mathematical models,Mathematical statistics,Problems; exercises; etc,R (Computer program language),Statistics},
  langid = {english},
  number = {103},
  pagetotal = {426},
  series = {Springer Texts in Statistics}
}

@report{jarvisQuantifyingImpactPhysical2020,
  title = {Quantifying the Impact of Physical Distance Measures on the Transmission of {{COVID}}-19 in the {{UK}}},
  author = {Jarvis, Christopher I and Van Zandvoort, Kevin and Gimma, Amy and Prem, Kiesha and {CMMID COVID-19 working group} and Klepac, Petra and Rubin, G James and Edmunds, W John},
  date = {2020-04-03},
  institution = {{Epidemiology}},
  doi = {10.1101/2020.03.31.20049023},
  url = {http://medrxiv.org/lookup/doi/10.1101/2020.03.31.20049023},
  urldate = {2020-04-12},
  abstract = {Background:  To mitigate and slow the spread of COVID-19, many countries have adopted unprecedented physical distancing policies, including the UK. We evaluate whether these measures might be sufficient to control the epidemic by estimating their impact on the reproduction number (R0, the average number of secondary cases generated per case). Methods:  We asked a representative sample of UK adults about their contact patterns on the previous day. The questionnaire documents the age and location of contacts and as well as a measure of their intimacy (whether physical contact was made or not). In addition, we asked about adherence to different physical distancing measures. The first surveys were sent on Tuesday 24th March, one day after a “lockdown” was implemented across the UK. We compared measured contact patterns during the lockdown to patterns of social contact made during a non-epidemic period. By comparing these, we estimated the change in reproduction number as a consequence of the physical distancing measures imposed. We used a meta-analysis of published estimates to inform our estimates of the reproduction number before interventions were put in place. Findings:  We found a 73\% reduction in the average daily number of contacts observed per participant (from 10.2 to 2.9). This would be sufficient to reduce R0 from 2.6 prior to lockdown to 0.62 (95\% confidence interval [CI] 0.37 - 0.89) after the lockdown, based on all types of contact and 0.37 (95\% CI = 0.22 - 0.53) for physical contacts only. Interpretation: The physical distancing measures adopted by the UK public have substantially reduced contact levels and will likely lead to a substantial impact and a decline in cases in the coming weeks. However, this projected decline in incidence will not occur immediately as there are significant delays between infection, the onset of symptomatic disease and hospitalisation, as well as further delays to these events being reported. Tracking behavioural change can give a more rapid assessment of the impact of physical distancing measures than routine epidemiological surveillance.},
  file = {/mnt/data/Google Drive/Zotero/storage/LSE79PJL/Jarvis et al. - 2020 - Quantifying the impact of physical distance measur.pdf},
  langid = {english},
  type = {preprint}
}

@article{jaskiewiczArgonauteCLIPMethod2012,
  title = {Argonaute {{CLIP}} – {{A}} Method to Identify in Vivo Targets of {{miRNAs}}},
  author = {Jaskiewicz, Lukasz and Bilen, Biter and Hausser, Jean and Zavolan, Mihaela},
  date = {2012-10},
  journaltitle = {Methods},
  shortjournal = {Methods},
  volume = {58},
  pages = {106--112},
  issn = {10462023},
  doi = {10.1016/j.ymeth.2012.09.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1046202312002460},
  urldate = {2019-09-21},
  file = {/mnt/data/Google Drive/Zotero/storage/G3WF2A6I/Jaskiewicz et al. - 2012 - Argonaute CLIP – A method to identify in vivo targ.pdf},
  langid = {english},
  number = {2}
}

@article{jegadeeshReturnsBuyingWinners1993,
  title = {Returns to {{Buying Winners}} and {{Selling Losers}}: {{Implications}} for {{Stock Market Efficiency}}},
  shorttitle = {Returns to {{Buying Winners}} and {{Selling Losers}}},
  author = {Jegadeesh, Narasimhan and Titman, Sheridan},
  date = {1993-03},
  journaltitle = {The Journal of Finance},
  shortjournal = {The Journal of Finance},
  volume = {48},
  pages = {65--91},
  issn = {00221082},
  doi = {10.1111/j.1540-6261.1993.tb04702.x},
  url = {http://doi.wiley.com/10.1111/j.1540-6261.1993.tb04702.x},
  urldate = {2019-08-26},
  abstract = {This paper documentsthat strategies which buy stocks that have performedwell in the past and sell stocks that have performedpoorlyin the past generate significant positive returns over 3- to 12-month holding periods. We find that the profitability of these strategies are not due to their systematic risk or to delayed stock price reactions to common factors. However, part of the abnormal returns generated in the first year after portfolio formation dissipates in the following two years. A similar pattern of returns around the earnings announcements of past winners and losers is also documented.},
  file = {/mnt/data/Google Drive/Zotero/storage/TX45FB84/Jegadeesh and Titman - 1993 - Returns to Buying Winners and Selling Losers Impl.pdf},
  langid = {english},
  number = {1}
}

@article{johanssonOpenChallengeAdvance2019,
  title = {An Open Challenge to Advance Probabilistic Forecasting for Dengue Epidemics},
  author = {Johansson, Michael A. and Apfeldorf, Karyn M. and Dobson, Scott and Devita, Jason and Buczak, Anna L. and Baugher, Benjamin and Moniz, Linda J. and Bagley, Thomas and Babin, Steven M. and Guven, Erhan and Yamana, Teresa K. and Shaman, Jeffrey and Moschou, Terry and Lothian, Nick and Lane, Aaron and Osborne, Grant and Jiang, Gao and Brooks, Logan C. and Farrow, David C. and Hyun, Sangwon and Tibshirani, Ryan J. and Rosenfeld, Roni and Lessler, Justin and Reich, Nicholas G. and Cummings, Derek A. T. and Lauer, Stephen A. and Moore, Sean M. and Clapham, Hannah E. and Lowe, Rachel and Bailey, Trevor C. and García-Díez, Markel and Carvalho, Marilia Sá and Rodó, Xavier and Sardar, Tridip and Paul, Richard and Ray, Evan L. and Sakrejda, Krzysztof and Brown, Alexandria C. and Meng, Xi and Osoba, Osonde and Vardavas, Raffaele and Manheim, David and Moore, Melinda and Rao, Dhananjai M. and Porco, Travis C. and Ackley, Sarah and Liu, Fengchen and Worden, Lee and Convertino, Matteo and Liu, Yang and Reddy, Abraham and Ortiz, Eloy and Rivero, Jorge and Brito, Humberto and Juarrero, Alicia and Johnson, Leah R. and Gramacy, Robert B. and Cohen, Jeremy M. and Mordecai, Erin A. and Murdock, Courtney C. and Rohr, Jason R. and Ryan, Sadie J. and Stewart-Ibarra, Anna M. and Weikel, Daniel P. and Jutla, Antarpreet and Khan, Rakibul and Poultney, Marissa and Colwell, Rita R. and Rivera-García, Brenda and Barker, Christopher M. and Bell, Jesse E. and Biggerstaff, Matthew and Swerdlow, David and Mier-y-Teran-Romero, Luis and Forshey, Brett M. and Trtanj, Juli and Asher, Jason and Clay, Matt and Margolis, Harold S. and Hebbeler, Andrew M. and George, Dylan and Chretien, Jean-Paul},
  date = {2019-11-26},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {116},
  pages = {24268--24274},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1909865116},
  url = {https://www.pnas.org/content/116/48/24268},
  urldate = {2021-05-30},
  abstract = {A wide range of research has promised new tools for forecasting infectious disease dynamics, but little of that research is currently being applied in practice, because tools do not address key public health needs, do not produce probabilistic forecasts, have not been evaluated on external data, or do not provide sufficient forecast skill to be useful. We developed an open collaborative forecasting challenge to assess probabilistic forecasts for seasonal epidemics of dengue, a major global public health problem. Sixteen teams used a variety of methods and data to generate forecasts for 3 epidemiological targets (peak incidence, the week of the peak, and total incidence) over 8 dengue seasons in Iquitos, Peru and San Juan, Puerto Rico. Forecast skill was highly variable across teams and targets. While numerous forecasts showed high skill for midseason situational awareness, early season skill was low, and skill was generally lowest for high incidence seasons, those for which forecasts would be most valuable. A comparison of modeling approaches revealed that average forecast skill was lower for models including biologically meaningful data and mechanisms and that both multimodel and multiteam ensemble forecasts consistently outperformed individual model forecasts. Leveraging these insights, data, and the forecasting framework will be critical to improve forecast skill and the application of forecasts in real time for epidemic preparedness and response. Moreover, key components of this project—integration with public health needs, a common forecasting framework, shared and standardized data, and open participation—can help advance infectious disease forecasting beyond dengue.},
  eprint = {31712420},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/EZME2294/Johansson et al. - 2019 - An open challenge to advance probabilistic forecas.pdf;/mnt/data/Google Drive/Zotero/storage/4QECBFPX/24268.html},
  keywords = {dengue,epidemic,forecast,Peru,Puerto Rico},
  langid = {english},
  number = {48}
}

@article{jombartCostInsecurityFlareup2020,
  title = {The Cost of Insecurity: From Flare-up to Control of a Major {{Ebola}} Virus Disease Hotspot during the Outbreak in the {{Democratic Republic}} of the {{Congo}}, 2019},
  shorttitle = {The Cost of Insecurity},
  author = {Jombart, Thibaut and Jarvis, Christopher I. and Mesfin, Samuel and Tabal, Nabil and Mossoko, Mathias and Mpia, Luigino Minikulu and Abedi, Aaron Aruna and Chene, Sonia and Forbin, Ekokobe Elias and Belizaire, Marie Roseline D. and de Radiguès, Xavier and Ngombo, Richy and Tutu, Yannick and Finger, Flavio and Crowe, Madeleine and Edmunds, W. John and Nsio, Justus and Yam, Abdoulaye and Diallo, Boubacar and Gueye, Abdou Salam and Ahuka-Mundeke, Steve and Yao, Michel and Fall, Ibrahima Socé},
  date = {2020-01-16},
  journaltitle = {Eurosurveillance},
  volume = {25},
  pages = {1900735},
  issn = {1560-7917},
  doi = {10.2807/1560-7917.ES.2020.25.2.1900735},
  url = {https://www.eurosurveillance.org/content/10.2807/1560-7917.ES.2020.25.2.1900735},
  urldate = {2020-01-28},
  abstract = {The ongoing Ebola outbreak in the eastern Democratic Republic of the Congo is facing unprecedented levels of insecurity and violence. We evaluate the likely impact in terms of added transmissibility and cases of major security incidents in the Butembo coordination hub. We also show that despite this additional burden, an adapted response strategy involving enlarged ring vaccination around clusters of cases and enhanced community engagement managed to bring this main hotspot under control.},
  file = {/mnt/data/Google Drive/Zotero/storage/PZWFGHLK/Jombart et al. - 2020 - The cost of insecurity from flare-up to control o.pdf;/mnt/data/Google Drive/Zotero/storage/U2AUSPSS/1560-7917.ES.2020.25.2.html},
  langid = {english},
  number = {2}
}

@article{jordanEvaluatingProbabilisticForecasts2019,
  title = {Evaluating {{Probabilistic Forecasts}} with {{{\textbf{scoringRules}}}}},
  author = {Jordan, Alexander and Krüger, Fabian and Lerch, Sebastian},
  date = {2019},
  journaltitle = {Journal of Statistical Software},
  shortjournal = {J. Stat. Soft.},
  volume = {90},
  issn = {1548-7660},
  doi = {10.18637/jss.v090.i12},
  url = {http://www.jstatsoft.org/v90/i12/},
  urldate = {2020-02-13},
  abstract = {Probabilistic forecasts in the form of probability distributions over future events have become popular in several fields including meteorology, hydrology, economics, and demography. In typical applications, many alternative statistical models and data sources can be used to produce probabilistic forecasts. Hence, evaluating and selecting among competing methods is an important task. The scoringRules package for R provides functionality for comparative evaluation of probabilistic models based on proper scoring rules, covering a wide range of situations in applied work. This paper discusses implementation and usage details, presents case studies from meteorology and economics, and points to the relevant background literature.},
  file = {/mnt/data/Google Drive/Zotero/storage/DSYW6QUF/Jordan et al. - 2019 - Evaluating Probabilistic Forecasts with bscoring.pdf},
  langid = {english},
  number = {12}
}

@article{jordanEvaluatingProbabilisticForecasts2019a,
  title = {Evaluating {{Probabilistic Forecasts}} with {{{\textbf{scoringRules}}}}},
  author = {Jordan, Alexander and Krüger, Fabian and Lerch, Sebastian},
  date = {2019},
  journaltitle = {Journal of Statistical Software},
  shortjournal = {J. Stat. Soft.},
  volume = {90},
  issn = {1548-7660},
  doi = {10.18637/jss.v090.i12},
  url = {http://www.jstatsoft.org/v90/i12/},
  urldate = {2020-03-07},
  abstract = {Probabilistic forecasts in the form of probability distributions over future events have become popular in several fields including meteorology, hydrology, economics, and demography. In typical applications, many alternative statistical models and data sources can be used to produce probabilistic forecasts. Hence, evaluating and selecting among competing methods is an important task. The scoringRules package for R provides functionality for comparative evaluation of probabilistic models based on proper scoring rules, covering a wide range of situations in applied work. This paper discusses implementation and usage details, presents case studies from meteorology and economics, and points to the relevant background literature.},
  file = {/mnt/data/Google Drive/Zotero/storage/K5UPEP6Y/Jordan et al. - 2019 - Evaluating Probabilistic Forecasts with bscoring.pdf},
  langid = {english},
  number = {12}
}

@article{juulFixedtimeDescriptiveStatistics2020,
  title = {Fixed-Time Descriptive Statistics Underestimate Extremes of Epidemic Curve Ensembles},
  author = {Juul, Jonas L. and Græsbøll, Kaare and Christiansen, Lasse Engbo and Lehmann, Sune},
  date = {2020-12-08},
  journaltitle = {Nature Physics},
  pages = {1--4},
  publisher = {{Nature Publishing Group}},
  issn = {1745-2481},
  doi = {10.1038/s41567-020-01121-y},
  url = {https://www.nature.com/articles/s41567-020-01121-y},
  urldate = {2021-01-04},
  abstract = {The uncertainty associated with epidemic forecasts is often simulated with ensembles of epidemic trajectories based on combinations of parameters. We show that the standard approach for summarizing such ensembles systematically suppresses critical epidemiological information.},
  file = {/mnt/data/Google Drive/Zotero/storage/LI3L2VNA/Juul et al. - 2020 - Fixed-time descriptive statistics underestimate ex.pdf;/mnt/data/Google Drive/Zotero/storage/2D89TP3X/s41567-020-01121-y.html},
  langid = {english}
}

@article{kanamuraApplicationPairsTrading,
  title = {The {{Application}} of {{Pairs Trading}} to {{Energy Futures Markets}}},
  author = {Kanamura, Takashi and Rachev, Svetlozar T and Fabozzi, Frank J},
  pages = {39},
  abstract = {This paper investigates the usefulness of a hedge fund trading strategy known as “pairs trading” applied to energy futures markets. The profit of a simplified pairs trading strategy is modeled by using a mean-reverting process of the futures price spread. According to the comparative statics of the model, the strong mean-reversion and high volatility of the spread give rise to the high expected return from trading. Analyzing energy futures (more specifically, WTI crude oil, heating oil, and natural gas futures) traded on the New York Mercantile Exchange, we present empirical evidence that pairs trading can produce a relatively stable profit. We also identify the sources of the profit from a pair trading strategy, focusing on the characteristics of energy. The results suggest that natural gas futures trading may be more profitable than WTI crude oil and heating oil due to its strong mean-reversion and high volatility, while seasonality may also be relevant to the strategy’s profitability. Moreover, natural gas futures trading may be more vulnerable to event risk (as measured by price spikes) than the others. Finally, we investigate the profitability of cross commodities pairs trading.},
  file = {/mnt/data/Google Drive/Zotero/storage/L4HZPQQN/Kanamura et al. - The Application of Pairs Trading to Energy Futures.pdf},
  langid = {english}
}

@online{KarrierenBeiDeloitte,
  title = {Karrieren Bei {{Deloitte}}},
  url = {https://jobs.deloitte.de/search/?q=&optionsFacetsDD_facility=&optionsFacetsDD_shifttype==},
  urldate = {2019-08-15},
  file = {/mnt/data/Google Drive/Zotero/storage/U364IZ5W/search.html}
}

@article{keelinMetalogDistributions2016,
  title = {The {{Metalog Distributions}}},
  author = {Keelin, Thomas W.},
  date = {2016-12},
  journaltitle = {Decision Analysis},
  shortjournal = {Decision Analysis},
  volume = {13},
  pages = {243--277},
  issn = {1545-8490, 1545-8504},
  doi = {10.1287/deca.2016.0338},
  url = {http://pubsonline.informs.org/doi/10.1287/deca.2016.0338},
  urldate = {2019-11-06},
  abstract = {The metalog distributions constitute a new system of continuous univariate probability distributions designed for flexibility, simplicity, and ease/speed of use in practice. The system is comprised of unbounded, semi-bounded, and bounded distributions, each of which offers nearly unlimited shape flexibility compared to Pearson, Johnson, and other traditional systems of distributions. Explicit shapeflexibility comparisons are provided. Unlike other distributions that require non-linear optimization for parameter estimation, the metalog quantile functions and PDFs have simple closed-form expressions that are quantile-parameterized linearly by CDF data. Applications in fish biology and hydrology show how metalogs may aid data and distribution research by imposing fewer shape constraints than other commonly used distributions. Applications in decision analysis show how the metalog system can be specified with three assessed quantiles, how it facilities Monte Carlo simulation, and how applying it aided an actual decision that would have been made wrongly based on commonly-used discrete methods.},
  file = {/mnt/data/Google Drive/Zotero/storage/VC6KP5LD/Keelin - 2016 - The Metalog Distributions.pdf},
  langid = {english},
  number = {4}
}

@article{keelinMetalogDistributions2016a,
  title = {The {{Metalog Distributions}}},
  author = {Keelin, Thomas W.},
  date = {2016-11-28},
  journaltitle = {Decision Analysis},
  shortjournal = {Decision Analysis},
  volume = {13},
  pages = {243--277},
  publisher = {{INFORMS}},
  issn = {1545-8490},
  doi = {10.1287/deca.2016.0338},
  url = {https://pubsonline.informs.org/doi/10.1287/deca.2016.0338},
  urldate = {2020-09-05},
  abstract = {The metalog distributions constitute a new system of continuous univariate probability distributions designed for flexibility, simplicity, and ease/speed of use in practice. The system is comprised of unbounded, semibounded, and bounded distributions, each of which offers nearly unlimited shape flexibility compared to previous systems of distributions. Explicit shape-flexibility comparisons are provided. Unlike other distributions that require nonlinear optimization for parameter estimation, the metalog quantile functions and probability density functions have simple closed-form expressions that are quantile parameterized linearly by cumulative-distribution-function data. Applications in fish biology and hydrology show how metalogs may aid data and distribution research by imposing fewer shape constraints than other commonly used distributions. Applications in decision analysis show how the metalog system can be specified with three assessed quantiles, how it facilities Monte Carlo simulation, and how applying it aided an actual decision that would have been made wrongly based on commonly used discrete methods.This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. You are free to download this work and share with others for any purpose, except commercially, if you distribute your contributions under the same license as the original, and you must attribute this work as “Decision Analysis. Copyright © 2016 The Author(s). https://doi.org/10.1287/deca.2016.0338, used under a Creative Commons Attribution License: https://creativecommons.org/licenses/by-nc-sa/4.0/.”},
  file = {/mnt/data/Google Drive/Zotero/storage/D9TF46KD/Keelin - 2016 - The Metalog Distributions.pdf;/mnt/data/Google Drive/Zotero/storage/AJNSY2VJ/deca.2016.html},
  number = {4}
}

@article{kharchenkoBayesianApproachSinglecell2014,
  title = {Bayesian Approach to Single-Cell Differential Expression Analysis},
  author = {Kharchenko, Peter V. and Silberstein, Lev and Scadden, David T.},
  date = {2014-07},
  journaltitle = {Nature Methods},
  volume = {11},
  pages = {740--742},
  issn = {1548-7105},
  doi = {10.1038/nmeth.2967},
  url = {https://www.nature.com/articles/nmeth.2967},
  urldate = {2019-05-03},
  abstract = {Single-cell data provide a means to dissect the composition of complex tissues and specialized cellular environments. However, the analysis of such measurements is complicated by high levels of technical noise and intrinsic biological variability. We describe a probabilistic model of expression-magnitude distortions typical of single-cell RNA-sequencing measurements, which enables detection of differential expression signatures and identification of subpopulations of cells in a way that is more tolerant of noise.},
  file = {/mnt/data/Google Drive/Zotero/storage/URQFX6DA/Kharchenko et al. - 2014 - Bayesian approach to single-cell differential expr.pdf;/mnt/data/Google Drive/Zotero/storage/J6ZBLML6/nmeth.html},
  langid = {english},
  number = {7}
}

@article{killickOptimalDetectionChangepoints2012,
  title = {Optimal Detection of Changepoints with a Linear Computational Cost},
  author = {Killick, R. and Fearnhead, P. and Eckley, I. A.},
  date = {2012-12},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {107},
  pages = {1590--1598},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2012.737745},
  url = {http://arxiv.org/abs/1101.1438},
  urldate = {2020-04-03},
  abstract = {We consider the problem of detecting multiple changepoints in large data sets. Our focus is on applications where the number of changepoints will increase as we collect more data: for example in genetics as we analyse larger regions of the genome, or in finance as we observe time-series over longer periods. We consider the common approach of detecting changepoints through minimising a cost function over possible numbers and locations of changepoints. This includes several established procedures for detecting changing points, such as penalised likelihood and minimum description length. We introduce a new ∗R. Killick is Senior Research Associate, Department of Mathematics \& Statistics, Lancaster University, Lancaster, UK (E-mail: r.killick@lancs.ac.uk). P. Fearnhead is Professor, Department of Mathematics \& Statistics, Lancaster University, Lancaster, UK (E-mail: p.fearnhead@lancs.ac.uk). I.A. Eckley is Senior Lecturer, Department of Mathematics \& Statistics, Lancaster University, Lancaster, UK (E-mail: i.eckley@lancs.ac.uk). The authors are grateful to Richard Davis and Alice Cleynen for providing the Auto-PARM and PDPA software respectively. Part of this research was conducted whilst R. Killick was a jointly funded Engineering and Physical Sciences Research Council (EPSRC) / Shell Research Ltd graduate student at Lancaster University. Both I.A. Eckley and R. Killick also gratefully acknowledge the financial support of the EPSRC grant number EP/I016368/1.},
  archiveprefix = {arXiv},
  eprint = {1101.1438},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/NUKTI622/Killick et al. - 2012 - Optimal detection of changepoints with a linear co.pdf},
  keywords = {Quantitative Biology - Genomics,Quantitative Biology - Quantitative Methods,Statistics - Methodology},
  langid = {english},
  number = {500}
}

@article{kneibNikosBosseFelix,
  title = {Nikos {{Bosse Felix Süttmann}}},
  author = {Kneib, Thomas and Röder, Jan},
  pages = {56},
  file = {/mnt/data/Google Drive/Zotero/storage/LBNBMGPU/Kneib and Röder - Nikos Bosse Felix Süttmann.pdf},
  langid = {english}
}

@report{kneibPrimerBayesianDistributional2017,
  title = {A Primer on {{Bayesian}} Distributional Regression},
  author = {Kneib, Thomas and Umlauf, Nikolaus},
  date = {2017},
  institution = {{Working Papers in Economics and Statistics}},
  url = {https://www.econstor.eu/handle/10419/180164},
  urldate = {2019-09-16},
  abstract = {Bayesian methods have become increasingly popular in the past two decades. With the constant rise of computational power even very complex models can be estimated on virtually any modern computer. Moreover, interest has shifted from conditional mean models to probabilistic distributional models capturing location, scale, shape and other aspects of a response distribution, where covariate effects can have flexible forms, e.g., linear, nonlinear, spatial or random effects. This tutorial paper discusses how to select models in the Bayesian distributional regression setting, how to monitor convergence of the Markov chains, evaluate relevance of effects using simultaneous credible intervals and how to use simulation-based inference also for quantities derived from the original model parameterisation. We exemplify the work flow using daily weather data on (i) temperatures on Germany's highest mountain and (ii) extreme values of precipitation all over Germany.},
  file = {/mnt/data/Google Drive/Zotero/storage/UPCP2WD6/Kneib and Umlauf - 2017 - A primer on Bayesian distributional regression.pdf;/mnt/data/Google Drive/Zotero/storage/U2TEPYD2/180164.html},
  langid = {english},
  number = {2017-13},
  type = {Working Paper}
}

@article{kosinskiFacialRecognitionTechnology2021,
  title = {Facial Recognition Technology Can Expose Political Orientation from Naturalistic Facial Images},
  author = {Kosinski, Michal},
  date = {2021-01-11},
  journaltitle = {Scientific Reports},
  volume = {11},
  pages = {100},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-79310-1},
  url = {https://www.nature.com/articles/s41598-020-79310-1},
  urldate = {2021-03-07},
  abstract = {Ubiquitous facial recognition technology can expose individuals’ political orientation, as faces of liberals and conservatives consistently differ. A facial recognition algorithm was applied to naturalistic images of 1,085,795 individuals to predict their political orientation by comparing their similarity to faces of liberal and conservative others. Political orientation was correctly classified in 72\% of liberal–conservative face pairs, remarkably better than chance (50\%), human accuracy (55\%), or one afforded by a 100-item personality questionnaire (66\%). Accuracy was similar across countries (the U.S., Canada, and the UK), environments (Facebook and dating websites), and when comparing faces across samples. Accuracy remained high (69\%) even when controlling for age, gender, and ethnicity. Given the widespread use of facial recognition, our findings have critical implications for the protection of privacy and civil liberties.},
  file = {/mnt/data/Google Drive/Zotero/storage/ED39WE57/Kosinski - 2021 - Facial recognition technology can expose political.pdf;/mnt/data/Google Drive/Zotero/storage/99YLNPLX/s41598-020-79310-1.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{kraemerUtilizingGeneralHuman2019,
  title = {Utilizing General Human Movement Models to Predict the Spread of Emerging Infectious Diseases in Resource Poor Settings},
  author = {Kraemer, M. U. G. and Golding, N. and Bisanzio, D. and Bhatt, S. and Pigott, D. M. and Ray, S. E. and Brady, O. J. and Brownstein, J. S. and Faria, N. R. and Cummings, D. A. T. and Pybus, O. G. and Smith, D. L. and Tatem, A. J. and Hay, S. I. and Reiner, R. C.},
  date = {2019-12},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {9},
  pages = {5151},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-41192-3},
  url = {http://www.nature.com/articles/s41598-019-41192-3},
  urldate = {2019-09-16},
  file = {/mnt/data/Google Drive/Zotero/storage/Y98P356R/Kraemer et al. - 2019 - Utilizing general human movement models to predict.pdf},
  langid = {english},
  number = {1}
}

@article{krugerPredictiveInferenceBased,
  title = {Predictive {{Inference Based}} on {{Markov Chain Monte Carlo Output}}},
  author = {Krüger, Fabian and Lerch, Sebastian and Thorarinsdottir, Thordis and Gneiting, Tilmann},
  journaltitle = {International Statistical Review},
  volume = {n/a},
  issn = {1751-5823},
  doi = {10.1111/insr.12405},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12405},
  urldate = {2021-03-24},
  abstract = {In Bayesian inference, predictive distributions are typically in the form of samples generated via Markov chain Monte Carlo or related algorithms. In this paper, we conduct a systematic analysis of how to make and evaluate probabilistic forecasts from such simulation output. Based on proper scoring rules, we develop a notion of consistency that allows to assess the adequacy of methods for estimating the stationary distribution underlying the simulation output. We then provide asymptotic results that account for the salient features of Bayesian posterior simulators and derive conditions under which choices from the literature satisfy our notion of consistency. Importantly, these conditions depend on the scoring rule being used, such that the choices of approximation method and scoring rule are intertwined. While the logarithmic rule requires fairly stringent conditions, the continuous ranked probability score yields consistent approximations under minimal assumptions. These results are illustrated in a simulation study and an economic data example. Overall, mixture-of-parameters approximations that exploit the parametric structure of Bayesian models perform particularly well. Under the continuous ranked probability score, the empirical distribution function is a simple and appealing alternative option.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12405},
  file = {/mnt/data/Google Drive/Zotero/storage/BXU537BS/Krüger et al. - Predictive Inference Based on Markov Chain Monte C.pdf;/mnt/data/Google Drive/Zotero/storage/R9H5JU56/insr.html},
  keywords = {Bayesian methods,model evaluation,probabilistic forecasting,proper scoring rules},
  langid = {english},
  number = {n/a}
}

@book{kruschkeBayesianEstimationHierarchical2015,
  title = {Bayesian {{Estimation}} in {{Hierarchical Models}}},
  author = {Kruschke, John K. and Vanpaemel, Wolf},
  editor = {Busemeyer, Jerome R. and Wang, Zheng and Townsend, James T. and Eidels, Ami},
  date = {2015-12-10},
  volume = {1},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780199957996.013.13},
  url = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780199957996.001.0001/oxfordhb-9780199957996-e-13},
  urldate = {2019-04-05},
  abstract = {Bayesian data analysis involves describing data by meaningful mathematical models, and allocating credibility to parameter values that are consistent with the data and with prior knowledge. The Bayesian approach is ideally suited for constructing hierarchical models, which are useful for data structures with multiple levels, such as data from individuals who are members of groups which in turn are in higher-level organizations. Hierarchical models have parameters that meaningfully describe the data at their multiple levels and connect information within and across levels. Bayesian methods are very flexible and straightforward for estimating parameters of complex hierarchical models (and simpler models too). We provide an introduction to the ideas of hierarchical models and to the Bayesian estimation of their parameters, illustrated with two extended examples. One example considers baseball batting averages of individual players grouped by fielding position. A second example uses a hierarchical extension of a cognitive process model to examine individual differences in attention allocation of people who have eating disorders. We conclude by discussing Bayesian model comparison as a case of hierarchical modeling.},
  file = {/mnt/data/Google Drive/Zotero/storage/JGCUA954/Kruschke and Vanpaemel - 2015 - Bayesian Estimation in Hierarchical Models.pdf},
  langid = {english}
}

@article{kruschkeBayesianEstimationSupersedes2013,
  title = {Bayesian Estimation Supersedes the t Test.},
  author = {Kruschke, John K.},
  date = {2013},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {142},
  pages = {573--603},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0029146},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0029146},
  urldate = {2019-04-15},
  abstract = {Bayesian estimation for 2 groups provides complete distributions of credible values for the effect size, group means and their difference, standard deviations and their difference, and the normality of the data. The method handles outliers. The decision rule can accept the null value (unlike traditional t tests) when certainty in the estimate is high (unlike Bayesian model comparison using Bayes factors). The method also yields precise estimates of statistical power for various research goals. The software and programs are free and run on Macintosh, Windows, and Linux platforms.},
  file = {/mnt/data/Google Drive/Zotero/storage/FRBW2QBE/Kruschke - 2013 - Bayesian estimation supersedes the t test..pdf},
  langid = {english},
  number = {2}
}

@article{kruschkeBayesianEstimationSupersedes2013a,
  title = {Bayesian Estimation Supersedes the t Test.},
  author = {Kruschke, John K.},
  date = {2013},
  journaltitle = {Journal of Experimental Psychology: General},
  shortjournal = {Journal of Experimental Psychology: General},
  volume = {142},
  pages = {573--603},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0029146},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0029146},
  urldate = {2019-05-06},
  langid = {english},
  number = {2}
}

@article{kucharskiEffectivenessRingVaccination,
  title = {Effectiveness of {{Ring Vaccination}} as {{Control Strategy}} for {{Ebola Virus Disease}} - {{Volume}} 22, {{Number}} 1—{{January}} 2016 - {{Emerging Infectious Diseases}} Journal - {{CDC}}},
  author = {Kucharski, Adam J. and Eggo, Rosalind M. and Watson, Conall and Camacho, Anton and Funk, Sebastian and Edmunds, W. John},
  doi = {10.3201/eid2201.151410},
  url = {https://wwwnc.cdc.gov/eid/article/22/1/15-1410_article},
  urldate = {2020-01-27},
  abstract = {Using an Ebola virus disease transmission model, we found that addition of ring vaccination at the outset of the West Africa epidemic might not have l...},
  file = {/mnt/data/Google Drive/Zotero/storage/GABQRTK5/Kucharski et al. - Effectiveness of Ring Vaccination as Control Strat.pdf;/mnt/data/Google Drive/Zotero/storage/GKN58IHT/15-1410_article.html},
  langid = {american}
}

@article{lambertZeroInflatedPoissonRegression1992,
  title = {Zero-{{Inflated Poisson Regression}}, with an {{Application}} to {{Defects}} in {{Manufacturing}}},
  author = {Lambert, Diane},
  date = {1992},
  journaltitle = {Technometrics},
  volume = {34},
  pages = {1--14},
  issn = {0040-1706},
  doi = {10.2307/1269547},
  abstract = {[Zero-inflated Poisson (ZIP) regression is a model for count data with excess zeros. It assumes that with probability p the only possible observation is 0, and with probability 1 - p, a Poisson(λ) random variable is observed. For example, when manufacturing equipment is properly aligned, defects may be nearly impossible. But when it is misaligned, defects may occur according to a Poisson(λ) distribution. Both the probability p of the perfect, zero defect state and the mean number of defects λ in the imperfect state may depend on covariates. Sometimes p and λ are unrelated; other times p is a simple function of λ such as p=1/(1+λ \textsuperscript{τ}) for an unknown constant τ. In either case, ZIP regression models are easy to fit. The maximum likelihood estimates (MLE's) are approximately normal in large samples, and confidence intervals can be constructed by inverting likelihood ratio tests or using the approximate normality of the MLE's. Simulations suggest that the confidence intervals based on likelihood ratio tests are better, however. Finally, ZIP regression models are not only easy to interpret, but they can also lead to more refined data analyses. For example, in an experiment concerning soldering defects on printed wiring boards, two sets of conditions gave about the same mean number of defects, but the perfect state was more likely under one set of conditions and the mean number of defects in the imperfect state was smaller under the other set of conditions; that is, ZIP regression can show not only which conditions give lower mean number of defects but also why the means are lower.]},
  eprint = {1269547},
  eprinttype = {jstor},
  number = {1}
}

@article{lambertZeroInflatedPoissonRegression1992a,
  title = {Zero-{{Inflated Poisson Regression}}, {{With An Application}} to {{Defects}} in {{Manufacturing}}},
  author = {Lambert, Diane},
  date = {1992-02-01},
  journaltitle = {Technometrics},
  shortjournal = {Technometrics},
  volume = {34},
  pages = {1--14},
  doi = {10.1080/00401706.1992.10485228},
  abstract = {Zero-inflated Poisson (ZIP) regression is a model for count data with excess zeros. It assumes that with probability p the only possible observation is 0, and with probability 1 – p, a Poisson(λ) random variable is observed. For example, when manufacturing equipment is properly aligned, defects may be nearly impossible. But when it is misaligned, defects may occur according to a Poisson(λ) distribution. Both the probability p of the perfect, zero defect state and the mean number of defects λ in the imperfect state may depend on covariates. Sometimes p and λ are unrelated; other times p is a simple function of λ such as p = l/(1 + λ) for an unknown constant T. In either case, ZIP regression models are easy to fit. The maximum likelihood estimates (MLE's) are approximately normal in large samples, and confidence intervals can be constructed by inverting likelihood ratio tests or using the approximate normality of the MLE's. Simulations suggest that the confidence intervals based on likelihood ratio tests are better, however. Finally, ZIP regression models are not only easy to interpret, but they can also lead to more refined data analyses. For example, in an experiment concerning soldering defects on printed wiring boards, two sets of conditions gave about the same mean number of defects, but the perfect state was more likely under one set of conditions and the mean number of defects in the imperfect state was smaller under the other set of conditions; that is, ZIP regression can show not only which conditions give lower mean number of defects but also why the means are lower.},
  file = {/mnt/data/Google Drive/Zotero/storage/ZNF6FCZT/Lambert - 1992 - Zero-Inflated Poisson Regression, With An Applicat.pdf}
}

@software{leeGeoslegendDeepLearningforSpatiotemporalPrediction2019,
  title = {Geoslegend/{{Deep}}-{{Learning}}-for-{{Spatio}}-Temporal-{{Prediction}}},
  author = {Lee, Seongkyu},
  date = {2019-09-01T07:30:57Z},
  origdate = {2018-11-01T02:02:00Z},
  url = {https://github.com/geoslegend/Deep-Learning-for-Spatio-temporal-Prediction},
  urldate = {2019-09-19},
  abstract = {Contribute to geoslegend/Deep-Learning-for-Spatio-temporal-Prediction development by creating an account on GitHub.}
}

@software{leeGeoslegendDeepLearningforSpatiotemporalPrediction2020,
  title = {Geoslegend/{{Deep}}-{{Learning}}-for-{{Spatio}}-Temporal-{{Prediction}}},
  author = {Lee, Seongkyu},
  date = {2020-01-12T06:47:04Z},
  origdate = {2018-11-01T02:02:00Z},
  url = {https://github.com/geoslegend/Deep-Learning-for-Spatio-temporal-Prediction},
  urldate = {2020-01-13},
  abstract = {Contribute to geoslegend/Deep-Learning-for-Spatio-temporal-Prediction development by creating an account on GitHub.}
}

@article{leekSvaseqRemovingBatch2014,
  title = {Svaseq: Removing Batch Effects and Other Unwanted Noise from Sequencing Data},
  shorttitle = {Svaseq},
  author = {Leek, Jeffrey T.},
  date = {2014-12-01},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Res},
  volume = {42},
  pages = {e161},
  issn = {0305-1048},
  doi = {10.1093/nar/gku864},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4245966/},
  urldate = {2019-04-16},
  abstract = {It is now known that unwanted noise and unmodeled artifacts such as batch effects can dramatically reduce the accuracy of statistical inference in genomic experiments. These sources of noise must be modeled and removed to accurately measure biological variability and to obtain correct statistical inference when performing high-throughput genomic analysis. We introduced surrogate variable analysis (sva) for estimating these artifacts by (i) identifying the part of the genomic data only affected by artifacts and (ii) estimating the artifacts with principal components or singular vectors of the subset of the data matrix. The resulting estimates of artifacts can be used in subsequent analyses as adjustment factors to correct analyses. Here I describe a version of the sva approach specifically created for count data or FPKMs from sequencing experiments based on appropriate data transformation. I also describe the addition of supervised sva (ssva) for using control probes to identify the part of the genomic data only affected by artifacts. I present a comparison between these versions of sva and other methods for batch effect estimation on simulated data, real count-based data and FPKM-based data. These updates are available through the sva Bioconductor package and I have made fully reproducible analysis using these methods available from: https://github.com/jtleek/svaseq.},
  eprint = {25294822},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/HXAUTNR9/Leek - 2014 - svaseq removing batch effects and other unwanted .pdf},
  number = {21},
  pmcid = {PMC4245966}
}

@article{leekSvaseqRemovingBatch2014a,
  title = {Svaseq: Removing Batch Effects and Other Unwanted Noise from Sequencing Data},
  shorttitle = {Svaseq},
  author = {Leek, Jeffrey T.},
  date = {2014-12-01},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Res.},
  volume = {42},
  issn = {1362-4962},
  doi = {10.1093/nar/gku864},
  abstract = {It is now known that unwanted noise and unmodeled artifacts such as batch effects can dramatically reduce the accuracy of statistical inference in genomic experiments. These sources of noise must be modeled and removed to accurately measure biological variability and to obtain correct statistical inference when performing high-throughput genomic analysis. We introduced surrogate variable analysis (sva) for estimating these artifacts by (i) identifying the part of the genomic data only affected by artifacts and (ii) estimating the artifacts with principal components or singular vectors of the subset of the data matrix. The resulting estimates of artifacts can be used in subsequent analyses as adjustment factors to correct analyses. Here I describe a version of the sva approach specifically created for count data or FPKMs from sequencing experiments based on appropriate data transformation. I also describe the addition of supervised sva (ssva) for using control probes to identify the part of the genomic data only affected by artifacts. I present a comparison between these versions of sva and other methods for batch effect estimation on simulated data, real count-based data and FPKM-based data. These updates are available through the sva Bioconductor package and I have made fully reproducible analysis using these methods available from: https://github.com/jtleek/svaseq.},
  eprint = {25294822},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/MTE8EDKU/Leek - 2014 - svaseq removing batch effects and other unwanted .pdf},
  keywords = {Algorithms,Animals,Artifacts,Gene Expression Profiling,Genomics,High-Throughput Nucleotide Sequencing,Software,Zebrafish},
  langid = {english},
  number = {21},
  pmcid = {PMC4245966}
}

@article{leeLagrangeMultiplierTest1991,
  title = {A {{Lagrange}} Multiplier Test for {{GARCH}} Models},
  author = {Lee, John H. H.},
  date = {1991-11-01},
  journaltitle = {Economics Letters},
  shortjournal = {Economics Letters},
  volume = {37},
  pages = {265--271},
  issn = {0165-1765},
  doi = {10.1016/0165-1765(91)90221-6},
  url = {http://www.sciencedirect.com/science/article/pii/0165176591902216},
  urldate = {2019-09-13},
  abstract = {This paper extends the Lagrange multiplier (LM) test to testing noise disturbances against GARCH disturbances in the linear regression model. The resulting LM test for the GARCH alternative is identical to the LM test for an ARCH alternative},
  file = {/mnt/data/Google Drive/Zotero/storage/RB6X4696/0165176591902216.html},
  number = {3}
}

@article{leeLagrangeMultiplierTest1991a,
  title = {A {{Lagrange}} Multiplier Test for {{GARCH}} Models},
  author = {Lee, John H. H.},
  date = {1991-11-01},
  journaltitle = {Economics Letters},
  shortjournal = {Economics Letters},
  volume = {37},
  pages = {265--271},
  issn = {0165-1765},
  doi = {10.1016/0165-1765(91)90221-6},
  url = {http://www.sciencedirect.com/science/article/pii/0165176591902216},
  urldate = {2019-09-13},
  abstract = {This paper extends the Lagrange multiplier (LM) test to testing noise disturbances against GARCH disturbances in the linear regression model. The resulting LM test for the GARCH alternative is identical to the LM test for an ARCH alternative},
  file = {/mnt/data/Google Drive/Zotero/storage/HDT3EYZK/Lee - 1991 - A Lagrange multiplier test for GARCH models.pdf;/mnt/data/Google Drive/Zotero/storage/PUVV7FFW/0165176591902216.html},
  number = {3}
}

@report{lemaitreAssessingImpactNonpharmaceutical2020,
  title = {Assessing the Impact of Non-Pharmaceutical Interventions on {{SARS}}-{{CoV}}-2 Transmission in {{Switzerland}}},
  author = {Lemaitre, Joseph Chadi and Perez-Saez, Javier and Azman, Andrew and Rinaldo, Andrea and Fellay, Jacques},
  date = {2020-05-08},
  institution = {{Epidemiology}},
  doi = {10.1101/2020.05.04.20090639},
  url = {http://medrxiv.org/lookup/doi/10.1101/2020.05.04.20090639},
  urldate = {2020-05-13},
  abstract = {Following the rapid dissemination of COVID-19 cases in Switzerland, large-scale non-pharmaceutical interventions (NPIs) were implemented by the cantons and the federal government between February 28 and March 20. Estimates of the impact of these interventions on SARS-CoV-2 transmission are critical for decision making in this and future outbreaks. We here aim to assess the impact of these NPIs on disease transmission by estimating changes in the basic reproduction number (R0) at national and cantonal levels in relation to the timing of these NPIs. We estimate the time-varying R0 nationally and in twelve cantons by fitting a stochastic transmission model explicitly simulating within hospital dynamics. We use individual-level data of {$>$}1,000 hospitalized patients in Switzerland and public daily reports of hospitalizations and deaths. We estimate the national R0 was 3.15 (95\% CI: 2.13-3.76) at the start of the epidemic. Starting from around March 6, we find a strong reduction in R0 with a 85\% median decrease (95\% quantile range, QR: 83\%-90\%) to a value of 0.44 (95\% QR: 0.27-0.65) in the period of March 29-April 5. At the cantonal-level R0 decreased over the course of the epidemic between 71\% and 94\%. We found that reductions in R0 were synchronous with changes in mobility patterns as estimated through smartphone activity, which started before the official implementation of NPIs. We found that most of the reduction of transmission is due to behavioural changes as opposed to natural immunity, the latter accounting for only about 3\% of the total reduction in effective transmission. As Switzerland considers relaxing some of the restrictions of social mixing, current estimates of R0 well below one are promising. However most of inferred transmission reduction was due to behaviour change ({$<$}3\% due to natural immunity buildup), with an estimated 97\% (95\% QR: 96.6\%-97.2\%) of the Swiss population still susceptible to SARS-CoV-2 as of April 24. These results warrant a cautious relaxation of social distance practices and close monitoring of changes in both the basic and effective reproduction numbers.},
  file = {/mnt/data/Google Drive/Zotero/storage/63CW59X2/Lemaitre et al. - 2020 - Assessing the impact of non-pharmaceutical interve.pdf},
  langid = {english},
  type = {preprint}
}

@online{leon-noveloBayesianEstimationNegative2015,
  title = {Bayesian {{Estimation}} of {{Negative Binomial Parameters}} with {{Applications}} to {{RNA}}-{{Seq Data}}},
  author = {Leon-Novelo, Luis and Fuentes, Claudio and Emerson, Sarah},
  date = {2015-12-01},
  url = {http://arxiv.org/abs/1512.00475},
  urldate = {2019-03-26},
  abstract = {RNA-Seq data characteristically exhibits large variances, which need to be appropriately accounted for in the model. We first explore the effects of this variability on the maximum likelihood estimator (MLE) of the overdispersion parameter of the negative binomial distribution, and propose instead the use an estimator obtained via maximization of the marginal likelihood in a conjugate Bayesian framework. We show, via simulation studies, that the marginal MLE can better control this variation and produce a more stable and reliable estimator. We then formulate a conjugate Bayesian hierarchical model, in which the estimate of overdispersion is a marginalized estimate and use this estimator to propose a Bayesian test to detect differentially expressed genes with RNA-Seq data. We use numerical studies to show that our much simpler approach is competitive with other negative binomial based procedures, and we use a real data set to illustrate the implementation and flexibility of the procedure.},
  archiveprefix = {arXiv},
  eprint = {1512.00475},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/TEJH9NPP/Leon-Novelo et al. - 2015 - Bayesian Estimation of Negative Binomial Parameter.pdf;/mnt/data/Google Drive/Zotero/storage/2EUYYKYK/1512.html},
  keywords = {Statistics - Methodology},
  primaryclass = {stat}
}

@article{lerchForecasterDilemmaExtreme2017,
  title = {Forecaster’s {{Dilemma}}: {{Extreme Events}} and {{Forecast Evaluation}}},
  shorttitle = {Forecaster’s {{Dilemma}}},
  author = {Lerch, Sebastian and Thorarinsdottir, Thordis L. and Ravazzolo, Francesco and Gneiting, Tilmann},
  date = {2017-02},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {32},
  pages = {106--127},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/16-STS588},
  url = {https://projecteuclid.org/euclid.ss/1491465630},
  urldate = {2020-08-15},
  abstract = {In public discussions of the quality of forecasts, attention typically focuses on the predictive performance in cases of extreme events. However, the restriction of conventional forecast evaluation methods to subsets of extreme observations has unexpected and undesired effects, and is bound to discredit skillful forecasts when the signal-to-noise ratio in the data generating process is low. Conditioning on outcomes is incompatible with the theoretical assumptions of established forecast evaluation methods, thereby confronting forecasters with what we refer to as the forecaster’s dilemma. For probabilistic forecasts, proper weighted scoring rules have been proposed as decision-theoretically justifiable alternatives for forecast evaluation with an emphasis on extreme events. Using theoretical arguments, simulation experiments and a real data study on probabilistic forecasts of U.S. inflation and gross domestic product (GDP) growth, we illustrate and discuss the forecaster’s dilemma along with potential remedies.},
  file = {/mnt/data/Google Drive/Zotero/storage/6T79RWK7/Lerch et al. - 2017 - Forecaster’s Dilemma Extreme Events and Forecast .pdf;/mnt/data/Google Drive/Zotero/storage/GCBLD2K7/1491465630.html},
  keywords = {Diebold–Mariano test,hindsight bias,likelihood ratio test,Neyman–Pearson lemma,predictive performance,probabilistic forecast,proper weighted scoring rule,rare and extreme events},
  langid = {english},
  mrnumber = {MR3634309},
  number = {1},
  zmnumber = {06946266}
}

@article{lerchForecasterDilemmaExtreme2017a,
  title = {Forecaster’s {{Dilemma}}: {{Extreme Events}} and {{Forecast Evaluation}}},
  shorttitle = {Forecaster’s {{Dilemma}}},
  author = {Lerch, Sebastian and Thorarinsdottir, Thordis L. and Ravazzolo, Francesco and Gneiting, Tilmann},
  date = {2017-02},
  journaltitle = {Statistical Science},
  volume = {32},
  pages = {106--127},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/16-STS588},
  url = {https://projecteuclid.org/journals/statistical-science/volume-32/issue-1/Forecasters-Dilemma-Extreme-Events-and-Forecast-Evaluation/10.1214/16-STS588.full},
  urldate = {2021-04-08},
  abstract = {In public discussions of the quality of forecasts, attention typically focuses on the predictive performance in cases of extreme events. However, the restriction of conventional forecast evaluation methods to subsets of extreme observations has unexpected and undesired effects, and is bound to discredit skillful forecasts when the signal-to-noise ratio in the data generating process is low. Conditioning on outcomes is incompatible with the theoretical assumptions of established forecast evaluation methods, thereby confronting forecasters with what we refer to as the forecaster’s dilemma. For probabilistic forecasts, proper weighted scoring rules have been proposed as decision-theoretically justifiable alternatives for forecast evaluation with an emphasis on extreme events. Using theoretical arguments, simulation experiments and a real data study on probabilistic forecasts of U.S. inflation and gross domestic product (GDP) growth, we illustrate and discuss the forecaster’s dilemma along with potential remedies.},
  file = {/mnt/data/Google Drive/Zotero/storage/QWMU34VY/Lerch et al. - 2017 - Forecaster’s Dilemma Extreme Events and Forecast .pdf;/mnt/data/Google Drive/Zotero/storage/NW3BIDLL/16-STS588.html},
  keywords = {Diebold–Mariano test,hindsight bias,likelihood ratio test,Neyman–Pearson lemma,predictive performance,probabilistic forecast,proper weighted scoring rule,rare and extreme events},
  number = {1}
}

@article{lesslerMappingBurdenCholera2018,
  title = {Mapping the Burden of Cholera in Sub-{{Saharan Africa}} and Implications for Control: An Analysis of Data across Geographical Scales},
  shorttitle = {Mapping the Burden of Cholera in Sub-{{Saharan Africa}} and Implications for Control},
  author = {Lessler, Justin and Moore, Sean M. and Luquero, Francisco J. and McKay, Heather S. and Grais, Rebecca and Henkens, Myriam and Mengel, Martin and Dunoyer, Jessica and M'bangombe, Maurice and Lee, Elizabeth C. and Djingarey, Mamoudou Harouna and Sudre, Bertrand and Bompangue, Didier and Fraser, Robert S. M. and Abubakar, Abdinasir and Perea, William and Legros, Dominique and Azman, Andrew S.},
  date = {2018-05-12},
  journaltitle = {The Lancet},
  shortjournal = {The Lancet},
  volume = {391},
  pages = {1908--1915},
  issn = {0140-6736, 1474-547X},
  doi = {10.1016/S0140-6736(17)33050-7},
  url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(17)33050-7/abstract},
  urldate = {2019-10-29},
  abstract = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}h3{$>$}Background{$<$}/h3{$><$}p{$>$}Cholera remains a persistent health problem in sub-Saharan Africa and worldwide. Cholera can be controlled through appropriate water and sanitation, or by oral cholera vaccination, which provides transient (∼3 years) protection, although vaccine supplies remain scarce. We aimed to map cholera burden in sub-Saharan Africa and assess how geographical targeting could lead to more efficient interventions.{$<$}/p{$><$}h3{$>$}Methods{$<$}/h3{$><$}p{$>$}We combined information on cholera incidence in sub-Saharan Africa (excluding Djibouti and Eritrea) from 2010 to 2016 from datasets from WHO, Médecins Sans Frontières, ProMED, ReliefWeb, ministries of health, and the scientific literature. We divided the study region into 20 km × 20 km grid cells and modelled annual cholera incidence in each grid cell assuming a Poisson process adjusted for covariates and spatially correlated random effects. We combined these findings with data on population distribution to estimate the number of people living in areas of high cholera incidence ({$>$}1 case per 1000 people per year). We further estimated the reduction in cholera incidence that could be achieved by targeting cholera prevention and control interventions at areas of high cholera incidence.{$<$}/p{$><$}h3{$>$}Findings{$<$}/h3{$><$}p{$>$}We included 279 datasets covering 2283 locations in our analyses. In sub-Saharan Africa (excluding Djibouti and Eritrea), a mean of 141 918 cholera cases (95\% credible interval [CrI] 141 538–146 505) were reported per year. 4·0\% (95\% CrI 1·7–16·8) of districts, home to 87·2 million people (95\% CrI 60·3 million to 118·9 million), have high cholera incidence. By focusing on the highest incidence districts first, effective targeted interventions could eliminate 50\% of the region's cholera by covering 35·3 million people (95\% CrI 26·3 million to 62·0 million), which is less than 4\% of the total population.{$<$}/p{$><$}h3{$>$}Interpretation{$<$}/h3{$><$}p{$>$}Although cholera occurs throughout sub-Saharan Africa, its highest incidence is concentrated in a small proportion of the continent. Prioritising high-risk areas could substantially increase the efficiency of cholera control programmes.{$<$}/p{$><$}h3{$>$}Funding{$<$}/h3{$><$}p{$>$}The Bill \& Melinda Gates Foundation.{$<$}/p{$>$}},
  eprint = {29502905},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/R34ESHXT/Lessler et al. - 2018 - Mapping the burden of cholera in sub-Saharan Afric.pdf;/mnt/data/Google Drive/Zotero/storage/G9WXKBP6/fulltext.html},
  langid = {english},
  number = {10133}
}

@article{leutbecherEnsembleSizeHow2019,
  title = {Ensemble Size: {{How}} Suboptimal Is Less than Infinity?},
  shorttitle = {Ensemble Size},
  author = {Leutbecher, Martin},
  date = {2019},
  journaltitle = {Quarterly Journal of the Royal Meteorological Society},
  volume = {145},
  pages = {107--128},
  issn = {1477-870X},
  doi = {10.1002/qj.3387},
  url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.3387},
  urldate = {2021-07-05},
  abstract = {Ensemble forecasts are the method of choice in numerical weather prediction (NWP) to generate probabilistic forecasts. The number of members in an ensemble is an important factor in determining how well a probability distribution of a weather-related variable can be estimated. Having only a finite number of members reduces the average skill such a probabilistic forecast can have. Increasing ensemble size is therefore desirable; however, ensemble size is also proportional to the computational cost. Having a small ensemble size limits the cost and makes other improvements, such as increases in spatial resolution, feasible. This article examines how average skill measures with metrics such as the continuous ranked probability score, the quantile score, and the Dawid–Sebastiani score converge with ensemble size. A numerical experiment with a 200 member ensemble using the European Centre for Medium-Range Weather Forecasts (ECMWF) Integrated Forecasting System (IFS) model at a resolution of 29 km and a forecast range of 15 days provides data to compare the convergence of probabilistic skill in a current NWP system with theoretical expectations derived for perfectly reliable ensembles with exchangeable members. Results in the first part of the article can help users of operational NWP ensemble forecasts formulate their minimum requirement in terms of ensemble size. In the second part, requirements for scientists who test changes to NWP systems are examined. Using proper scores and fair scores, it is explored whether testing changes in the ensemble forecasts can be meaningful with fewer members than in the operational configuration. Results are based on medium-range numerical experiments with 50 members. Two experiments test the activation of a representation of model uncertainty and three other experiments test changes in horizontal resolution from 29 to 18 km and from 29 to 45 km.},
  annotation = {\_eprint: https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3387},
  file = {/mnt/data/Google Drive/Zotero/storage/5S7BT4E4/Leutbecher - 2019 - Ensemble size How suboptimal is less than infinit.pdf;/mnt/data/Google Drive/Zotero/storage/GNK8ZM9M/qj.html},
  keywords = {ensemble size,ensembles,fair score,forecast verification,forecasting,numerical weather prediction,proper scoring rule},
  langid = {english},
  number = {S1}
}

@article{lewinBayesianModellingDifferential,
  title = {Bayesian {{Modelling}} of {{Diﬀerential Gene Expression}}},
  author = {Lewin, Alex and Richardson, Sylvia and Marshall, Clare and Glazier, Anne and Aitman, Tim},
  pages = {28},
  file = {/mnt/data/Google Drive/Zotero/storage/Z8RSZVBM/Lewin et al. - Bayesian Modelling of Diﬀerential Gene Expression.pdf},
  langid = {english}
}

@article{liBayesianMixtureModel2017,
  title = {A {{Bayesian}} Mixture Model for Clustering and Selection of Feature Occurrence Rates under Mean Constraints: {{LI}} et Al.},
  shorttitle = {A {{Bayesian}} Mixture Model for Clustering and Selection of Feature Occurrence Rates under Mean Constraints},
  author = {Li, Qiwei and Guindani, Michele and Reich, Brian J. and Bondell, Howard D. and Vannucci, Marina},
  date = {2017-12},
  journaltitle = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  volume = {10},
  pages = {393--409},
  issn = {19321864},
  doi = {10.1002/sam.11350},
  url = {http://doi.wiley.com/10.1002/sam.11350},
  urldate = {2019-03-26},
  file = {/mnt/data/Google Drive/Zotero/storage/X7JDUHJL/Li et al. - 2017 - A Bayesian mixture model for clustering and select.pdf;/mnt/data/Google Drive/Zotero/storage/SG97HCMU/sam.html},
  langid = {english},
  number = {6}
}

@article{liBayesianMixtureModel2017a,
  title = {A {{Bayesian}} Mixture Model for Clustering and Selection of Feature Occurrence Rates under Mean Constraints: {{LI}} et Al.},
  shorttitle = {A {{Bayesian}} Mixture Model for Clustering and Selection of Feature Occurrence Rates under Mean Constraints},
  author = {Li, Qiwei and Guindani, Michele and Reich, Brian J. and Bondell, Howard D. and Vannucci, Marina},
  date = {2017-12},
  journaltitle = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  volume = {10},
  pages = {393--409},
  issn = {19321864},
  doi = {10.1002/sam.11350},
  url = {http://doi.wiley.com/10.1002/sam.11350},
  urldate = {2019-04-15},
  file = {/mnt/data/Google Drive/Zotero/storage/TZKGSGF9/Li et al. - 2017 - A Bayesian mixture model for clustering and select.pdf},
  langid = {english},
  number = {6}
}

@article{lindenUsingNegativeBinomial2011,
  title = {Using the Negative Binomial Distribution to Model Overdispersion in Ecological Count Data},
  author = {Lindén, Andreas and Mäntyniemi, Samu},
  date = {2011-07-01},
  journaltitle = {Ecology},
  shortjournal = {Ecology},
  volume = {92},
  pages = {1414--1421},
  issn = {0012-9658},
  doi = {10.1890/10-1831.1},
  url = {https://esajournals.onlinelibrary.wiley.com/doi/10.1890/10-1831.1},
  urldate = {2020-01-29},
  abstract = {A Poisson process is a commonly used starting point for modeling stochastic variation of ecological count data around a theoretical expectation. However, data typically show more variation than implied by the Poisson distribution. Such overdispersion is often accounted for by using models with different assumptions about how the variance changes with the expectation. The choice of these assumptions can naturally have apparent consequences for statistical inference. We propose a parameterization of the negative binomial distribution, where two overdispersion parameters are introduced to allow for various quadratic mean?variance relationships, including the ones assumed in the most commonly used approaches. Using bird migration as an example, we present hypothetical scenarios on how overdispersion can arise due to sampling, flocking behavior or aggregation, environmental variability, or combinations of these factors. For all considered scenarios, mean?variance relationships can be appropriately described by the negative binomial distribution with two overdispersion parameters. To illustrate, we apply the model to empirical migration data with a high level of overdispersion, gaining clearly different model fits with different assumptions about mean?variance relationships. The proposed framework can be a useful approximation for modeling marginal distributions of independent count data in likelihood-based analyses.},
  file = {/mnt/data/Google Drive/Zotero/storage/8ITCFURB/10-1831.html},
  keywords = {bird migration,count data,environmental stochasticity,flocking,generalized linear models,mean–variance relationship,negative binomial distribution,overdispersion,Poisson process,sampling error},
  number = {7}
}

@online{linkFittingBayesianStructural,
  title = {Fitting {{Bayesian}} Structural Time Series with the Bsts {{R}} Package},
  author = {{link}, Get and Facebook and Twitter and Pinterest and Email and Apps, Other},
  url = {http://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html},
  urldate = {2019-11-04},
  abstract = {by STEVEN L. SCOTT   Time series data are everywhere, but time series modeling is a fairly specialized area within statistics and data scien...},
  file = {/mnt/data/Google Drive/Zotero/storage/6I3IP6MX/fitting-bayesian-structural-time-series.html}
}

@online{linkFittingBayesianStructurala,
  title = {Fitting {{Bayesian}} Structural Time Series with the Bsts {{R}} Package},
  author = {{link}, Get and Facebook and Twitter and Pinterest and Email and Apps, Other},
  url = {http://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html},
  urldate = {2020-01-13},
  abstract = {by STEVEN L. SCOTT   Time series data are everywhere, but time series modeling is a fairly specialized area within statistics and data scien...},
  file = {/mnt/data/Google Drive/Zotero/storage/RI92BKFZ/fitting-bayesian-structural-time-series.html}
}

@article{liReviewStatisticalPostprocessing2017,
  title = {A Review on Statistical Postprocessing Methods for Hydrometeorological Ensemble Forecasting},
  author = {Li, Wentao and Duan, Qingyun and Miao, Chiyuan and Ye, Aizhong and Gong, Wei and Di, Zhenhua},
  date = {2017},
  journaltitle = {WIREs Water},
  volume = {4},
  pages = {e1246},
  issn = {2049-1948},
  doi = {10.1002/wat2.1246},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1246},
  urldate = {2020-04-07},
  abstract = {Computer simulation models have been widely used to generate hydrometeorological forecasts. As the raw forecasts contain uncertainties arising from various sources, including model inputs and outputs, model initial and boundary conditions, model structure, and model parameters, it is necessary to apply statistical postprocessing methods to quantify and reduce those uncertainties. Different postprocessing methods have been developed for meteorological forecasts (e.g., precipitation) and for hydrological forecasts (e.g., streamflow) due to their different statistical properties. In this paper, we conduct a comprehensive review of the commonly used statistical postprocessing methods for both meteorological and hydrological forecasts. Moreover, methods to generate ensemble members that maintain the observed spatiotemporal and intervariable dependency are reviewed. Finally, some perspectives on the further development of statistical postprocessing methods for hydrometeorological ensemble forecasting are provided. WIREs Water 2017, 4:e1246. doi: 10.1002/wat2.1246 This article is categorized under: Science of Water {$>$} Methods Science of Water {$>$} Water Extremes},
  file = {/mnt/data/Google Drive/Zotero/storage/YQBGAS3E/wat2.html},
  langid = {english},
  number = {6}
}

@article{liReviewStatisticalPostprocessing2017a,
  title = {A Review on Statistical Postprocessing Methods for Hydrometeorological Ensemble Forecasting},
  author = {Li, Wentao and Duan, Qingyun and Miao, Chiyuan and Ye, Aizhong and Gong, Wei and Di, Zhenhua},
  date = {2017},
  journaltitle = {WIREs Water},
  volume = {4},
  pages = {e1246},
  issn = {2049-1948},
  doi = {10.1002/wat2.1246},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1246},
  urldate = {2020-04-07},
  abstract = {Computer simulation models have been widely used to generate hydrometeorological forecasts. As the raw forecasts contain uncertainties arising from various sources, including model inputs and outputs, model initial and boundary conditions, model structure, and model parameters, it is necessary to apply statistical postprocessing methods to quantify and reduce those uncertainties. Different postprocessing methods have been developed for meteorological forecasts (e.g., precipitation) and for hydrological forecasts (e.g., streamflow) due to their different statistical properties. In this paper, we conduct a comprehensive review of the commonly used statistical postprocessing methods for both meteorological and hydrological forecasts. Moreover, methods to generate ensemble members that maintain the observed spatiotemporal and intervariable dependency are reviewed. Finally, some perspectives on the further development of statistical postprocessing methods for hydrometeorological ensemble forecasting are provided. WIREs Water 2017, 4:e1246. doi: 10.1002/wat2.1246 This article is categorized under: Science of Water {$>$} Methods Science of Water {$>$} Water Extremes},
  file = {/mnt/data/Google Drive/Zotero/storage/ECCB4UFB/wat2.html},
  langid = {english},
  number = {6}
}

@article{liReviewStatisticalPostprocessing2017b,
  title = {A Review on Statistical Postprocessing Methods for Hydrometeorological Ensemble Forecasting},
  author = {Li, Wentao and Duan, Qingyun and Miao, Chiyuan and Ye, Aizhong and Gong, Wei and Di, Zhenhua},
  date = {2017},
  journaltitle = {WIREs Water},
  volume = {4},
  pages = {e1246},
  issn = {2049-1948},
  doi = {10.1002/wat2.1246},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1246},
  urldate = {2020-04-07},
  abstract = {Computer simulation models have been widely used to generate hydrometeorological forecasts. As the raw forecasts contain uncertainties arising from various sources, including model inputs and outputs, model initial and boundary conditions, model structure, and model parameters, it is necessary to apply statistical postprocessing methods to quantify and reduce those uncertainties. Different postprocessing methods have been developed for meteorological forecasts (e.g., precipitation) and for hydrological forecasts (e.g., streamflow) due to their different statistical properties. In this paper, we conduct a comprehensive review of the commonly used statistical postprocessing methods for both meteorological and hydrological forecasts. Moreover, methods to generate ensemble members that maintain the observed spatiotemporal and intervariable dependency are reviewed. Finally, some perspectives on the further development of statistical postprocessing methods for hydrometeorological ensemble forecasting are provided. WIREs Water 2017, 4:e1246. doi: 10.1002/wat2.1246 This article is categorized under: Science of Water {$>$} Methods Science of Water {$>$} Water Extremes},
  file = {/mnt/data/Google Drive/Zotero/storage/6WQKZ5C5/Li et al. - 2017 - A review on statistical postprocessing methods for.pdf;/mnt/data/Google Drive/Zotero/storage/DE2Z8TLS/wat2.html},
  langid = {english},
  number = {6}
}

@article{liuDRMECountbasedDifferential2016,
  title = {{{DRME}}: {{Count}}-Based Differential {{RNA}} Methylation Analysis at Small Sample Size Scenario},
  shorttitle = {{{DRME}}},
  author = {Liu, Lian and Zhang, Shao-Wu and Gao, Fan and Zhang, Yixin and Huang, Yufei and Chen, Runsheng and Meng, Jia},
  date = {2016-04-15},
  journaltitle = {Analytical Biochemistry},
  shortjournal = {Anal. Biochem.},
  volume = {499},
  pages = {15--23},
  issn = {1096-0309},
  doi = {10.1016/j.ab.2016.01.014},
  abstract = {Differential methylation, which concerns difference in the degree of epigenetic regulation via methylation between two conditions, has been formulated as a beta or beta-binomial distribution to address the within-group biological variability in sequencing data. However, a beta or beta-binomial model is usually difficult to infer at small sample size scenario with discrete reads count in sequencing data. On the other hand, as an emerging research field, RNA methylation has drawn more and more attention recently, and the differential analysis of RNA methylation is significantly different from that of DNA methylation due to the impact of transcriptional regulation. We developed DRME to better address the differential RNA methylation problem. The proposed model can effectively describe within-group biological variability at small sample size scenario and handles the impact of transcriptional regulation on RNA methylation. We tested the newly developed DRME algorithm on simulated and 4 MeRIP-Seq case-control studies and compared it with Fisher's exact test. It is in principle widely applicable to several other RNA-related data types as well, including RNA Bisulfite sequencing and PAR-CLIP. The code together with an MeRIP-Seq dataset is available online (https://github.com/lzcyzm/DRME) for evaluation and reproduction of the figures shown in this article.},
  eprint = {26851340},
  eprinttype = {pmid},
  keywords = {Algorithms,Differential methylation,MeRIP-Seq,Methylation,N(6)-Methyladenosine (m(6)A),Negative binomial distribution,Particle Size,R/Bioconductor package,RNA,RNA methylation},
  langid = {english}
}

@article{liuQNBDifferentialRNA2017,
  title = {{{QNB}}: Differential {{RNA}} Methylation Analysis for Count-Based Small-Sample Sequencing Data with a Quad-Negative Binomial Model},
  shorttitle = {{{QNB}}},
  author = {Liu, Lian and Zhang, Shao-Wu and Huang, Yufei and Meng, Jia},
  date = {2017-08-31},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {18},
  pages = {387},
  issn = {1471-2105},
  doi = {10.1186/s12859-017-1808-4},
  url = {https://doi.org/10.1186/s12859-017-1808-4},
  urldate = {2019-02-25},
  abstract = {As a newly emerged research area, RNA epigenetics has drawn increasing attention recently for the participation of RNA methylation and other modifications in a number of crucial biological processes. Thanks to high throughput sequencing techniques, such as, MeRIP-Seq, transcriptome-wide RNA methylation profile is now available in the form of count-based data, with which it is often of interests to study the dynamics at epitranscriptomic layer. However, the sample size of RNA methylation experiment is usually very small due to its costs; and additionally, there usually exist a large number of genes whose methylation level cannot be accurately estimated due to their low expression level, making differential RNA methylation analysis a difficult task.},
  file = {/mnt/data/Google Drive/Zotero/storage/2KXY76NX/Liu et al. - 2017 - QNB differential RNA methylation analysis for cou.pdf;/mnt/data/Google Drive/Zotero/storage/2BDKEXHI/s12859-017-1808-4.html},
  number = {1}
}

@article{liWisdomCrowdsAction2016,
  title = {The Wisdom of Crowds in Action: {{Forecasting}} Epidemic Diseases with a Web-Based Prediction Market System},
  shorttitle = {The Wisdom of Crowds in Action},
  author = {Li, Eldon Y. and Tung, Chen-Yuan and Chang, Shu-Hsun},
  date = {2016-08-01},
  journaltitle = {International Journal of Medical Informatics},
  shortjournal = {International Journal of Medical Informatics},
  volume = {92},
  pages = {35--43},
  issn = {1386-5056},
  doi = {10.1016/j.ijmedinf.2016.04.014},
  url = {https://www.sciencedirect.com/science/article/pii/S1386505616300715},
  urldate = {2021-05-27},
  abstract = {Background The quest for an effective system capable of monitoring and predicting the trends of epidemic diseases is a critical issue for communities worldwide. With the prevalence of Internet access, more and more researchers today are using data from both search engines and social media to improve the prediction accuracy. In particular, a prediction market system (PMS) exploits the wisdom of crowds on the Internet to effectively accomplish relatively high accuracy. Objective This study presents the architecture of a PMS and demonstrates the matching mechanism of logarithmic market scoring rules. The system was implemented to predict infectious diseases in Taiwan with the wisdom of crowds in order to improve the accuracy of epidemic forecasting. Methods The PMS architecture contains three design components: database clusters, market engine, and Web applications. The system accumulated knowledge from 126 health professionals for 31 weeks to predict five disease indicators: the confirmed cases of dengue fever, the confirmed cases of severe and complicated influenza, the rate of enterovirus infections, the rate of influenza-like illnesses, and the confirmed cases of severe and complicated enterovirus infection. Results Based on the winning ratio, the PMS predicts the trends of three out of five disease indicators more accurately than does the existing system that uses the five-year average values of historical data for the same weeks. In addition, the PMS with the matching mechanism of logarithmic market scoring rules is easy to understand for health professionals and applicable to predict all the five disease indicators. Conclusions The PMS architecture of this study affords organizations and individuals to implement it for various purposes in our society. The system can continuously update the data and improve prediction accuracy in monitoring and forecasting the trends of epidemic diseases. Future researchers could replicate and apply the PMS demonstrated in this study to more infectious diseases and wider geographical areas, especially the under-developed countries across Asia and Africa.},
  file = {/mnt/data/Google Drive/Zotero/storage/ECDVT5IV/S1386505616300715.html},
  keywords = {Epidemic prediction,Infectious diseases,Logarithmic market scoring rules,Prediction market system,Real-time update,Web-based system,Wisdom of crowds},
  langid = {english}
}

@article{ljungMeasureLackFit1978,
  title = {On a Measure of Lack of Fit in Time Series Models},
  author = {Ljung, G. M. and Box, G. E. P.},
  date = {1978-08-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {65},
  pages = {297--303},
  issn = {0006-3444},
  doi = {10.1093/biomet/65.2.297},
  url = {https://academic.oup.com/biomet/article/65/2/297/236869},
  urldate = {2019-09-13},
  abstract = {Abstract.  The overall test for lack of fit in autoregressive-moving average models proposed by Box \&amp; Pierce (1970) is considered. It is shown that a substa},
  file = {/mnt/data/Google Drive/Zotero/storage/BJQA25WH/Ljung and Box - 1978 - On a measure of lack of fit in time series models.pdf;/mnt/data/Google Drive/Zotero/storage/Z7P63DIX/236869.html},
  langid = {english},
  number = {2}
}

@article{lloyd-smithMaximumLikelihoodEstimation2007,
  title = {Maximum {{Likelihood Estimation}} of the {{Negative Binomial Dispersion Parameter}} for {{Highly Overdispersed Data}}, with {{Applications}} to {{Infectious Diseases}}},
  author = {Lloyd-Smith, James O.},
  date = {2007-02-14},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {2},
  pages = {e180},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0000180},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0000180},
  urldate = {2019-04-05},
  abstract = {Background The negative binomial distribution is used commonly throughout biology as a model for overdispersed count data, with attention focused on the negative binomial dispersion parameter, k. A substantial literature exists on the estimation of k, but most attention has focused on datasets that are not highly overdispersed (i.e., those with k≥1), and the accuracy of confidence intervals estimated for k is typically not explored. Methodology This article presents a simulation study exploring the bias, precision, and confidence interval coverage of maximum-likelihood estimates of k from highly overdispersed distributions. In addition to exploring small-sample bias on negative binomial estimates, the study addresses estimation from datasets influenced by two types of event under-counting, and from disease transmission data subject to selection bias for successful outbreaks. Conclusions Results show that maximum likelihood estimates of k can be biased upward by small sample size or under-reporting of zero-class events, but are not biased downward by any of the factors considered. Confidence intervals estimated from the asymptotic sampling variance tend to exhibit coverage below the nominal level, with overestimates of k comprising the great majority of coverage errors. Estimation from outbreak datasets does not increase the bias of k estimates, but can add significant upward bias to estimates of the mean. Because k varies inversely with the degree of overdispersion, these findings show that overestimation of the degree of overdispersion is very rare for these datasets.},
  file = {/mnt/data/Google Drive/Zotero/storage/H886U24V/Lloyd-Smith - 2007 - Maximum Likelihood Estimation of the Negative Bino.pdf;/mnt/data/Google Drive/Zotero/storage/89I3PGCF/article.html},
  keywords = {Binomials,Epidemiology,Infectious disease control,Infectious disease epidemiology,Infectious disease surveillance,Probability distribution,Respiratory infections,Simulation and modeling},
  langid = {english},
  number = {2}
}

@online{loaiza-mayaFocusedBayesianPrediction2020,
  title = {Focused {{Bayesian Prediction}}},
  author = {Loaiza-Maya, Ruben and Martin, Gael M. and Frazier, David T.},
  date = {2020-08-21},
  url = {http://arxiv.org/abs/1912.12571},
  urldate = {2020-09-15},
  abstract = {We propose a new method for conducting Bayesian prediction that delivers accurate predictions without correctly specifying the unknown true data generating process. A prior is defined over a class of plausible predictive models. After observing data, we update the prior to a posterior over these models, via a criterion that captures a user-specified measure of predictive accuracy. Under regularity, this update yields posterior concentration onto the element of the predictive class that maximizes the expectation of the accuracy measure. In a series of simulation experiments and empirical examples we find notable gains in predictive accuracy relative to conventional likelihood-based prediction.},
  archiveprefix = {arXiv},
  eprint = {1912.12571},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/2922F324/Loaiza-Maya et al. - 2020 - Focused Bayesian Prediction.pdf},
  keywords = {Economics - General Economics,Statistics - Applications,Statistics - Methodology},
  langid = {english},
  primaryclass = {econ, q-fin, stat}
}

@online{loganc.brooksComparingEnsembleApproaches,
  title = {Comparing Ensemble Approaches for Short-Term Probabilistic {{COVID}}-19 Forecasts in the {{U}}.{{S}}. - {{International Institute}} of {{Forecasters}}},
  author = {{Logan C. Brooks} and {Evan L. Ray} and {Jacob Bien} and {Johannes Bracher} and {Aaron Rumack} and {Ryan J. Tibshirani} and {Nicholas G. Reich}},
  url = {https://forecasters.org/blog/2020/10/28/comparing-ensemble-approaches-for-short-term-probabilistic-covid-19-forecasts-in-the-u-s/},
  urldate = {2021-07-12},
  file = {/mnt/data/Google Drive/Zotero/storage/UCPVDFPE/comparing-ensemble-approaches-for-short-term-probabilistic-covid-19-forecasts-in-the-u-s.html},
  langid = {american}
}

@article{loveModeratedEstimationFold2014,
  title = {Moderated Estimation of Fold Change and Dispersion for {{RNA}}-Seq Data with {{DESeq2}}},
  author = {Love, Michael I and Huber, Wolfgang and Anders, Simon},
  date = {2014},
  journaltitle = {Genome Biology},
  shortjournal = {Genome Biol},
  volume = {15},
  issn = {1465-6906},
  doi = {10.1186/s13059-014-0550-8},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4302049/},
  urldate = {2018-12-02},
  abstract = {In comparative high-throughput sequencing assays, a fundamental task is the analysis of count data, such as read counts per gene in RNA-seq, for evidence of systematic changes across experimental conditions. Small replicate numbers, discreteness, large dynamic range and the presence of outliers require a suitable statistical approach. We present DESeq2, a method for differential analysis of count data, using shrinkage estimation for dispersions and fold changes to improve stability and interpretability of estimates. This enables a more quantitative analysis focused on the strength rather than the mere presence of differential expression. The DESeq2 package is available at http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html.},
  eprint = {25516281},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/9ZYZUDWF/Love et al. - 2014 - Moderated estimation of fold change and dispersion.pdf},
  number = {12},
  pmcid = {PMC4302049}
}

@article{luLearningPredictMiRNAmRNA2016,
  title = {Learning to {{Predict miRNA}}-{{mRNA Interactions}} from {{AGO CLIP Sequencing}} and {{CLASH Data}}},
  author = {Lu, Yuheng and Leslie, Christina S.},
  date = {2016-07-20},
  journaltitle = {PLoS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {12},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1005026},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4954643/},
  urldate = {2019-11-06},
  abstract = {Recent technologies like AGO CLIP sequencing and CLASH enable direct transcriptome-wide identification of AGO binding and miRNA target sites, but the most widely used miRNA target prediction algorithms do not exploit these data. Here we use discriminative learning on AGO CLIP and CLASH interactions to train a novel miRNA target prediction model. Our method combines two SVM classifiers, one to predict miRNA-mRNA duplexes and a second to learn a binding model of AGO’s local UTR sequence preferences and positional bias in 3’UTR isoforms. The duplex SVM model enables the prediction of non-canonical target sites and more accurately resolves miRNA interactions from AGO CLIP data than previous methods. The binding model is trained using a multi-task strategy to learn context-specific and common AGO sequence preferences. The duplex and common AGO binding models together outperform existing miRNA target prediction algorithms on held-out binding data. Open source code is available at https://bitbucket.org/leslielab/chimiric., MicroRNAs (or miRNAs) are a family of small RNA molecules that guide Argonaute (AGO) to specific target sites within mRNAs and regulate numerous biological processes in normal cells and in disease. Despite years of research, the principles of miRNA targeting are incompletely understood, and computational miRNA target prediction methods still achieve only modest performance. Most previous target prediction work has been based on indirect measurements of miRNA regulation, such as mRNA expression changes upon miRNA perturbation, without mapping actual binding sites, which limits accuracy and precludes discovery of more subtle miRNA targeting rules. The recent introduction of CLIP (UV crosslinking followed by immunoprecipitation) sequencing technologies enables direct identification of interactions between miRNAs and mRNAs. However, the data generated from these assays has not been fully exploited in target prediction. Here, we present a model to predict miRNA-mRNA interactions solely based on their sequences, using new technologies to map AGO and miRNA binding interactions with machine learning techniques. Our algorithm produces more accurate predictions than state-of-the-art methods based on indirect measurements. Moreover, interpretation of the learned model reveals novel features of miRNA-mRNA interactions, including potential cooperativity with specific RNA-binding proteins.},
  eprint = {27438777},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/J5RBW55G/Lu and Leslie - 2016 - Learning to Predict miRNA-mRNA Interactions from A.pdf},
  number = {7},
  pmcid = {PMC4954643}
}

@article{lunItDEliciousRecipe2016,
  title = {It's {{DE}}-Licious: {{A Recipe}} for {{Differential Expression Analyses}} of {{RNA}}-Seq {{Experiments Using Quasi}}-{{Likelihood Methods}} in {{edgeR}}},
  shorttitle = {It's {{DE}}-Licious},
  author = {Lun, Aaron T. L. and Chen, Yunshun and Smyth, Gordon K.},
  date = {2016},
  journaltitle = {Methods in Molecular Biology (Clifton, N.J.)},
  shortjournal = {Methods Mol. Biol.},
  volume = {1418},
  pages = {391--416},
  issn = {1940-6029},
  doi = {10.1007/978-1-4939-3578-9_19},
  abstract = {RNA sequencing (RNA-seq) is widely used to profile transcriptional activity in biological systems. Here we present an analysis pipeline for differential expression analysis of RNA-seq experiments using the Rsubread and edgeR software packages. The basic pipeline includes read alignment and counting, filtering and normalization, modelling of biological variability and hypothesis testing. For hypothesis testing, we describe particularly the quasi-likelihood features of edgeR. Some more advanced downstream analysis steps are also covered, including complex comparisons, gene ontology enrichment analyses and gene set testing. The code required to run each step is described, along with an outline of the underlying theory. The chapter includes a case study in which the pipeline is used to study the expression profiles of mammary gland cells in virgin, pregnant and lactating mice.},
  eprint = {27008025},
  eprinttype = {pmid},
  keywords = {Animals,Computational Biology,Databases; Genetic,Differential expression,Gene Expression Profiling,Generalized linear models,High-Throughput Nucleotide Sequencing,Humans,Likelihood Functions,Linear Models,Mice,Molecular Sequence Annotation,Quasi-likelihood,Read alignment,Read counts,RNA-seq,Sequence Alignment,Sequence Analysis; RNA,Software,Variability},
  langid = {english}
}

@book{m.davisMicroRNAsNotFineTuners2015,
  title = {{{MicroRNAs}}: {{Not}} “{{Fine}}-{{Tuners}}” but {{Key Regulators}} of {{Neuronal Development}} and {{Function}}},
  shorttitle = {{{MicroRNAs}}},
  author = {M. Davis, Gregory and Haas, Matilda and Pocock, Roger},
  date = {2015-11-24},
  volume = {6},
  doi = {10.3389/fneur.2015.00245},
  abstract = {MicroRNAs (miRNAs) are a class of short non-coding RNAs that operate as prominent post-transcriptional regulators of eukaryotic gene expression. miRNAs are abundantly expressed in the brain of most animals and exert diverse roles. The anatomical and functional complexity of the brain requires the precise coordination of multilayered gene regulatory networks. The flexibility, speed, and reversibility of miRNA function provide precise temporal and spatial gene regulatory capabilities that are crucial for the correct functioning of the brain. Studies have shown that the underlying molecular mechanisms controlled by miRNAs in the nervous systems of invertebrate and vertebrate models are remarkably conserved in humans. We endeavor to provide insight into the roles of miRNAs in the nervous systems of these model organisms and discuss how such information may be used to inform regarding diseases of the human brain.},
  file = {/mnt/data/Google Drive/Zotero/storage/HKCBZPB5/M. Davis et al. - 2015 - MicroRNAs Not “Fine-Tuners” but Key Regulators of.pdf}
}

@online{macheteContrastingProbabilisticScoring2012,
  title = {Contrasting {{Probabilistic Scoring Rules}}},
  author = {Machete, Reason Lesego},
  date = {2012-07-24},
  url = {http://arxiv.org/abs/1112.4530},
  urldate = {2020-03-21},
  abstract = {There are several scoring rules that one can choose from in order to score probabilistic forecasting models or estimate model parameters. Whilst it is generally agreed that proper scoring rules are preferable, there is no clear criterion for preferring one proper scoring rule above another. This manuscript compares and contrasts some commonly used proper scoring rules and provides guidance on scoring rule selection. In particular, it is shown that the logarithmic scoring rule prefers erring with more uncertainty, the spherical scoring rule prefers erring with lower uncertainty, whereas the other scoring rules are indifferent to either option.},
  archiveprefix = {arXiv},
  eprint = {1112.4530},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/8FYPC3Y4/Machete - 2012 - Contrasting Probabilistic Scoring Rules.pdf},
  keywords = {62B10; 62C05; 62G05; 62G07; 62F99; 62P05; 62P12; 62P20,Mathematics - Statistics Theory},
  langid = {english},
  primaryclass = {math, stat}
}

@article{macheteEarlyWarningCalibrated2013,
  title = {Early {{Warning}} with {{Calibrated}} and {{Sharper Probabilistic Forecasts}}},
  author = {Machete, Reason L.},
  date = {2013},
  journaltitle = {Journal of Forecasting},
  volume = {32},
  pages = {452--468},
  issn = {1099-131X},
  doi = {10.1002/for.2242},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2242},
  urldate = {2020-03-20},
  abstract = {ABSTRACTGiven a nonlinear model, a probabilistic forecast may be obtained by Monte Carlo simulations. At a given forecast horizon, Monte Carlo simulations yield sets of discrete forecasts, which can be converted to density forecasts. The resulting density forecasts will inevitably be downgraded by model misspecification. In order to enhance the quality of the density forecasts, one can mix them with the unconditional density. This paper examines the value of combining conditional density forecasts with the unconditional density. The findings have positive implications for issuing early warnings in different disciplines including economics and meteorology, but UK inflation forecasts are considered as an example. Copyright © 2012 John Wiley \& Sons, Ltd.},
  file = {/mnt/data/Google Drive/Zotero/storage/FHU93V4Z/Machete - 2013 - Early Warning with Calibrated and Sharper Probabil.pdf;/mnt/data/Google Drive/Zotero/storage/S554FQ63/for.html},
  keywords = {calibration,combining forecasts,density forecasts,scoring rule},
  langid = {english},
  number = {5}
}

@article{macheteEarlyWarningCalibrated2013a,
  title = {Early {{Warning}} with {{Calibrated}} and {{Sharper Probabilistic Forecasts}}},
  author = {Machete, Reason L.},
  date = {2013},
  journaltitle = {Journal of Forecasting},
  volume = {32},
  pages = {452--468},
  issn = {1099-131X},
  doi = {10.1002/for.2242},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2242},
  urldate = {2020-04-03},
  abstract = {ABSTRACTGiven a nonlinear model, a probabilistic forecast may be obtained by Monte Carlo simulations. At a given forecast horizon, Monte Carlo simulations yield sets of discrete forecasts, which can be converted to density forecasts. The resulting density forecasts will inevitably be downgraded by model misspecification. In order to enhance the quality of the density forecasts, one can mix them with the unconditional density. This paper examines the value of combining conditional density forecasts with the unconditional density. The findings have positive implications for issuing early warnings in different disciplines including economics and meteorology, but UK inflation forecasts are considered as an example. Copyright © 2012 John Wiley \& Sons, Ltd.},
  file = {/mnt/data/Google Drive/Zotero/storage/PKZT8IGY/Machete - 2013 - Early Warning with Calibrated and Sharper Probabil.pdf;/mnt/data/Google Drive/Zotero/storage/7LJPT7DX/for.html},
  keywords = {calibration,combining forecasts,density forecasts,scoring rule},
  langid = {english},
  number = {5}
}

@incollection{mackayHyperparametersOptimizeIntegrate1996,
  title = {Hyperparameters: {{Optimize}}, or {{Integrate Out}}?},
  shorttitle = {Hyperparameters},
  booktitle = {Maximum {{Entropy}} and {{Bayesian Methods}}},
  author = {MacKay, David J. C.},
  editor = {Heidbreder, Glenn R.},
  date = {1996},
  pages = {43--59},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-015-8729-7_2},
  url = {http://link.springer.com/10.1007/978-94-015-8729-7_2},
  urldate = {2019-01-02},
  abstract = {I examine two approximate methods for computational implementation of Bayesian hierarchical models, that is, models which include unknown hyperparameters such as regularization constants. In the `evidence framework' the model parameters are integrated over, and the resulting evidence is maximized over the hyperparameters. The optimized hyperparameters are used to de ne a Gaussian approximation to the posterior distribution. In the alternative `MAP' method, the true posterior probability is found by integrating over the hyperparameters. The true posterior is then maximized over the model parameters, and a Gaussian approximation is made. The similarities of the two approaches, and their relative merits, are discussed, and comparisons are made with the ideal hierarchical Bayesian solution.},
  file = {/mnt/data/Google Drive/Zotero/storage/LQ8PXJWY/MacKay - 1996 - Hyperparameters Optimize, or Integrate Out.pdf},
  isbn = {978-90-481-4407-5 978-94-015-8729-7},
  langid = {english}
}

@online{makridakisFutureForecastingCompetitions2021,
  title = {The Future of Forecasting Competitions: {{Design}} Attributes and Principles},
  shorttitle = {The Future of Forecasting Competitions},
  author = {Makridakis, Spyros and Fry, Chris and Petropoulos, Fotios and Spiliotis, Evangelos},
  date = {2021-05-19},
  url = {http://arxiv.org/abs/2102.04879},
  urldate = {2021-05-27},
  abstract = {Forecasting competitions are the equivalent of laboratory experimentation widely used in physical and life sciences. They provide useful, objective information to improve the theory and practice of forecasting, advancing the field, expanding its usage and enhancing its value to decision and policymakers. We describe ten design attributes to be considered when organizing forecasting competitions, taking into account trade-offs between optimal choices and practical concerns like costs, as well as the time and effort required to participate in them. Consequently, we map all major past competitions in respect to their design attributes, identifying similarities and differences between them, as well as design gaps, and making suggestions about the principles to be included in future competitions, putting a particular emphasis on learning as much as possible from their implementation in order to help improve forecasting accuracy and uncertainty. We discuss that the task of forecasting often presents a multitude of challenges that can be difficult to be captured in a single forecasting contest. To assess the caliber of a forecaster, we, therefore, propose that organizers of future competitions consider a multi-contest approach. We suggest the idea of a forecasting "athlon", where different challenges of varying characteristics take place.},
  archiveprefix = {arXiv},
  eprint = {2102.04879},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/2VIXS8PS/Makridakis et al. - 2021 - The future of forecasting competitions Design att.pdf;/mnt/data/Google Drive/Zotero/storage/SQ42KWKE/2102.html},
  keywords = {Statistics - Applications},
  primaryclass = {stat}
}

@article{makridakisStatisticalMachineLearning2018,
  title = {Statistical and {{Machine Learning}} Forecasting Methods: {{Concerns}} and Ways Forward},
  shorttitle = {Statistical and {{Machine Learning}} Forecasting Methods},
  author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  date = {2018-03-27},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {13},
  pages = {e0194889},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0194889},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194889},
  urldate = {2020-05-15},
  abstract = {Machine Learning (ML) methods have been proposed in the academic literature as alternatives to statistical ones for time series forecasting. Yet, scant evidence is available about their relative performance in terms of accuracy and computational requirements. The purpose of this paper is to evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. After comparing the post-sample accuracy of popular ML methods with that of eight traditional statistical ones, we found that the former are dominated across both accuracy measures used and for all forecasting horizons examined. Moreover, we observed that their computational requirements are considerably greater than those of statistical methods. The paper discusses the results, explains why the accuracy of ML models is below that of statistical ones and proposes some possible ways forward. The empirical results found in our research stress the need for objective and unbiased ways to test the performance of forecasting methods that can be achieved through sizable and open competitions allowing meaningful comparisons and definite conclusions.},
  file = {/mnt/data/Google Drive/Zotero/storage/GV5QJGJ8/Makridakis et al. - 2018 - Statistical and Machine Learning forecasting metho.pdf;/mnt/data/Google Drive/Zotero/storage/NSJH4N43/article.html},
  keywords = {Algorithms,Computing methods,Forecasting,Neural networks,Optimization,Preprocessing,Statistical methods,Support vector machines},
  langid = {english},
  number = {3}
}

@article{marioniRNAseqAssessmentTechnical2008,
  title = {{{RNA}}-Seq: {{An}} Assessment of Technical Reproducibility and Comparison with Gene Expression Arrays},
  shorttitle = {{{RNA}}-Seq},
  author = {Marioni, John C. and Mason, Christopher E. and Mane, Shrikant M. and Stephens, Matthew and Gilad, Yoav},
  date = {2008-09-01},
  journaltitle = {Genome Research},
  shortjournal = {Genome Res.},
  issn = {1088-9051, 1549-5469},
  doi = {10.1101/gr.079558.108},
  url = {http://genome.cshlp.org/content/early/2008/07/30/gr.079558.108},
  urldate = {2018-12-01},
  abstract = {Ultra-high-throughput sequencing is emerging as an attractive alternative to microarrays for genotyping, analysis of methylation patterns, and identification of transcription factor binding sites. Here, we describe an application of the Illumina sequencing (formerly Solexa sequencing) platform to study mRNA expression levels. Our goals were to estimate technical variance associated with Illumina sequencing in this context and to compare its ability to identify differentially expressed genes with existing array technologies. To do so, we estimated gene expression differences between liver and kidney RNA samples using multiple sequencing replicates, and compared the sequencing data to results obtained from Affymetrix arrays using the same RNA samples. We find that the Illumina sequencing data are highly replicable, with relatively little technical variation, and thus, for many purposes, it may suffice to sequence each mRNA sample only once (i.e., using one lane). The information in a single lane of Illumina sequencing data appears comparable to that in a single array in enabling identification of differentially expressed genes, while allowing for additional analyses such as detection of low-expressed genes, alternative splice variants, and novel transcripts. Based on our observations, we propose an empirical protocol and a statistical framework for the analysis of gene expression using ultra-high-throughput sequencing technology.},
  file = {/mnt/data/Google Drive/Zotero/storage/8LXW6IH5/Marioni et al. - 2008 - RNA-seq An assessment of technical reproducibilit.pdf;/mnt/data/Google Drive/Zotero/storage/J2KNF2GL/Marioni et al. - 2008 - RNA-seq An assessment of technical reproducibilit.pdf;/mnt/data/Google Drive/Zotero/storage/UBAV6T4C/Marioni et al. - 2008 - RNA-seq An assessment of technical reproducibilit.pdf;/mnt/data/Google Drive/Zotero/storage/FN28X7D2/gr.079558.108.html;/mnt/data/Google Drive/Zotero/storage/VKXW3ZBB/gr.079558.108.html;/mnt/data/Google Drive/Zotero/storage/XMPU6X4T/gr.079558.108.html},
  langid = {english}
}

@online{marzXGBoostLSSExtensionXGBoost2019,
  title = {{{XGBoostLSS}} -- {{An}} Extension of {{XGBoost}} to Probabilistic Forecasting},
  author = {März, Alexander},
  date = {2019-08-25},
  url = {http://arxiv.org/abs/1907.03178},
  urldate = {2020-02-06},
  abstract = {We propose a new framework of XGBoost that predicts the entire conditional distribution of a univariate response variable. In particular, XGBoostLSS models all moments of a parametric distribution (i.e., mean, location, scale and shape [LSS]) instead of the conditional mean only. Choosing from a wide range of continuous, discrete and mixed discrete-continuous distribution, modelling and predicting the entire conditional distribution greatly enhances the flexibility of XGBoost, as it allows to gain additional insight into the data generating process, as well as to create probabilistic forecasts from which prediction intervals and quantiles of interest can be derived. We present both a simulation study and real world examples that demonstrate the virtues of our approach.},
  archiveprefix = {arXiv},
  eprint = {1907.03178},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/NYKHUUHG/März - 2019 - XGBoostLSS -- An extension of XGBoost to probabili.pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  langid = {english},
  primaryclass = {cs, stat}
}

@article{mathesonScoringRulesContinuous1976,
  title = {Scoring {{Rules}} for {{Continuous Probability Distributions}}},
  author = {Matheson, James E. and Winkler, Robert L.},
  date = {1976-06-01},
  journaltitle = {Management Science},
  shortjournal = {Management Science},
  volume = {22},
  pages = {1087--1096},
  publisher = {{INFORMS}},
  issn = {0025-1909},
  doi = {10.1287/mnsc.22.10.1087},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.22.10.1087},
  urldate = {2020-08-13},
  abstract = {Personal, or subjective, probabilities are used as inputs to many inferential and decision-making models, and various procedures have been developed for the elicitation of such probabilities. Included among these elicitation procedures are scoring rules, which involve the computation of a score based on the assessor's stated probabilities and on the event that actually occurs. The development of scoring rules has, in general, been restricted to the elicitation of discrete probability distributions. In this paper, families of scoring rules for the elicitation of continuous probability distributions are developed and discussed.},
  file = {/mnt/data/Google Drive/Zotero/storage/SVJ7YPP7/Matheson and Winkler - 1976 - Scoring Rules for Continuous Probability Distribut.pdf;/mnt/data/Google Drive/Zotero/storage/H5CNZS4U/mnsc.22.10.html},
  number = {10}
}

@article{matkovichRISCRNASequencing2011,
  title = {{{RISC RNA}} Sequencing for Context-Specific Identification of in Vivo {{miR}} Targets},
  author = {Matkovich, Scot J and Van Booven, Derek J and Eschenbacher, William H and Dorn, Gerald W},
  date = {2011-01-07},
  journaltitle = {Circulation research},
  shortjournal = {Circ Res},
  volume = {108},
  pages = {18--26},
  issn = {0009-7330},
  doi = {10.1161/CIRCRESAHA.110.233528},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3017647/},
  urldate = {2018-09-12},
  abstract = {Rationale MicroRNAs (miRs) are expanding our understanding of cardiac disease and have the potential to transform cardiovascular therapeutics. One miR can target hundreds of individual mRNAs, but existing methodologies are not sufficient to accurately and comprehensively identify these mRNA targets in vivo. Objective To develop methods permitting identification of in vivo miR targets in an unbiased manner, using massively parallel sequencing of mouse cardiac transcriptomes in combination with sequencing of mRNA associated with mouse cardiac RNA-induced silencing complexes (RISCs). Methods and Results We optimized techniques for expression profiling small amounts of RNA without introducing amplification bias, and applied this to anti-Argonaute 2 immunoprecipitated RISCs (RISC-Seq) from mouse hearts. By comparing RNA-sequencing results of cardiac RISC and transcriptome from the same individual hearts, we defined 1,645 mRNAs consistently targeted to mouse cardiac RISCs. We employed this approach in hearts overexpressing miRs from Myh6 promoter-driven precursors (programmed RISC-Seq) to identify 209 in vivo targets of miR-133a and 81 in vivo targets of miR-499. Consistent with the fact that miR-133a and miR-499 have widely differing ‘seed’ sequences and belong to different miR families, only 6 targets were common to miR-133a- and miR-499-programmed hearts. Conclusions RISC-sequencing is a highly sensitive method for general RISC profiling and individual miR target identification in biological context, and is applicable to any tissue and any disease state. Summary MicroRNAs (miRs) are key regulators of mRNA translation in health and disease. While bioinformatic predictions suggest that a single miR may target hundreds of mRNAs, the number of experimentally verified targets of miRs is low. To enable comprehensive, unbiased examination of miR targets, we have performed deep RNA sequencing of cardiac transcriptomes in parallel with cardiac RNA-induced silencing complex (RISC)-associated RNAs (the RISCome), called RISC sequencing. We developed methods that did not require cross-linking of RNAs to RISCs or amplification of mRNA prior to sequencing, making it possible to rapidly perform RISC sequencing from intact tissue while avoiding amplification bias. Comparison of RISCome with transcriptome expression defined the degree of RISC enrichment for each mRNA. The majority of the mRNAs enriched in wild-type cardiac RISComes compared to transcriptomes were bioinformatically predicted to be targets of at least 1 of 139 cardiac-expressed miRs. Programming cardiomyocyte RISCs via transgenic overexpression in adult hearts of miR-133a or miR-499, two miRs that contain entirely different ‘seed’ sequences, elicited differing profiles of RISC-targeted mRNAs. Thus, RISC sequencing represents a highly sensitive method for general RISC profiling and individual miR target identification in biological context.},
  eprint = {21030712},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/5NF4N6YQ/Matkovich et al. - 2011 - RISC RNA sequencing for context-specific identific.pdf},
  number = {1},
  pmcid = {PMC3017647}
}

@article{matopradoInvestigatingMiRNAmRNARegulatory2016,
  title = {Investigating {{miRNA}}-{{mRNA}} Regulatory Networks Using Crosslinking Immunoprecipitation Methods for Biomarker and Target Discovery in Cancer},
  author = {Mato Prado, Mireia and Frampton, Adam E. and Giovannetti, Elisa and Stebbing, Justin and Castellano, Leandro and Krell, Jonathan},
  date = {2016-11},
  journaltitle = {Expert Review of Molecular Diagnostics},
  shortjournal = {Expert Rev. Mol. Diagn.},
  volume = {16},
  pages = {1155--1162},
  issn = {1744-8352},
  doi = {10.1080/14737159.2016.1239532},
  abstract = {INTRODUCTION: MicroRNAs (miRNAs) are small non-coding RNAs that regulate gene expression at the post-transcriptional level. Recently, different experimental approaches, such as RNA Sequencing, crosslinking immunoprecipitation (CLIP) methods and its variations, together with computational approaches have been developed to elucidate the miRNA-mRNA targetome. Areas covered: This report focuses on comparing the different experimental and computational approaches, describing their advantages and disadvantages and providing several examples of preclinical (in vitro and in vivo) and clinical studies that have identified miRNA target genes in various tumour types, including breast, ovary, colorectal and pancreas. Expert commentary: The combination of CLIP methods with bioinformatic analyses is essential to better predict miRNA-mRNA interactions and associate their specific pathways within the extensive regulatory network. Nevertheless, further studies are needed to overcome the difficulties these methods have, in order to find a gold standard method that identifies, without any bias, the regulatory association between miRNAs and their target mRNAs.},
  eprint = {27784183},
  eprinttype = {pmid},
  keywords = {Animals,biomarker,Biomarkers; Tumor,cancer,CLASH,Computational Biology,Gene Regulatory Networks,High-Throughput Nucleotide Sequencing,Humans,Immunoprecipitation,microRNA,MicroRNAs,Neoplasms,pancreatic Cancer,PAR-CLIP,RNA Interference,RNA sequencing,RNA; Messenger,Sequence Analysis; RNA,targetome,therapy},
  langid = {english},
  number = {11}
}

@article{mazaPapyroComparisonTMM2016,
  title = {In {{Papyro Comparison}} of {{TMM}} ({{edgeR}}), {{RLE}} ({{DESeq2}}), and {{MRN Normalization Methods}} for a {{Simple Two}}-{{Conditions}}-{{Without}}-{{Replicates RNA}}-{{Seq Experimental Design}}},
  author = {Maza, Elie},
  date = {2016},
  journaltitle = {Frontiers in Genetics},
  shortjournal = {Front Genet},
  volume = {7},
  pages = {164},
  issn = {1664-8021},
  doi = {10.3389/fgene.2016.00164},
  abstract = {In the past 5 years, RNA-Seq has become a powerful tool in transcriptome analysis even though computational methods dedicated to the analysis of high-throughput sequencing data are yet to be standardized. It is, however, now commonly accepted that the choice of a normalization procedure is an important step in such a process, for example in differential gene expression analysis. The present article highlights the similarities between three normalization methods: TMM from edgeR R package, RLE from DESeq2 R package, and MRN. Both TMM and DESeq2 are widely used for differential gene expression analysis. This paper introduces properties that show when these three methods will give exactly the same results. These properties are proven mathematically and illustrated by performing in silico calculations on a given RNA-Seq data set.},
  eprint = {27695478},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/PG2WQXP2/Maza - 2016 - In Papyro Comparison of TMM (edgeR), RLE (DESeq2),.pdf},
  keywords = {comparison of methods,DESeq2,edgeR,normalization,RNA-seq data},
  langid = {english},
  pmcid = {PMC5025571}
}

@online{mcandrewAdaptivelyStackingEnsembles2019,
  title = {Adaptively Stacking Ensembles for Influenza Forecasting with Incomplete Data},
  author = {McAndrew, Thomas and Reich, Nicholas G.},
  date = {2019-07-26},
  url = {http://arxiv.org/abs/1908.01675},
  urldate = {2020-04-12},
  abstract = {Seasonal influenza infects between 10 and 50 million people in the United States every year, overburdening hospitals during weeks of peak incidence. Named by the CDC as an important tool to fight the damaging effects of these epidemics, accurate forecasts of influenza and influenza-like illness (ILI) forewarn public health officials about when, and where, seasonal influenza outbreaks will hit hardest. Multi-model ensemble forecasts---weighted combinations of component models---have shown positive results in forecasting. Ensemble forecasts of influenza outbreaks have been static, training on all past ILI data at the beginning of a season, generating a set of optimal weights for each model in the ensemble, and keeping the weights constant. We propose an adaptive ensemble forecast that (i) changes model weights week-by-week throughout the influenza season, (ii) only needs the current influenza season's data to make predictions, and (iii) by introducing a prior distribution, shrinks weights toward the reference equal weighting approach and adjusts for observed ILI percentages that are subject to future revisions. We investigate the prior's ability to impact adaptive ensemble performance and, after finding an optimal prior via a cross-validation approach, compare our adaptive ensemble's performance to equal-weighted and static ensembles. Applied to forecasts of short-term ILI incidence at the regional and national level in the US, our adaptive model outperforms a naive equal-weighted ensemble, and has similar or better performance to the static ensemble, which requires multiple years of training data. Adaptive ensembles are able to quickly train and forecast during epidemics, and provide a practical tool to public health officials looking for forecasts that can conform to unique features of a specific season.},
  archiveprefix = {arXiv},
  eprint = {1908.01675},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/7SC4EU2N/McAndrew and Reich - 2019 - Adaptively stacking ensembles for influenza foreca.pdf;/mnt/data/Google Drive/Zotero/storage/KKANAKWG/1908.html},
  keywords = {Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{mcandrewAggregatingPredictionsExperts2020,
  title = {Aggregating Predictions from Experts: A Scoping Review of Statistical Methods, Experiments, and Applications},
  shorttitle = {Aggregating Predictions from Experts},
  author = {McAndrew, Thomas and Wattanachit, Nutcha and Gibson, G. Casey and Reich, Nicholas G.},
  date = {2020-05-16},
  url = {http://arxiv.org/abs/1912.11409},
  urldate = {2020-09-23},
  abstract = {Forecasts support decision making in a variety of applications. Statistical models can produce accurate forecasts given abundant training data, but when data is sparse, rapidly changing, or unavailable, statistical models may not be able to make accurate predictions. Expert judgmental forecasts---models that combine expert-generated predictions into a single forecast---can make predictions when training data is limited by relying on expert intuition to take the place of concrete training data. Researchers have proposed a wide array of algorithms to combine expert predictions into a single forecast, but there is no consensus on an optimal aggregation model. This scoping review surveyed recent literature on aggregating expert-elicited predictions. We gathered common terminology, aggregation methods, and forecasting performance metrics, and offer guidance to strengthen future work that is growing at an accelerated pace.},
  archiveprefix = {arXiv},
  eprint = {1912.11409},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/HCXWJIVH/McAndrew et al. - 2020 - Aggregating predictions from experts a scoping re.pdf;/mnt/data/Google Drive/Zotero/storage/GIXKCZ9P/1912.html},
  keywords = {Statistics - Applications},
  primaryclass = {stat}
}

@article{mcandrewAggregatingPredictionsExperts2021,
  title = {Aggregating Predictions from Experts: {{A}} Review of Statistical Methods, Experiments, and Applications},
  shorttitle = {Aggregating Predictions from Experts},
  author = {McAndrew, Thomas and Wattanachit, Nutcha and Gibson, Graham C. and Reich, Nicholas G.},
  date = {2021},
  journaltitle = {WIREs Computational Statistics},
  volume = {13},
  pages = {e1514},
  issn = {1939-0068},
  doi = {10.1002/wics.1514},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1514},
  urldate = {2021-05-30},
  abstract = {Forecasts support decision making in a variety of applications. Statistical models can produce accurate forecasts given abundant training data, but when data is sparse or rapidly changing, statistical models may not be able to make accurate predictions. Expert judgmental forecasts—models that combine expert-generated predictions into a single forecast—can make predictions when training data is limited by relying on human intuition. Researchers have proposed a wide array of algorithms to combine expert predictions into a single forecast, but there is no consensus on an optimal aggregation model. This review surveyed recent literature on aggregating expert-elicited predictions. We gathered common terminology, aggregation methods, and forecasting performance metrics, and offer guidance to strengthen future work that is growing at an accelerated pace. This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences {$>$} Clustering and Classification Statistical Learning and Exploratory Methods of the Data Sciences {$>$} Exploratory Data Analysis Statistical Learning and Exploratory Methods of the Data Sciences {$>$} Modeling Methods Statistical and Graphical Methods of Data Analysis {$>$} Multivariate Analysis},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1514},
  file = {/mnt/data/Google Drive/Zotero/storage/W29GXEKL/McAndrew et al. - 2021 - Aggregating predictions from experts A review of .pdf;/mnt/data/Google Drive/Zotero/storage/4LD7PJYF/wics.html},
  keywords = {consensus,expert judgment,forecast aggregation,forecast combination,judgmental forecasting},
  langid = {english},
  number = {2}
}

@article{mcandrewExpertJudgmentModel2020,
  title = {An Expert Judgment Model to Predict Early Stages of the {{COVID}}-19 Outbreak in the {{United States}}},
  author = {McAndrew, Thomas Charles and Reich, Nicholas G.},
  date = {2020-09-23},
  journaltitle = {medRxiv},
  pages = {2020.09.21.20196725},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  issn = {2019-6725},
  doi = {10.1101/2020.09.21.20196725},
  url = {https://www.medrxiv.org/content/10.1101/2020.09.21.20196725v1},
  urldate = {2020-09-23},
  abstract = {{$<$}p{$>$}During early stages of the COVID-19 pandemic, forecasts provided actionable information about disease transmission to public health decision-makers. Between February and May 2020, experts in infectious disease modeling made weekly predictions about the impact of the pandemic in the U.S. We aggregated these predictions into consensus predictions. In March and April 2020, experts predicted that the number of COVID-19 related deaths in the U.S. by the end of 2020 would be in the range of 150,000 to 250,000, with scenarios of near 1m deaths considered plausible. The wide range of possible future outcomes underscored the uncertainty surrounding the outbreak9s trajectory. Experts9 predictions of measurable short-term outcomes had varying levels of accuracy over the surveys but showed appropriate levels of uncertainty when aggregated. An expert consensus model can provide important insight early on in an emerging global catastrophe.{$<$}/p{$>$}},
  file = {/mnt/data/Google Drive/Zotero/storage/3M97Z8QX/McAndrew and Reich - 2020 - An expert judgment model to predict early stages o.pdf;/mnt/data/Google Drive/Zotero/storage/XEAHR8XR/2020.09.21.html},
  langid = {english}
}

@article{mccarthyDifferentialExpressionAnalysis2012,
  title = {Differential Expression Analysis of Multifactor {{RNA}}-{{Seq}} Experiments with Respect to Biological Variation},
  author = {McCarthy, Davis J. and Chen, Yunshun and Smyth, Gordon K.},
  date = {2012-05-01},
  journaltitle = {Nucleic Acids Research},
  volume = {40},
  pages = {4288--4297},
  issn = {1362-4962, 0305-1048},
  doi = {10.1093/nar/gks042},
  url = {https://academic.oup.com/nar/article/40/10/4288/2411520},
  urldate = {2018-12-01},
  file = {/mnt/data/Google Drive/Zotero/storage/8TKBTU43/McCarthy et al. - 2012 - Differential expression analysis of multifactor RN.pdf;/mnt/data/Google Drive/Zotero/storage/JHGMBRFA/McCarthy et al. - 2012 - Differential expression analysis of multifactor RN.pdf;/mnt/data/Google Drive/Zotero/storage/QQM3Y333/McCarthy et al. - 2012 - Differential expression analysis of multifactor RN.pdf;/mnt/data/Google Drive/Zotero/storage/W2C628C3/McCarthy et al. - 2012 - Differential expression analysis of multifactor RN.pdf;/mnt/data/Google Drive/Zotero/storage/253RSK4M/2411520.html;/mnt/data/Google Drive/Zotero/storage/LLNKSJAL/2411520.html;/mnt/data/Google Drive/Zotero/storage/Q6WFXKF6/2411520.html},
  langid = {english},
  number = {10}
}

@article{mcdermaidInterpretationDifferentialGene2018,
  title = {Interpretation of Differential Gene Expression Results of {{RNA}}-Seq Data: Review and Integration},
  shorttitle = {Interpretation of Differential Gene Expression Results of {{RNA}}-Seq Data},
  author = {McDermaid, Adam and Monier, Brandon and Zhao, Jing and Liu, Bingqiang and Ma, Qin},
  date = {2018-08-06},
  journaltitle = {Briefings in Bioinformatics},
  shortjournal = {Brief. Bioinformatics},
  issn = {1477-4054},
  doi = {10.1093/bib/bby067},
  abstract = {Differential gene expression (DGE) analysis is one of the most common applications of RNA-sequencing (RNA-seq) data. This process allows for the elucidation of differentially expressed genes across two or more conditions and is widely used in many applications of RNA-seq data analysis. Interpretation of the DGE results can be nonintuitive and time consuming due to the variety of formats based on the tool of choice and the numerous pieces of information provided in these results files. Here we reviewed DGE results analysis from a functional point of view for various visualizations. We also provide an R/Bioconductor package, Visualization of Differential Gene Expression Results using R, which generates information-rich visualizations for the interpretation of DGE results from three widely used tools, Cuffdiff, DESeq2 and edgeR. The implemented functions are also tested on five real-world data sets, consisting of one human, one Malus domestica and three Vitis riparia data sets.},
  eprint = {30099484},
  eprinttype = {pmid},
  langid = {english}
}

@book{mcelreathStatisticalRethinkingBayesian2018,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{Stan}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  date = {2018-01-03},
  edition = {1},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781315372495},
  url = {https://www.taylorfrancis.com/books/9781315362618},
  urldate = {2019-04-15},
  file = {/mnt/data/Google Drive/Zotero/storage/GXRZJRFH/McElreath - 2018 - Statistical Rethinking A Bayesian Course with Exa.pdf},
  isbn = {978-1-315-37249-5},
  langid = {english}
}

@book{mcelreathStatisticalRethinkingBayesian2018a,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{Stan}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  date = {2018-01-03},
  edition = {1},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781315372495},
  url = {https://www.taylorfrancis.com/books/9781315362618},
  urldate = {2019-06-21},
  file = {/mnt/data/Google Drive/Zotero/storage/L4IC4K2A/McElreath - 2018 - Statistical Rethinking A Bayesian Course with Exa.pdf},
  isbn = {978-1-315-37249-5},
  langid = {english}
}

@article{mcgowanCollaborativeEffortsForecast2019,
  title = {Collaborative Efforts to Forecast Seasonal Influenza in the {{United States}}, 2015–2016},
  author = {McGowan, Craig J. and Biggerstaff, Matthew and Johansson, Michael and Apfeldorf, Karyn M. and Ben-Nun, Michal and Brooks, Logan and Convertino, Matteo and Erraguntla, Madhav and Farrow, David C. and Freeze, John and Ghosh, Saurav and Hyun, Sangwon and Kandula, Sasikiran and Lega, Joceline and Liu, Yang and Michaud, Nicholas and Morita, Haruka and Niemi, Jarad and Ramakrishnan, Naren and Ray, Evan L. and Reich, Nicholas G. and Riley, Pete and Shaman, Jeffrey and Tibshirani, Ryan and Vespignani, Alessandro and Zhang, Qian and Reed, Carrie},
  date = {2019-01-24},
  journaltitle = {Scientific Reports},
  volume = {9},
  pages = {683},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-36361-9},
  url = {https://www.nature.com/articles/s41598-018-36361-9},
  urldate = {2021-05-30},
  abstract = {Since 2013, the Centers for Disease Control and Prevention (CDC) has hosted an annual influenza season forecasting challenge. The 2015–2016 challenge consisted of weekly probabilistic forecasts of multiple targets, including fourteen models submitted by eleven teams. Forecast skill was evaluated using a modified logarithmic score. We averaged submitted forecasts into a mean ensemble model and compared them against predictions based on historical trends. Forecast skill was highest for seasonal peak intensity and short-term forecasts, while forecast skill for timing of season onset and peak week was generally low. Higher forecast skill was associated with team participation in previous influenza forecasting challenges and utilization of ensemble forecasting techniques. The mean ensemble consistently performed well and outperformed historical trend predictions. CDC and contributing teams will continue to advance influenza forecasting and work to improve the accuracy and reliability of forecasts to facilitate increased incorporation into public health response efforts.},
  file = {/mnt/data/Google Drive/Zotero/storage/GHDYHNAE/McGowan et al. - 2019 - Collaborative efforts to forecast seasonal influen.pdf;/mnt/data/Google Drive/Zotero/storage/9ES4FDUU/s41598-018-36361-9.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{melnykovDistributionPosteriorProbabilities2013,
  title = {On the Distribution of Posterior Probabilities in Finite Mixture Models with Application in Clustering},
  author = {Melnykov, Volodymyr},
  date = {2013-11-01},
  journaltitle = {Journal of Multivariate Analysis},
  shortjournal = {Journal of Multivariate Analysis},
  volume = {122},
  pages = {175--189},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2013.07.014},
  url = {http://www.sciencedirect.com/science/article/pii/S0047259X13001462},
  urldate = {2019-05-29},
  abstract = {The paper discusses an approach based on the multivariate Delta method for approximating the distribution of posterior probabilities in finite mixture models. It can be used for developing distributions of many other characteristics involving posterior probabilities such as the entropy of fuzzy classification or expected cluster sizes. An application of the proposed methodology to clustering through merging mixture components is proposed and discussed. The methodology is studied and illustrated on simulated and well-known classification datasets with good results.},
  keywords = {BIC,Delta method,Distribution of posterior probabilities,Entropy,ICL,Model-based clustering,Multivariate Gaussian mixtures}
}

@article{mengExomebasedAnalysisRNA2013,
  title = {Exome-Based Analysis for {{RNA}} Epigenome Sequencing Data},
  author = {Meng, Jia and Cui, Xiaodong and Rao, Manjeet K. and Chen, Yidong and Huang, Yufei},
  date = {2013-06-15},
  journaltitle = {Bioinformatics (Oxford, England)},
  shortjournal = {Bioinformatics},
  volume = {29},
  pages = {1565--1567},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/btt171},
  abstract = {MOTIVATION: Fragmented RNA immunoprecipitation combined with RNA sequencing enabled the unbiased study of RNA epigenome at a near single-base resolution; however, unique features of this new type of data call for novel computational techniques. RESULT: Through examining the connections of RNA epigenome sequencing data with two well-studied data types, ChIP-Seq and RNA-Seq, we unveiled the salient characteristics of this new data type. The computational strategies were discussed accordingly, and a novel data processing pipeline was proposed that combines several existing tools with a newly developed exome-based approach 'exomePeak' for detecting, representing and visualizing the post-transcriptional RNA modification sites on the transcriptome. AVAILABILITY: The MATLAB package 'exomePeak' and additional details are available at http://compgenomics.utsa.edu/exomePeak/.},
  eprint = {23589649},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/4G6CQ7NZ/Meng et al. - 2013 - Exome-based analysis for RNA epigenome sequencing .pdf;/mnt/data/Google Drive/Zotero/storage/XTJSEZDK/btt171_Supplementary_Data.zip},
  keywords = {bearbeitet,Epigenesis; Genetic,Exome,HEK293 Cells,Humans,Immunoprecipitation,RNA Processing; Post-Transcriptional,Sequence Analysis; RNA,Software,Transcriptome},
  langid = {english},
  number = {12},
  pmcid = {PMC3673212}
}

@article{mengProtocolRNAMethylation2014,
  title = {A Protocol for {{RNA}} Methylation Differential Analysis with {{MeRIP}}-{{Seq}} Data and {{exomePeak R}}/{{Bioconductor}} Package},
  author = {Meng, Jia and Lu, Zhiliang and Liu, Hui and Zhang, Lin and Zhang, Shaowu and Chen, Yidong and Rao, Manjeet K. and Huang, Yufei},
  date = {2014-10-01},
  journaltitle = {Methods (San Diego, Calif.)},
  shortjournal = {Methods},
  volume = {69},
  pages = {274--281},
  issn = {1095-9130},
  doi = {10.1016/j.ymeth.2014.06.008},
  abstract = {Despite the prevalent studies of DNA/Chromatin related epigenetics, such as, histone modifications and DNA methylation, RNA epigenetics has not drawn deserved attention until a new affinity-based sequencing approach MeRIP-Seq was developed and applied to survey the global mRNA N6-methyladenosine (m(6)A) in mammalian cells. As a marriage of ChIP-Seq and RNA-Seq, MeRIP-Seq has the potential to study the transcriptome-wide distribution of various post-transcriptional RNA modifications. We have previously developed an R/Bioconductor package 'exomePeak' for detecting RNA methylation sites under a specific experimental condition or the identifying the differential RNA methylation sites in a case control study from MeRIP-Seq data. Compared with other relatively well studied data types such as ChIP-Seq and RNA-Seq, the study of MeRIP-Seq data is still at very early stage, and existing protocols are not optimized for dealing with the intrinsic characteristic of MeRIP-Seq data. We therein provide here a detailed and easy-to-use protocol of using exomePeak R/Bioconductor package along with other software programs for analysis of MeRIP-Seq data, which covers raw reads alignment, RNA methylation site detection, motif discovery, differential RNA methylation analysis, and functional analysis. Particularly, the rationales behind each processing step as well as the specific method used, the best practice, and possible alternative strategies are briefly discussed. The exomePeak R/Bioconductor package is freely available from Bioconductor: http://www.bioconductor.org/packages/release/bioc/html/exomePeak.html.},
  eprint = {24979058},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/ZZ5B9YM2/Meng et al. - 2014 - A protocol for RNA methylation differential analys.pdf},
  keywords = {Animals,Base Sequence,Differential RNA methylation,DNA Methylation,Epigenomics,exomePeak,Humans,MeRIP-Seq,N6-methyladenosine (m6A),RNA,RNA methylation,RNA Processing; Post-Transcriptional,RNA; Messenger,Sequence Analysis; RNA,Software},
  langid = {english},
  number = {3},
  pmcid = {PMC4194139}
}

@online{metaculusPreliminaryLookMetaculus2020,
  title = {A {{Preliminary Look}} at {{Metaculus}} and {{Expert Forecasts}}},
  author = {{Metaculus}},
  date = {2020-06-22},
  url = {https://www.metaculus.com/news/2020/06/02/LRT/},
  urldate = {2021-05-30},
  file = {/mnt/data/Google Drive/Zotero/storage/FVCKCU7K/LRT.html}
}

@online{MetalogDistributions,
  title = {The {{Metalog Distributions}}},
  url = {http://metalogdistributions.com/publications.html},
  urldate = {2019-11-06},
  file = {/mnt/data/Google Drive/Zotero/storage/37KXJM2V/publications.html}
}

@article{meyerComprehensiveAnalysisMRNA2012,
  title = {Comprehensive Analysis of {{mRNA}} Methylation Reveals Enrichment in 3' {{UTRs}} and near Stop Codons},
  author = {Meyer, Kate D. and Saletore, Yogesh and Zumbo, Paul and Elemento, Olivier and Mason, Christopher E. and Jaffrey, Samie R.},
  date = {2012-06-22},
  journaltitle = {Cell},
  shortjournal = {Cell},
  volume = {149},
  pages = {1635--1646},
  issn = {1097-4172},
  doi = {10.1016/j.cell.2012.05.003},
  abstract = {Methylation of the N(6) position of adenosine (m(6)A) is a posttranscriptional modification of RNA with poorly understood prevalence and physiological relevance. The recent discovery that FTO, an obesity risk gene, encodes an m(6)A demethylase implicates m(6)A as an important regulator of physiological processes. Here, we present a method for transcriptome-wide m(6)A localization, which combines m(6)A-specific methylated RNA immunoprecipitation with next-generation sequencing (MeRIP-Seq). We use this method to identify mRNAs of 7,676 mammalian genes that contain m(6)A, indicating that m(6)A is a common base modification of mRNA. The m(6)A modification exhibits tissue-specific regulation and is markedly increased throughout brain development. We find that m(6)A sites are enriched near stop codons and in 3' UTRs, and we uncover an association between m(6)A residues and microRNA-binding sites within 3' UTRs. These findings provide a resource for identifying transcripts that are substrates for adenosine methylation and reveal insights into the epigenetic regulation of the mammalian transcriptome.},
  eprint = {22608085},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/8T6IK7CQ/Meyer et al. - 2012 - Comprehensive analysis of mRNA methylation reveals.pdf},
  keywords = {3' Untranslated Regions,Adenosine,Codon; Terminator,Methylation,RNA Processing; Post-Transcriptional,RNA; Messenger,RNA; Untranslated,Transcriptome},
  langid = {english},
  number = {7},
  pmcid = {PMC3383396}
}

@article{meyerDynamicEpitranscriptomeN6methyladenosine2014,
  title = {The Dynamic Epitranscriptome: {{N6}}-Methyladenosine and Gene Expression Control},
  shorttitle = {The Dynamic Epitranscriptome},
  author = {Meyer, Kate D. and Jaffrey, Samie R.},
  date = {2014-05},
  journaltitle = {Nature Reviews. Molecular Cell Biology},
  shortjournal = {Nat. Rev. Mol. Cell Biol.},
  volume = {15},
  pages = {313--326},
  issn = {1471-0080},
  doi = {10.1038/nrm3785},
  abstract = {N(6)-methyladenosine (m(6)A) is a modified base that has long been known to be present in non-coding RNAs, ribosomal RNA, polyadenylated RNA and at least one mammalian mRNA. However, our understanding of the prevalence of this modification has been fundamentally redefined by transcriptome-wide m(6)A mapping studies, which have shown that m(6)A is present in a large subset of the transcriptome in specific regions of mRNA. This suggests that mRNA may undergo post-transcriptional methylation to regulate its fate and function, which is analogous to methyl modifications in DNA. Thus, the pattern of methylation constitutes an mRNA 'epitranscriptome'. The identification of adenosine methyltransferases ('writers'), m(6)A demethylating enzymes ('erasers') and m(6)A-binding proteins ('readers') is helping to define cellular pathways for the post-transcriptional regulation of mRNAs.},
  eprint = {24713629},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/89BJZS2C/Meyer and Jaffrey - 2014 - The dynamic epitranscriptome N6-methyladenosine a.pdf},
  keywords = {Adenosine,AlkB Homolog 5; RNA Demethylase,Alpha-Ketoglutarate-Dependent Dioxygenase FTO,Animals,Dioxygenases,Epigenesis; Genetic,Gene Expression,Humans,Membrane Proteins,Methylation,Methyltransferases,Proteins,RNA Stability,RNA; Messenger,Transcriptome},
  langid = {english},
  number = {5},
  pmcid = {PMC4393108}
}

@article{meyerHhh4EndemicepidemicModeling,
  title = {Hhh4: {{Endemic}}-Epidemic Modeling of Areal Count Time Series},
  author = {Meyer, Sebastian and Held, Leonhard and Höhle, Michael},
  pages = {23},
  abstract = {The availability of geocoded health data and the inherent temporal structure of communicable diseases have led to an increased interest in statistical models and software for spatio-temporal data with epidemic features. The R package surveillance can handle various levels of aggregation at which infective events have been recorded. This vignette illustrates the analysis of area-level time series of counts using the endemic-epidemic multivariate time-series model “hhh4” described in, e.g., Meyer and Held (2014, Section 3). See vignette("hhh4") for a more general introduction to hhh4 models, including the univariate and non-spatial bivariate case. We first describe the general modeling approach and then exemplify data handling, model fitting, visualization, and simulation methods for weekly counts of measles infections by district in the Weser-Ems region of Lower Saxony, Germany, 2001–2002.},
  file = {/mnt/data/Google Drive/Zotero/storage/UJLPF42G/Meyer et al. - hhh4 Endemic-epidemic modeling of areal count tim.pdf},
  langid = {english}
}

@article{meyerHhh4EndemicepidemicModelinga,
  title = {Hhh4: {{Endemic}}-Epidemic Modeling of Areal Count Time Series},
  author = {Meyer, Sebastian and Held, Leonhard and Höhle, Michael},
  pages = {23},
  abstract = {The availability of geocoded health data and the inherent temporal structure of communicable diseases have led to an increased interest in statistical models and software for spatio-temporal data with epidemic features. The R package surveillance can handle various levels of aggregation at which infective events have been recorded. This vignette illustrates the analysis of area-level time series of counts using the endemic-epidemic multivariate time-series model “hhh4” described in, e.g., Meyer and Held (2014, Section 3). See vignette("hhh4") for a more general introduction to hhh4 models, including the univariate and non-spatial bivariate case. We first describe the general modeling approach and then exemplify data handling, model fitting, visualization, and simulation methods for weekly counts of measles infections by district in the Weser-Ems region of Lower Saxony, Germany, 2001–2002.},
  file = {/mnt/data/Google Drive/Zotero/storage/2BBGK8KT/Meyer et al. - hhh4 Endemic-epidemic modeling of areal count tim.pdf},
  langid = {english}
}

@article{meyerSpatioTemporalAnalysisEpidemic2017,
  title = {Spatio-{{Temporal Analysis}} of {{Epidemic Phenomena Using}} the {{R Package}} Surveillance},
  author = {Meyer, Sebastian and Held, Leonhard and Höhle, Michael},
  date = {2017-05-03},
  journaltitle = {Journal of Statistical Software},
  volume = {77},
  pages = {1--55},
  issn = {1548-7660},
  doi = {10.18637/jss.v077.i11},
  url = {https://www.jstatsoft.org/index.php/jss/article/view/v077i11},
  urldate = {2019-10-29},
  file = {/mnt/data/Google Drive/Zotero/storage/AJL3KXFX/Meyer et al. - 2017 - Spatio-Temporal Analysis of Epidemic Phenomena Usi.pdf;/mnt/data/Google Drive/Zotero/storage/K3RTGPW8/v077i11.html},
  keywords = {branching process with immigration,endemic-epidemic modeling,infectious disease epidemiology,multivariate time series of counts,self-exciting point process,spatio-temporal surveillance data},
  langid = {english},
  number = {1}
}

@article{mittalSeqCLIPMiRNA2014,
  title = {Seq and {{CLIP}} through the {{miRNA}} World},
  author = {Mittal, Nitish and Zavolan, Mihaela},
  date = {2014-01-25},
  journaltitle = {Genome Biology},
  shortjournal = {Genome Biol.},
  volume = {15},
  pages = {202},
  issn = {1474-760X},
  doi = {10.1186/gb4151},
  abstract = {High-throughput sequencing of RNAs crosslinked to Argonaute proteins reveals not only a multitude of atypical miRNA binding sites but also of miRNA targets with atypical functions, and can be used to infer quantitative models of miRNA-target interaction strength.},
  eprint = {24460822},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/ZH9SALWU/Mittal and Zavolan - 2014 - Seq and CLIP through the miRNA world.pdf},
  keywords = {Animals,Argonaute Proteins,Binding Sites,High-Throughput Nucleotide Sequencing,Humans,Immunoprecipitation,MicroRNAs,RNA; Untranslated,Sequence Analysis; RNA},
  langid = {english},
  number = {1},
  pmcid = {PMC4053862}
}

@article{mnihPlayingAtariDeep,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  pages = {9},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  file = {/mnt/data/Google Drive/Zotero/storage/M372IDGU/Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf},
  langid = {english}
}

@article{monacheProbabilisticAspectsMeteorological2006,
  title = {Probabilistic Aspects of Meteorological and Ozone Regional Ensemble Forecasts},
  author = {Monache, Luca Delle and Hacker, Joshua P. and Zhou, Yongmei and Deng, Xingxiu and Stull, Roland B.},
  date = {2006},
  journaltitle = {Journal of Geophysical Research: Atmospheres},
  volume = {111},
  issn = {2156-2202},
  doi = {10.1029/2005JD006917},
  url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2005JD006917},
  urldate = {2020-04-07},
  abstract = {This study investigates whether probabilistic ozone forecasts from an ensemble can be made with skill: i.e., high verification resolution and reliability. Twenty-eight ozone forecasts were generated over the Lower Fraser Valley, British Columbia, Canada, for the 5-day period 11–15 August 2004 and compared with 1-hour averaged measurements of ozone concentrations at five stations. The forecasts were obtained by driving the Community Multiscale Air Quality Model (CMAQ) model with four meteorological forecasts and seven emission scenarios: a control run, ±50\% NOx, ±50\% volatile organic compounds (VOC), and ±50\% NOx combined with VOC. Probabilistic forecast quality is verified using relative operating characteristic curves, Talagrand diagrams, and a new reliability index. Results show that both meteorology and emission perturbations are needed to have a skillful probabilistic forecast system: the meteorology perturbation is important to capture the ozone temporal and spatial distribution and the emission perturbation is needed to span the range of ozone concentration magnitudes. Emission perturbations are more important than meteorology perturbations for capturing the likelihood of high ozone concentrations. Perturbations involving NOx resulted in a more skillful probabilistic forecast for the episode analyzed, and therefore the 50\% perturbation values appear to span much of the emission uncertainty for this case. All of the ensembles analyzed show a high ozone concentration bias in the Talagrand diagrams, even when the biases from the unperturbed emissions forecasts are removed from all ensemble members. This result indicates nonlinearity in the ensemble, which arises from both ozone chemistry and its interaction with input from particular meteorological models.},
  file = {/mnt/data/Google Drive/Zotero/storage/VJAUP5ZB/Monache et al. - 2006 - Probabilistic aspects of meteorological and ozone .pdf;/mnt/data/Google Drive/Zotero/storage/GT337KX3/2005JD006917.html},
  keywords = {ensemble,ozone,probabilistic forecasts},
  langid = {english},
  number = {D24}
}

@article{montero-mansoFFORMAFeaturebasedForecast2020,
  title = {{{FFORMA}}: {{Feature}}-Based Forecast Model Averaging},
  shorttitle = {{{FFORMA}}},
  author = {Montero-Manso, Pablo and Athanasopoulos, George and Hyndman, Rob J. and Talagala, Thiyanga S.},
  date = {2020-01},
  journaltitle = {International Journal of Forecasting},
  shortjournal = {International Journal of Forecasting},
  volume = {36},
  pages = {86--92},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2019.02.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019300895},
  urldate = {2020-04-06},
  abstract = {We propose an automated method for obtaining weighted forecast combinations using time series features. The proposed approach involves two phases. First, we use a collection of time series to train a meta-model to assign weights to various possible forecasting methods with the goal of minimizing the average forecasting loss obtained from a weighted forecast combination. The inputs to the meta-model are features extracted from each series. In the second phase, we forecast new series using a weighted forecast combination where the weights are obtained from our previously trained meta-model. Our method outperforms a simple forecast combination, and outperforms all of the most popular individual methods in the time series forecasting literature. The approach achieved second position in the M4 competition.},
  file = {/mnt/data/Google Drive/Zotero/storage/MNTZFESR/Montero-Manso et al. - 2020 - FFORMA Feature-based forecast model averaging.pdf},
  langid = {english},
  number = {1}
}

@article{morganHowDecisionMakers2019,
  title = {How Decision Makers Can Use Quantitative Approaches to Guide Outbreak Responses},
  author = {Morgan, Oliver},
  date = {2019-07-08},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {374},
  pages = {20180365},
  doi = {10.1098/rstb.2018.0365},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2018.0365},
  urldate = {2019-09-16},
  abstract = {Decision makers are responsible for directing staffing, logistics, selecting public health interventions, communicating to professionals and the public, planning future response needs, and establishing strategic and tactical priorities along with their funding requirements. Decision makers need to rapidly synthesize data from different experts across multiple disciplines, bridge data gaps and translate epidemiological analysis into an operational set of decisions for disease control. Analytic approaches can be defined for specific response phases: investigation, scale-up and control. These approaches include: improved applications of quantitative methods to generate insightful epidemiological descriptions of outbreaks; robust investigations of causal agents and risk factors; tools to assess response needs; identifying and monitoring optimal interventions or combinations of interventions; and forecasting for response planning. Data science and quantitative approaches can improve decision-making in outbreak response. To realize these benefits, we need to develop a structured approach that will improve the quality and timeliness of data collected during outbreaks, establish analytic teams within the response structure and define a research agenda for data analytics in outbreak response.This article is part of the theme issue ‘Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control’. This theme issue is linked with the earlier issue ‘Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes’.},
  file = {/mnt/data/Google Drive/Zotero/storage/L6K9JPT7/Morgan - 2019 - How decision makers can use quantitative approache.pdf;/mnt/data/Google Drive/Zotero/storage/CXQI7XFI/rstb.2018.html},
  number = {1776}
}

@article{morrisUsingSimulationStudies2019,
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  date = {2019},
  journaltitle = {Statistics in Medicine},
  volume = {38},
  pages = {2074--2102},
  issn = {1097-0258},
  doi = {10.1002/sim.8086},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8086},
  urldate = {2020-06-18},
  abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8086},
  file = {/mnt/data/Google Drive/Zotero/storage/SLAE8FU9/Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf;/mnt/data/Google Drive/Zotero/storage/P294K42G/sim.html},
  keywords = {graphics for simulation,Monte Carlo,simulation design,simulation reporting,simulation studies},
  langid = {english},
  number = {11}
}

@article{munzelExactPairedRank2002,
  title = {An {{Exact Paired Rank Test}}},
  author = {Munzel, Ullrich and Brunner, Edgar},
  date = {2002},
  journaltitle = {Biometrical Journal},
  volume = {44},
  pages = {584--593},
  issn = {1521-4036},
  doi = {10.1002/1521-4036(200207)44:5<584::AID-BIMJ584>3.0.CO;2-9},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1521-4036%28200207%2944%3A5%3C584%3A%3AAID-BIMJ584%3E3.0.CO%3B2-9},
  urldate = {2020-03-24},
  abstract = {An exact rank test for two dependent samples based on overall mid-ranks is discussed which can be applied to metric as well as to ordinal data. The exact conditional distribution of the test statistic given the observed vector of rank differences is determined. A recursion formula is given as well as a fast shift algorithm in SAS/IML code. Moreover, it is demonstrated that the paired rank test can be more powerful than other tests for paired samples by means of a simulation study. Finally, the test is applied to a psychiatric trial with longitudinal ordinal data.},
  file = {/mnt/data/Google Drive/Zotero/storage/ZSSNJHSV/Munzel and Brunner - 2002 - An Exact Paired Rank Test.pdf;/mnt/data/Google Drive/Zotero/storage/Z68EMMEQ/1521-4036(200207)445584AID-BIMJ5843.0.html},
  keywords = {Ordered categorical data,Ordinal data,Shift algorithm,Sign test,Ties,Wilcoxon signed rank test},
  langid = {english},
  number = {5}
}

@article{murphyImpactEnsembleForecasts1988,
  title = {The Impact of Ensemble Forecasts on Predictability},
  author = {Murphy, J. M.},
  date = {1988},
  journaltitle = {Quarterly Journal of the Royal Meteorological Society},
  volume = {114},
  pages = {463--493},
  issn = {1477-870X},
  doi = {10.1002/qj.49711448010},
  url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.49711448010},
  urldate = {2021-06-01},
  abstract = {An estimate of the mean effect of ensemble-averaging on forecast skill, under idealized ‘perfect model’ conditions, is obtained from a set of eight independent 50-day winter ensemble forecast experiments made with a hemispheric version of the Meteorological Office (UKMO) 5-level general circulation model. Each ensemble forecast consisted of seven individual integrations. Initial conditions for these were obtained by adding spatially correlated perturbations to a given wintertime analysis, and a further integration created in the same manner was used to represent nature, giving the perfect model approach. The ensemble-mean forecast shows a clear improvement in amplitude and phase skill compared with individual forecasts, the period of significant predictability for daily fields being increased by 50\%. The improvement in skill is consistent with simple theoretical estimates based on the perfect model assumption. These calculations are used to deduce how ensemble-mean forecast skill should vary with the size of ensemble. The superiority of the ensemble-mean is maintained when forecasts are spatially smoothed or time-averaged. The spread of an ensemble distribution can in principle give an a priori indication of forecast skill. A moderate level of correlation between ensemble spread and the forecast skill of the ensemble-mean is found on the hemispheric scale. The extent to which the potential benefits of ensemble forecasting may be achieved in reality depends on the model's practical forecast skill. Since the practical skill of the 5-level model is rather low, an ensemble-mean forecast is on average no better than an individual forecast up to the normal limit of deterministic predictability. However, in four experiments where the individual forecasts show skill beyond this point, the ensemble-mean forecast does give increased skill. Spatial variations in both the practical and perfect model skills of an ensemble-mean anomaly field are found to be related to corresponding variations in the statistical significance of the anomaly field. For example, the average perfect model skill, in regions where the ensemble-mean anomaly is significantly different from zero, exceeds the full field skill in all experiments for forecast days 1–15, and in all but two cases for days 16–30.},
  annotation = {\_eprint: https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.49711448010},
  file = {/mnt/data/Google Drive/Zotero/storage/7MTXUGZU/Murphy - 1988 - The impact of ensemble forecasts on predictability.pdf;/mnt/data/Google Drive/Zotero/storage/JYU9TEKU/qj.html},
  langid = {english},
  number = {480}
}

@article{murphyNoteRankedProbability1971,
  title = {A {{Note}} on the {{Ranked Probability Score}}},
  author = {Murphy, Allan H.},
  date = {1971-02-01},
  journaltitle = {Journal of Applied Meteorology},
  shortjournal = {J. Appl. Meteor.},
  volume = {10},
  pages = {155--156},
  publisher = {{American Meteorological Society}},
  issn = {0021-8952},
  doi = {10.1175/1520-0450(1971)010<0155:ANOTRP>2.0.CO;2},
  url = {https://journals.ametsoc.org/jamc/article/10/1/155/348891/A-Note-on-the-Ranked-Probability-Score},
  urldate = {2020-08-13},
  file = {/mnt/data/Google Drive/Zotero/storage/K7IF4UNP/Murphy - 1971 - A Note on the Ranked Probability Score.pdf;/mnt/data/Google Drive/Zotero/storage/PRWJQBMK/A-Note-on-the-Ranked-Probability-Score.html},
  keywords = {ranked probability score,RPS},
  langid = {english},
  number = {1}
}

@article{murphyRankedProbabilityScore1969,
  title = {On the “{{Ranked Probability Score}}”},
  author = {Murphy, Allan H.},
  date = {1969-12-01},
  journaltitle = {Journal of Applied Meteorology},
  shortjournal = {J. Appl. Meteor.},
  volume = {8},
  pages = {988--989},
  publisher = {{American Meteorological Society}},
  issn = {0021-8952},
  doi = {10.1175/1520-0450(1969)008<0988:OTPS>2.0.CO;2},
  url = {https://journals.ametsoc.org/jamc/article/8/6/988/352211/On-the-Ranked-Probability-Score},
  urldate = {2020-08-13},
  file = {/mnt/data/Google Drive/Zotero/storage/UQL3KTF4/Murphy - 1969 - On the “Ranked Probability Score”.pdf;/mnt/data/Google Drive/Zotero/storage/WRJSISSC/On-the-Ranked-Probability-Score.html},
  langid = {english},
  number = {6}
}

@article{namugayaModellingVolatilityStock2014,
  title = {Modelling {{Volatility}} of {{Stock Returns}}: {{Is GARCH}}(1,1) {{Enough}}?},
  author = {Namugaya, Jalira and Weke, Patrick G O and Charles, W M},
  date = {2014},
  journaltitle = {International Journal of Sciences},
  volume = {16},
  pages = {8},
  abstract = {In this paper, we apply the Generalized Autoregressive Conditional Heteroscedasticity (GARCH) model of different lag order to model volatility of stock returns on Uganda Securities Exchange (USE). We use the Quasi Maximum Likelihood Estimation (QMLE) method to estimate the models. Akaike Information Criteria (AIC) and Bayesian Information Criteria (BIC) are used to select the best GARCH(p,q) model. From the empirical results, it has been found that USE returns are non-normal, positively skewed and stationary. Overall, GARCH(1,1) outperformed the other GARCH(p,q) models in modeling volatility of USE returns.},
  file = {/mnt/data/Google Drive/Zotero/storage/BX25AUH9/Namugaya et al. - 2014 - Modelling Volatility of Stock Returns Is GARCH(1,.pdf},
  langid = {english},
  number = {2}
}

@online{nationalhealthcommissionoftheprcFeb13Daily,
  title = {Feb 13: {{Daily}} Briefing on Novel Coronavirus Cases in {{China}}},
  author = {{National Health Commission of the PRC}},
  url = {http://en.nhc.gov.cn/2020-02/13/c_76512.htm},
  urldate = {2020-08-12},
  file = {/mnt/data/Google Drive/Zotero/storage/62JJKCDL/c_76512.html}
}

@article{nauFittingNonseasonalARIMA,
  title = {Fitting Nonseasonal {{ARIMA}} Models},
  author = {Nau, Robert},
  pages = {21},
  file = {/mnt/data/Google Drive/Zotero/storage/4DN5KZGH/Nau - Notes on nonseasonal ARIMA models.pdf},
  langid = {english}
}

@article{nauNotesNonseasonalARIMA,
  title = {Notes on Nonseasonal {{ARIMA}} Models},
  author = {Nau, Robert},
  pages = {21},
  file = {/mnt/data/Google Drive/Zotero/storage/WVBL3IIT/Nau - Notes on nonseasonal ARIMA models.pdf},
  langid = {english}
}

@article{nishiuraEffectiveReproductionNumber2009,
  title = {The {{Effective Reproduction Number}} as a {{Prelude}} to {{Statistical Estimation}} of {{Time}}-{{Dependent Epidemic Trends}}},
  author = {Nishiura, Hiroshi and Chowell, Gerardo},
  date = {2009},
  journaltitle = {Mathematical and Statistical Estimation Approaches in Epidemiology},
  shortjournal = {Mathematical and Statistical Estimation Approaches in Epidemiology},
  pages = {103--121},
  doi = {10.1007/978-90-481-2313-1_5},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7121794/},
  urldate = {2020-08-12},
  abstract = {Although the basic reproduction number, R 0, is useful for understanding the transmissibility of a disease and designing various intervention strategies, the classic threshold quantity theoretically assumes that the epidemic first occurs in a fully susceptible population, and hence, R 0 is essentially a mathematically defined quantity. In many instances, it is of practical importance to evaluate time-dependent variations in the transmission potential of infectious diseases. Explanation of the time course of an epidemic can be partly achieved by estimating the effective reproduction number, R(t), defined as the actual average number of secondary cases per primary case at calendar time t (for t {$>$}0). R(t) shows time-dependent variation due to the decline in susceptible individuals (intrinsic factors) and the implementation of control measures (extrinsic factors). If R(t){$<$}1, it suggests that the epidemic is in decline and may be regarded as being under control at time t (vice versa, if R(t){$>$}1). This chapter describes the primer of mathematics and statistics of R(t) and discusses other similar markers of transmissibility as a function of time.},
  eprint = {null},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/MIV8KGGK/Nishiura and Chowell - 2009 - The Effective Reproduction Number as a Prelude to .pdf},
  pmcid = {PMC7121794}
}

@article{nortonIgnoranceIndifference2008,
  title = {Ignorance and {{Indifference}}*},
  author = {Norton, John D.},
  date = {2008-01},
  journaltitle = {Philosophy of Science},
  shortjournal = {Philosophy of Science},
  volume = {75},
  pages = {45--68},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/587822},
  url = {https://www.journals.uchicago.edu/doi/10.1086/587822},
  urldate = {2019-10-22},
  file = {/mnt/data/Google Drive/Zotero/storage/LXYCPXK9/Norton - 2008 - Ignorance and Indifference.pdf},
  langid = {english},
  number = {1}
}

@article{nousuStatisticalPostprocessingEnsemble2019,
  title = {Statistical Post-Processing of Ensemble Forecasts of the Height of New Snow},
  author = {Nousu, Jari-Pekka and Lafaysse, Matthieu and Vernay, Matthieu and Bellier, Joseph and Evin, Guillaume and Joly, Bruno},
  date = {2019-09-26},
  journaltitle = {Nonlinear Processes in Geophysics},
  volume = {26},
  pages = {339--357},
  issn = {1023-5809},
  doi = {10.5194/npg-26-339-2019},
  url = {https://www.nonlin-processes-geophys.net/26/339/2019/},
  urldate = {2020-04-08},
  abstract = {{$<$}p{$><$}strong{$>$}Abstract.{$<$}/strong{$>$} Forecasting the height of new snow (HN) is crucial for avalanche hazard forecasting, road viability, ski resort management and tourism attractiveness. Météo-France operates the PEARP-S2M probabilistic forecasting system, including 35 members of the PEARP Numerical Weather Prediction system, where the SAFRAN downscaling tool refines the elevation resolution and the Crocus snowpack model represents the main physical processes in the snowpack. It provides better HN forecasts than direct NWP diagnostics but exhibits significant biases and underdispersion. We applied a statistical post-processing to these ensemble forecasts, based on non-homogeneous regression with a censored shifted Gamma distribution. Observations come from manual measurements of 24\&thinsp;h HN in the French Alps and Pyrenees. The calibration is tested at the station scale and the massif scale (i.e. aggregating different stations over areas of 1000\&thinsp;km\textsuperscript{2}). Compared to the raw forecasts, similar improvements are obtained for both spatial scales. Therefore, the post-processing can be applied at any point of the massifs. Two training datasets are tested: (1) a 22-year homogeneous reforecast for which the NWP model resolution and physical options are identical to the operational system but without the same initial perturbations; (2) 3-year real-time forecasts with a heterogeneous model configuration but the same perturbation methods. The impact of the training dataset depends on lead time and on the evaluation criteria. The long-term reforecast improves the reliability of severe snowfall but leads to overdispersion due to the discrepancy in real-time perturbations. Thus, the development of reliable automatic forecasting products of HN needs long reforecasts as homogeneous as possible with the operational systems.{$<$}/p{$>$}},
  file = {/mnt/data/Google Drive/Zotero/storage/PCPFWPVD/Nousu et al. - 2019 - Statistical post-processing of ensemble forecasts .pdf;/mnt/data/Google Drive/Zotero/storage/X26LRXTJ/2019.html},
  langid = {english},
  number = {3}
}

@article{nouvelletSimpleApproachMeasure2018,
  title = {A Simple Approach to Measure Transmissibility and Forecast Incidence},
  author = {Nouvellet, Pierre and Cori, Anne and Garske, Tini and Blake, Isobel M. and Dorigatti, Ilaria and Hinsley, Wes and Jombart, Thibaut and Mills, Harriet L. and Nedjati-Gilani, Gemma and Van Kerkhove, Maria D. and Fraser, Christophe and Donnelly, Christl A. and Ferguson, Neil M. and Riley, Steven},
  date = {2018-03-01},
  journaltitle = {Epidemics},
  shortjournal = {Epidemics},
  volume = {22},
  pages = {29--35},
  issn = {1755-4365},
  doi = {10.1016/j.epidem.2017.02.012},
  url = {http://www.sciencedirect.com/science/article/pii/S1755436517300245},
  urldate = {2020-03-29},
  abstract = {Outbreaks of novel pathogens such as SARS, pandemic influenza and Ebola require substantial investments in reactive interventions, with consequent implementation plans sometimes revised on a weekly basis. Therefore, short-term forecasts of incidence are often of high priority. In light of the recent Ebola epidemic in West Africa, a forecasting exercise was convened by a network of infectious disease modellers. The challenge was to forecast unseen “future” simulated data for four different scenarios at five different time points. In a similar method to that used during the recent Ebola epidemic, we estimated current levels of transmissibility, over variable time-windows chosen in an ad hoc way. Current estimated transmissibility was then used to forecast near-future incidence. We performed well within the challenge and often produced accurate forecasts. A retrospective analysis showed that our subjective method for deciding on the window of time with which to estimate transmissibility often resulted in the optimal choice. However, when near-future trends deviated substantially from exponential patterns, the accuracy of our forecasts was reduced. This exercise highlights the urgent need for infectious disease modellers to develop more robust descriptions of processes – other than the widespread depletion of susceptible individuals – that produce non-exponential patterns of incidence.},
  file = {/mnt/data/Google Drive/Zotero/storage/6NPQQ8AP/Nouvellet et al. - 2018 - A simple approach to measure transmissibility and .pdf;/mnt/data/Google Drive/Zotero/storage/AY6K9FY5/S1755436517300245.html},
  keywords = {Branching process,Forecasting,MCMC,Rapid response,Renewal equation},
  langid = {english},
  series = {The {{RAPIDD Ebola Forecasting Challenge}}}
}

@article{nowotarskiComputingElectricitySpot2015,
  title = {Computing Electricity Spot Price Prediction Intervals Using Quantile Regression and Forecast Averaging},
  author = {Nowotarski, Jakub and Weron, Rafał},
  date = {2015-09-01},
  journaltitle = {Computational Statistics},
  shortjournal = {Comput Stat},
  volume = {30},
  pages = {791--803},
  issn = {1613-9658},
  doi = {10.1007/s00180-014-0523-0},
  url = {https://doi.org/10.1007/s00180-014-0523-0},
  urldate = {2020-05-27},
  abstract = {We examine possible accuracy gains from forecast averaging in the context of interval forecasts of electricity spot prices. First, we test whether constructing empirical prediction intervals (PI) from combined electricity spot price forecasts leads to better forecasts than those obtained from individual methods. Next, we propose a new method for constructing PI—Quantile Regression Averaging (QRA)—which utilizes the concept of quantile regression and a pool of point forecasts of individual (i.e. not combined) models. While the empirical PI from combined forecasts do not provide significant gains, the QRA-based PI are found to be more accurate than those of the best individual model—the smoothed nonparametric autoregressive model.},
  file = {/mnt/data/Google Drive/Zotero/storage/73B9W3G7/Nowotarski and Weron - 2015 - Computing electricity spot price prediction interv.pdf},
  langid = {english},
  number = {3}
}

@article{nowotarskiComputingElectricitySpot2015a,
  title = {Computing Electricity Spot Price Prediction Intervals Using Quantile Regression and Forecast Averaging},
  author = {Nowotarski, Jakub and Weron, Rafał},
  date = {2015-09-01},
  journaltitle = {Computational Statistics},
  shortjournal = {Comput Stat},
  volume = {30},
  pages = {791--803},
  issn = {1613-9658},
  doi = {10.1007/s00180-014-0523-0},
  url = {https://doi.org/10.1007/s00180-014-0523-0},
  urldate = {2020-08-10},
  abstract = {We examine possible accuracy gains from forecast averaging in the context of interval forecasts of electricity spot prices. First, we test whether constructing empirical prediction intervals (PI) from combined electricity spot price forecasts leads to better forecasts than those obtained from individual methods. Next, we propose a new method for constructing PI—Quantile Regression Averaging (QRA)—which utilizes the concept of quantile regression and a pool of point forecasts of individual (i.e. not combined) models. While the empirical PI from combined forecasts do not provide significant gains, the QRA-based PI are found to be more accurate than those of the best individual model—the smoothed nonparametric autoregressive model.},
  file = {/mnt/data/Google Drive/Zotero/storage/9RL8366L/Nowotarski and Weron - 2015 - Computing electricity spot price prediction interv.pdf},
  keywords = {ensemble,qra,Quantile Regression Average},
  langid = {english},
  number = {3}
}

@article{oidtmanTradeoffsIndividualEnsemble2021,
  title = {Trade-Offs between Individual and Ensemble Forecasts of an Emerging Infectious Disease},
  author = {Oidtman, Rachel J. and Omodei, Elisa and Kraemer, Moritz U. G. and Castañeda-Orjuela, Carlos A. and Cruz-Rivera, Erica and Misnaza-Castrillón, Sandra and Cifuentes, Myriam Patricia and Rincon, Luz Emilse and Cañon, Viviana and de Alarcon, Pedro and España, Guido and Huber, John H. and Hill, Sarah C. and Barker, Christopher M. and Johansson, Michael A. and Manore, Carrie A. and Reiner, Robert C. and Rodriguez-Barraquer, Isabel and Siraj, Amir S. and Frias-Martinez, Enrique and García-Herranz, Manuel and Perkins, T. Alex},
  date = {2021-03-01},
  journaltitle = {medRxiv},
  pages = {2021.02.25.21252363},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2021.02.25.21252363},
  url = {https://www.medrxiv.org/content/10.1101/2021.02.25.21252363v1},
  urldate = {2021-03-10},
  abstract = {{$<$}p{$>$}When new pathogens emerge, numerous questions arise about their future spread, some of which can be addressed with probabilistic forecasts. The many uncertainties about the epidemiology of emerging pathogens can make it difficult to choose among model structures and assumptions, however. To assess the potential for uncertainties about emerging pathogens to affect forecasts of their spread, we evaluated the performance of a suite of 16 forecasting models in the context of the 2015-2016 Zika epidemic in Colombia. Each model featured a different combination of assumptions about the role of human mobility in driving transmission, spatiotemporal variation in transmission potential, and the number of times the virus was introduced. All models used the same core transmission model and the same iterative data assimilation algorithm to generate forecasts. By assessing forecast performance through time using logarithmic scoring with ensemble weighting, we found that which model assumptions had the most ensemble weight changed through time. In particular, spatially coupled models had higher ensemble weights in the early and late phases of the epidemic, whereas non-spatial models had higher ensemble weights at the peak of the epidemic. We compared forecast performance of the equally-weighted ensemble model to each individual model and identified a trade-off whereby certain individual models outperformed the ensemble model early in the epidemic but the ensemble model outperformed all individual models on average. On balance, our results suggest that suites of models that span uncertainty across alternative assumptions are necessary to obtain robust forecasts in the context of emerging infectious diseases.{$<$}/p{$>$}},
  file = {/mnt/data/Google Drive/Zotero/storage/4P9NALWV/Oidtman et al. - 2021 - Trade-offs between individual and ensemble forecas.pdf;/mnt/data/Google Drive/Zotero/storage/E2FPME9E/2021.02.25.html;/mnt/data/Google Drive/Zotero/storage/UAULNVQ4/2021.02.25.html},
  langid = {english}
}

@online{OptimisingRenewalModels,
  title = {Optimising {{Renewal Models}} for {{Real}}-{{Time Epidemic Prediction}} and {{Estimation}} | {{bioRxiv}}},
  url = {https://www.biorxiv.org/content/10.1101/835181v2.full},
  urldate = {2020-04-02},
  file = {/mnt/data/Google Drive/Zotero/storage/8ZTZEYVU/835181v2.html}
}

@article{OrderEstimationAccumulated,
  title = {Order {{Estimation}} by {{Accumulated Prediction Errors}}},
  pages = {8},
  abstract = {This paper presents a new criterion based on prediction error which allows the estimation of the number of parameters as well as structures in statistical models. The criterion is valid for short and long samples alike. Unlike Akaike's earlier criterion, also based on prediction error, the criterion proposed here appears to produce consistent error estimates in ARMA processes.},
  file = {/mnt/data/Google Drive/Zotero/storage/JTIXZW74/Order Estimation by Accumulated Prediction Errors.pdf},
  langid = {english}
}

@report{paragOptimisingRenewalModels2019,
  title = {Optimising {{Renewal Models}} for {{Real}}-{{Time Epidemic Prediction}} and {{Estimation}}},
  author = {Parag, Kv and Donnelly, Ca},
  date = {2019-11-08},
  institution = {{Bioinformatics}},
  doi = {10.1101/835181},
  url = {http://biorxiv.org/lookup/doi/10.1101/835181},
  urldate = {2020-04-03},
  abstract = {Abstract                        The effective reproduction number,             R                            t                          , is an important prognostic for infectious disease epidemics. Significant changes in             R                            t                          can forewarn about new transmissions or predict the efficacy of interventions. The renewal model infers             R                            t                          from incidence data and has been applied to Ebola virus disease and pandemic influenza outbreaks, among others. This model estimates             R                            t                          using a sliding window of length             k             . While this facilitates real-time detection of statistically significant             R                            t                          fluctuations, inference is highly             k             -sensitive. Models with too large or small             k             might ignore meaningful changes or over-interpret noise-induced ones. No principled             k             -selection scheme exists. We develop a practical yet rigorous scheme using the accumulated prediction error (APE) metric from information theory. We derive exact incidence prediction distributions and integrate these within an APE framework to identify the             k             best supported by available data. We find that this             k             optimises short-term prediction accuracy and expose how common, heuristic             k             -choices, which seem sensible, could be misleading.},
  file = {/mnt/data/Google Drive/Zotero/storage/3VCUX3E7/Parag and Donnelly - 2019 - Optimising Renewal Models for Real-Time Epidemic P.pdf},
  langid = {english},
  type = {preprint}
}

@article{petropoulosWisdomDataGetting2021,
  title = {The {{Wisdom}} of the {{Data}}: {{Getting}} the {{Most Out}} of {{Univariate Time Series Forecasting}}},
  shorttitle = {The {{Wisdom}} of the {{Data}}},
  author = {Petropoulos, Fotios and Spiliotis, Evangelos},
  date = {2021-09},
  journaltitle = {Forecasting},
  volume = {3},
  pages = {478--497},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/forecast3030029},
  url = {https://www.mdpi.com/2571-9394/3/3/29},
  urldate = {2021-07-11},
  abstract = {Forecasting is a challenging task that typically requires making assumptions about the observed data but also the future conditions. Inevitably, any forecasting process will result in some degree of inaccuracy. The forecasting performance will further deteriorate as the uncertainty increases. In this article, we focus on univariate time series forecasting and we review five approaches that one can use to enhance the performance of standard extrapolation methods. Much has been written about the “wisdom of the crowds” and how collective opinions will outperform individual ones. We present the concept of the “wisdom of the data” and how data manipulation can result in information extraction which, in turn, translates to improved forecast accuracy by aggregating (combining) forecasts computed on different perspectives of the same data. We describe and discuss approaches that are based on the manipulation of local curvatures (theta method), temporal aggregation, bootstrapping, sub-seasonal and incomplete time series. We compare these approaches with regards to how they extract information from the data, their computational cost, and their performance.},
  file = {/mnt/data/Google Drive/Zotero/storage/NP44WTN5/Petropoulos and Spiliotis - 2021 - The Wisdom of the Data Getting the Most Out of Un.pdf;/mnt/data/Google Drive/Zotero/storage/D7XRHTHB/29.html},
  issue = {3},
  keywords = {bagging,combination,information,sub-seasonal series,temporal aggregation,theta,uncertainty},
  langid = {english},
  number = {3}
}

@article{phillipsAsymptoticPropertiesResidual1990,
  title = {Asymptotic {{Properties}} of {{Residual Based Tests}} for {{Cointegration}}},
  author = {Phillips, P. C. B. and Ouliaris, S.},
  date = {1990},
  journaltitle = {Econometrica},
  volume = {58},
  pages = {165--193},
  issn = {0012-9682},
  doi = {10.2307/2938339},
  abstract = {This paper develops an asymptotic theory for residual based tests for cointegration. These tests involve procedures that are designed to detect the presence of a unit root in the residuals of (cointegrating) regressions among the levels of economic time series. Attention is given to the augmented Dickey-Fuller (ADF) test that is recommended by Engle-Granger (1987) and the Z\textsubscript{α} and Z\textsubscript{t} unit root tests recently proposed by Phillips (1987). Two new tests are also introduced, one of which is invariant to the normalization of the cointegrating regression. All of these tests are shown to be asymptotically similar and simple representations of their limiting distributions are given in terms of standard Brownian motion. The ADF and Z\textsubscript{t} tests are asymptotically equivalent. Power properties of the tests are also studied. The analysis shows that all the tests are consistent if suitably constructed but that the ADF and Z\textsubscript{t} tests have slower rates of divergence under cointegration than the other tests. This indicates that, at least in large samples, the Z\textsubscript{α} test should have superior power properties. The paper concludes by addressing the larger issue of test formulation. Some major pitfalls are discovered in procedures that are designed to test a null of cointegration (rather than no cointegration). These defects provide strong arguments against the indiscriminate use of such test formulations and support the continuing use of residual based unit root tests. A full set of critical values for residual based tests is included. These allow for demeaned and detrended data and cointegrating regressions with up to five variables.},
  eprint = {2938339},
  eprinttype = {jstor},
  file = {/mnt/data/Google Drive/Zotero/storage/6AEVGRF9/Phillips and Ouliaris - 1990 - Asymptotic Properties of Residual Based Tests for .pdf},
  number = {1}
}

@article{pickrellUnderstandingMechanismsUnderlying2010,
  title = {Understanding Mechanisms Underlying Human Gene Expression Variation with {{RNA}} Sequencing},
  author = {Pickrell, Joseph K. and Marioni, John C. and Pai, Athma A. and Degner, Jacob F. and Engelhardt, Barbara E. and Nkadori, Everlyne and Veyrieras, Jean-Baptiste and Stephens, Matthew and Gilad, Yoav and Pritchard, Jonathan K.},
  date = {2010-04-01},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {464},
  pages = {768--772},
  issn = {1476-4687},
  doi = {10.1038/nature08872},
  abstract = {Understanding the genetic mechanisms underlying natural variation in gene expression is a central goal of both medical and evolutionary genetics, and studies of expression quantitative trait loci (eQTLs) have become an important tool for achieving this goal. Although all eQTL studies so far have assayed messenger RNA levels using expression microarrays, recent advances in RNA sequencing enable the analysis of transcript variation at unprecedented resolution. We sequenced RNA from 69 lymphoblastoid cell lines derived from unrelated Nigerian individuals that have been extensively genotyped by the International HapMap Project. By pooling data from all individuals, we generated a map of the transcriptional landscape of these cells, identifying extensive use of unannotated untranslated regions and more than 100 new putative protein-coding exons. Using the genotypes from the HapMap project, we identified more than a thousand genes at which genetic variation influences overall expression levels or splicing. We demonstrate that eQTLs near genes generally act by a mechanism involving allele-specific expression, and that variation that influences the inclusion of an exon is enriched within and near the consensus splice sites. Our results illustrate the power of high-throughput sequencing for the joint analysis of variation in transcription, splicing and allele-specific expression across individuals.},
  eprint = {20220758},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/GYDKG9XQ/Pickrell et al. - 2010 - Understanding mechanisms underlying human gene exp.pdf},
  keywords = {African Continental Ancestry Group,Alleles,Consensus Sequence,DNA; Complementary,Exons,Gene Expression Profiling,Gene Expression Regulation,Genetic Variation,Humans,Nigeria,Polymorphism; Single Nucleotide,Quantitative Trait Loci,RNA Splice Sites,RNA; Messenger,Sequence Analysis; RNA,Transcription; Genetic},
  langid = {english},
  number = {7289},
  pmcid = {PMC3089435}
}

@article{piegorschMaximumLikelihoodEstimation1990,
  title = {Maximum {{Likelihood Estimation}} for the {{Negative Binomial Dispersion Parameter}}},
  author = {Piegorsch, Walter W.},
  date = {1990-09},
  journaltitle = {Biometrics},
  volume = {46},
  pages = {863},
  issn = {0006341X},
  doi = {10.2307/2532104},
  abstract = {A follow-up investigation to that given by Clark and Perry (1989, Biometrics 45, 309-316) is presented, giving details for maximum likelihood estimation for the dispersion parameter from a negative binomial distribution.},
  eprint = {2532104},
  eprinttype = {jstor},
  file = {/mnt/data/Google Drive/Zotero/storage/W6HQ2J8A/Piegorsch - 1990 - Maximum Likelihood Estimation for the Negative Bin.pdf},
  langid = {english},
  number = {3}
}

@article{qiSampleexpandMethodPredicting2014,
  title = {Sample-Expand Method for Predicting the Specified Structure of Microporous Aluminophosphate},
  author = {Qi, Miao and Qin, Zhanmin and Gao, Na and Kong, Jun and Guo, Yuting and Lu, Yinghua},
  date = {2014-02-01},
  journaltitle = {Microporous and Mesoporous Materials},
  shortjournal = {Microporous and Mesoporous Materials},
  volume = {185},
  pages = {1--6},
  issn = {1387-1811},
  doi = {10.1016/j.micromeso.2013.10.009},
  url = {http://www.sciencedirect.com/science/article/pii/S1387181113005167},
  urldate = {2020-04-15},
  abstract = {Imbalanced data sets often exist in many real-world fields and this problem has got more and more attention in recent years. In this paper, a sample-expand method is proposed as data pre-processing procedure to improve the predictive performance of the zeolite synthesis on imbalance data set. First, the data pre-processing is implemented for expanding samples by exploring the marginal structure of the given data set using k-nearest neighbor algorithm (KNN). Then, the expanded data set is input to support vector machines (SVM) for classification. Finally, Q times n-fold cross-validations procedure (CVs) is adopted to assess the prediction performance. The advantage of the data pre-processing is that it can obtain stable data set for establishing the training model and abide by the classification criteria of SVM, such that the improved predictive performance is achievable. Moreover, other classical machine learning methods are also presented to accomplish the prediction task. Compared experimental results demonstrate that SVM method can reach very satisfactory predictive accuracy on the pre-processing data set. Specially, the phase diagram of gel composition is provided as a guiding role for subsequent rational synthesis experiments.},
  file = {/mnt/data/Google Drive/Zotero/storage/XIGBX538/S1387181113005167.html},
  keywords = {-nearest neighbor,Prediction,Sample-expand,Support vector machines,Zeolite synthesis},
  langid = {english}
}

@online{QuickNoteWhat2018,
  title = {A Quick Note What {{I}} Infer from P\_loo and {{Pareto}} k Values},
  date = {2018-03-03T13:07:43+00:00},
  url = {https://discourse.mc-stan.org/t/a-quick-note-what-i-infer-from-p-loo-and-pareto-k-values/3446/2},
  urldate = {2020-01-14},
  abstract = {Nice!  A quick question: How do you define \# of parameters in hierarchical models? Does the \# of poarameters in this setting depend on a sparse (very few parameters due to pooling) and a dense (many parameters) data situation?  It would be nice to discuss this bit specifically in the case study given how ubiquotus hierarchical models are used.},
  file = {/mnt/data/Google Drive/Zotero/storage/Z4AL42QY/3446.html},
  langid = {american},
  organization = {{The Stan Forums}}
}

@article{radProfitabilityPairsTrading2016,
  title = {The Profitability of Pairs Trading Strategies: Distance, Cointegration and Copula Methods},
  shorttitle = {The Profitability of Pairs Trading Strategies},
  author = {Rad, Hossein and Low, Rand Kwong Yew and Faff, Robert},
  date = {2016-10-02},
  journaltitle = {Quantitative Finance},
  volume = {16},
  pages = {1541--1558},
  issn = {1469-7688},
  doi = {10.1080/14697688.2016.1164337},
  url = {https://doi.org/10.1080/14697688.2016.1164337},
  urldate = {2019-09-14},
  abstract = {We perform an extensive and robust study of the performance of three different pairs trading strategies—the distance, cointegration and copula methods—on the entire US equity market from 1962 to 2014 with time-varying trading costs. For the cointegration and copula methods, we design a computationally efficient two-step pairs trading strategy. In terms of economic outcomes, the distance, cointegration and copula methods show a mean monthly excess return of 91, 85 and 43 bps (38, 33 and 5 bps) before transaction costs (after transaction costs), respectively. In terms of continued profitability, from 2009, the frequency of trading opportunities via the distance and cointegration methods is reduced considerably, whereas this frequency remains stable for the copula method. Further, the copula method shows better performance for its unconverged trades compared to those of the other methods. While the liquidity factor is negatively correlated to all strategies’ returns, we find no evidence of their correlation to market excess returns. All strategies show positive and significant alphas after accounting for various risk-factors. We also find that in addition to all strategies performing better during periods of significant volatility, the cointegration method is the superior strategy during turbulent market conditions.},
  file = {/mnt/data/Google Drive/Zotero/storage/WKCU8TDD/Rad et al. - 2016 - The profitability of pairs trading strategies dis.pdf;/mnt/data/Google Drive/Zotero/storage/JRE6IUKF/14697688.2016.html},
  keywords = {Cointegration,Copula,G11,G12,G14,Pairs trading,Quantitative strategies,Statistical arbitrage},
  number = {10}
}

@article{rafteryBayesianModelAveraging1997,
  title = {Bayesian {{Model Averaging}} for {{Linear Regression Models}}},
  author = {Raftery, Adrian E. and Madigan, David and Hoeting, Jennifer A.},
  date = {1997-03-01},
  journaltitle = {Journal of the American Statistical Association},
  volume = {92},
  pages = {179--191},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1997.10473615},
  url = {https://doi.org/10.1080/01621459.1997.10473615},
  urldate = {2020-08-23},
  abstract = {We consider the problem of accounting for model uncertainty in linear regression models. Conditioning on a single selected model ignores model uncertainty, and thus leads to the underestimation of uncertainty when making inferences about quantities of interest. A Bayesian solution to this problem involves averaging over all possible models (i.e., combinations of predictors) when making inferences about quantities of interest. This approach is often not practical. In this article we offer two alternative approaches. First, we describe an ad hoc procedure, “Occam's window,” which indicates a small set of models over which a model average can be computed. Second, we describe a Markov chain Monte Carlo approach that directly approximates the exact solution. In the presence of model uncertainty, both of these model averaging procedures provide better predictive performance than any single model that might reasonably have been selected. In the extreme case where there are many candidate predictors but no relationship between any of them and the response, standard variable selection procedures often choose some subset of variables that yields a high R 2 and a highly significant overall F value. In this situation, Occam's window usually indicates the null model (or a small number of models including the null model) as the only one (or ones) to be considered thus largely resolving the problem of selecting significant models when there is no signal in the data. Software to implement our methods is available from StatLib.},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.1997.10473615},
  file = {/mnt/data/Google Drive/Zotero/storage/8PJJD37C/Raftery et al. - 1997 - Bayesian Model Averaging for Linear Regression Mod.pdf;/mnt/data/Google Drive/Zotero/storage/UCQQ9WVM/01621459.1997.html},
  keywords = {Bayes factor,Markov chain Monte Carlo model composition,Model uncertainty,Occam's window,Posterior model probability},
  number = {437}
}

@article{rafteryUsingBayesianModel2005,
  title = {Using {{Bayesian Model Averaging}} to {{Calibrate Forecast Ensembles}}},
  author = {Raftery, Adrian E. and Gneiting, Tilmann and Balabdaoui, Fadoua and Polakowski, Michael},
  date = {2005-05-01},
  journaltitle = {Monthly Weather Review},
  shortjournal = {Mon. Wea. Rev.},
  volume = {133},
  pages = {1155--1174},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/MWR2906.1},
  url = {https://journals.ametsoc.org/doi/full/10.1175/MWR2906.1},
  urldate = {2020-06-02},
  abstract = {Ensembles used for probabilistic weather forecasting often exhibit a spread-error correlation, but they tend to be underdispersive. This paper proposes a statistical method for postprocessing ensembles based on Bayesian model averaging (BMA), which is a standard method for combining predictive distributions from different sources. The BMA predictive probability density function (PDF) of any quantity of interest is a weighted average of PDFs centered on the individual bias-corrected forecasts, where the weights are equal to posterior probabilities of the models generating the forecasts and reflect the models' relative contributions to predictive skill over the training period. The BMA weights can be used to assess the usefulness of ensemble members, and this can be used as a basis for selecting ensemble members; this can be useful given the cost of running large ensembles. The BMA PDF can be represented as an unweighted ensemble of any desired size, by simulating from the BMA predictive distribution. The BMA predictive variance can be decomposed into two components, one corresponding to the between-forecast variability, and the second to the within-forecast variability. Predictive PDFs or intervals based solely on the ensemble spread incorporate the first component but not the second. Thus BMA provides a theoretical explanation of the tendency of ensembles to exhibit a spread-error correlation but yet be underdispersive. The method was applied to 48-h forecasts of surface temperature in the Pacific Northwest in January–June 2000 using the University of Washington fifth-generation Pennsylvania State University–NCAR Mesoscale Model (MM5) ensemble. The predictive PDFs were much better calibrated than the raw ensemble, and the BMA forecasts were sharp in that 90\% BMA prediction intervals were 66\% shorter on average than those produced by sample climatology. As a by-product, BMA yields a deterministic point forecast, and this had root-mean-square errors 7\% lower than the best of the ensemble members and 8\% lower than the ensemble mean. Similar results were obtained for forecasts of sea level pressure. Simulation experiments show that BMA performs reasonably well when the underlying ensemble is calibrated, or even overdispersed.},
  file = {/mnt/data/Google Drive/Zotero/storage/BVZ3BIIP/Raftery et al. - 2005 - Using Bayesian Model Averaging to Calibrate Foreca.pdf;/mnt/data/Google Drive/Zotero/storage/HZJ3R7W2/MWR2906.html},
  number = {5}
}

@article{rayPredictionInfectiousDisease2018,
  title = {Prediction of Infectious Disease Epidemics via Weighted Density Ensembles},
  author = {Ray, Evan L. and Reich, Nicholas G.},
  editor = {Viboud, Cecile},
  date = {2018-02-20},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {14},
  pages = {e1005910},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005910},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1005910},
  urldate = {2020-12-15},
  file = {/mnt/data/Google Drive/Zotero/storage/PSNK58EF/Ray and Reich - 2018 - Prediction of infectious disease epidemics via wei.pdf},
  langid = {english},
  number = {2}
}

@article{recchiaHowWellDid2021,
  title = {How Well Did Experts and Laypeople Forecast the Size of the {{COVID}}-19 Pandemic?},
  author = {Recchia, Gabriel and Freeman, Alexandra L. J. and Spiegelhalter, David},
  date = {2021-05-05},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {16},
  pages = {e0250935},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0250935},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0250935},
  urldate = {2021-06-02},
  abstract = {Throughout the COVID-19 pandemic, social and traditional media have disseminated predictions from experts and nonexperts about its expected magnitude. How accurate were the predictions of ‘experts’—individuals holding occupations or roles in subject-relevant fields, such as epidemiologists and statisticians—compared with those of the public? We conducted a survey in April 2020 of 140 UK experts and 2,086 UK laypersons; all were asked to make four quantitative predictions about the impact of COVID-19 by 31 Dec 2020. In addition to soliciting point estimates, we asked participants for lower and higher bounds of a range that they felt had a 75\% chance of containing the true answer. Experts exhibited greater accuracy and calibration than laypersons, even when restricting the comparison to a subset of laypersons who scored in the top quartile on a numeracy test. Even so, experts substantially underestimated the ultimate extent of the pandemic, and the mean number of predictions for which the expert intervals contained the actual outcome was only 1.8 (out of 4), suggesting that experts should consider broadening the range of scenarios they consider plausible. Predictions of the public were even more inaccurate and poorly calibrated, suggesting that an important role remains for expert predictions as long as experts acknowledge their uncertainty.},
  file = {/mnt/data/Google Drive/Zotero/storage/X2FMBK56/Recchia et al. - 2021 - How well did experts and laypeople forecast the si.pdf;/mnt/data/Google Drive/Zotero/storage/TT7K727A/article.html},
  keywords = {COVID 19,Epidemiological statistics,Forecasting,Numeracy,Pandemics,Probability distribution,Statistical distributions,Virus testing},
  langid = {english},
  number = {5}
}

@article{reichAccuracyRealtimeMultimodel2019,
  title = {Accuracy of Real-Time Multi-Model Ensemble Forecasts for Seasonal Influenza in the {{U}}.{{S}}.},
  author = {Reich, Nicholas G. and McGowan, Craig J. and Yamana, Teresa K. and Tushar, Abhinav and Ray, Evan L. and Osthus, Dave and Kandula, Sasikiran and Brooks, Logan C. and Crawford-Crudell, Willow and Gibson, Graham Casey and Moore, Evan and Silva, Rebecca and Biggerstaff, Matthew and Johansson, Michael A. and Rosenfeld, Roni and Shaman, Jeffrey},
  date = {2019-11-22},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {15},
  pages = {e1007486},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007486},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007486},
  urldate = {2020-08-07},
  abstract = {Seasonal influenza results in substantial annual morbidity and mortality in the United States and worldwide. Accurate forecasts of key features of influenza epidemics, such as the timing and severity of the peak incidence in a given season, can inform public health response to outbreaks. As part of ongoing efforts to incorporate data and advanced analytical methods into public health decision-making, the United States Centers for Disease Control and Prevention (CDC) has organized seasonal influenza forecasting challenges since the 2013/2014 season. In the 2017/2018 season, 22 teams participated. A subset of four teams created a research consortium called the FluSight Network in early 2017. During the 2017/2018 season they worked together to produce a collaborative multi-model ensemble that combined 21 separate component models into a single model using a machine learning technique called stacking. This approach creates a weighted average of predictive densities where the weight for each component is determined by maximizing overall ensemble accuracy over past seasons. In the 2017/2018 influenza season, one of the largest seasonal outbreaks in the last 15 years, this multi-model ensemble performed better on average than all individual component models and placed second overall in the CDC challenge. It also outperformed the baseline multi-model ensemble created by the CDC that took a simple average of all models submitted to the forecasting challenge. This project shows that collaborative efforts between research teams to develop ensemble forecasting approaches can bring measurable improvements in forecast accuracy and important reductions in the variability of performance from year to year. Efforts such as this, that emphasize real-time testing and evaluation of forecasting models and facilitate the close collaboration between public health officials and modeling researchers, are essential to improving our understanding of how best to use forecasts to improve public health response to seasonal and emerging epidemic threats.},
  file = {/mnt/data/Google Drive/Zotero/storage/IJEAL83T/Reich et al. - 2019 - Accuracy of real-time multi-model ensemble forecas.pdf;/mnt/data/Google Drive/Zotero/storage/A2M4D7R7/article.html},
  keywords = {Body weight,ensemble,Epidemiology,forecast,Forecasting,Infectious disease surveillance,Infectious diseases,Influenza,prediction,Public and occupational health,Seasons},
  langid = {english},
  number = {11}
}

@article{renSmallRNAsMeet2014,
  title = {Small {{RNAs}} Meet Their Targets: When Methylation Defends {{miRNAs}} from Uridylation},
  shorttitle = {Small {{RNAs}} Meet Their Targets},
  author = {Ren, Guodong and Chen, Xuemei and Yu, Bin},
  date = {2014},
  journaltitle = {RNA biology},
  shortjournal = {RNA Biol},
  volume = {11},
  pages = {1099--1104},
  issn = {1555-8584},
  doi = {10.4161/rna.36243},
  abstract = {Small RNAs are incorporated into Argonaute protein-containing complexes to guide the silencing of target RNAs in both animals and plants. The abundance of endogenous small RNAs is precisely controlled at multiple levels including transcription, processing and Argonaute loading. In addition to these processes, 3' end modification of small RNAs, the topic of a research area that has rapidly evolved over the last several years, adds another layer of regulation of their abundance, diversity and function. Here, we review our recent understanding of small RNA 3' end methylation and tailing.},
  eprint = {25483033},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/MKJ7WUQC/Ren et al. - 2014 - Small RNAs meet their targets when methylation de.pdf},
  keywords = {Animals,argonaute,Argonaute Proteins,Humans,methylation,Methylation,MicroRNAs,RNA 3' End Processing,RNA; Small Interfering,small RNA,Uridine,uridylation},
  langid = {english},
  number = {9},
  pmcid = {PMC4615765}
}

@article{reschenhoferDoesAnyoneNeed2013,
  title = {Does {{Anyone Need}} a {{GARCH}}(1,1)?},
  author = {Reschenhofer, Erhard},
  date = {2013-01-23},
  journaltitle = {Journal of Finance and Accounting},
  volume = {1},
  pages = {48--53},
  doi = {10.12691/jfa-1-2-2},
  url = {http://pubs.sciepub.com/jfa/1/2/2/abstract.html},
  urldate = {2019-09-13},
  abstract = {Hansen and Lunde [16] posed the question Does anything beat a GARCH(1,1)? and compared a large number of parametric volatility models in an extensive empirical study. They found that no other model provides significantly better forecasts than the GARCH(1,1) model. In contrast, this paper arrives at the conclusion that simple robust estimators such as weighted medians of past (squared) returns outperform the GARCH(1,1) model both in-sample as well as out-of-sample. This conclusion is based on theoretical arguments as well as on empirical evidence.},
  file = {/mnt/data/Google Drive/Zotero/storage/XFEPCCHP/Reschenhofer - 2013 - Does Anyone Need a GARCH(1,1).pdf;/mnt/data/Google Drive/Zotero/storage/ZG5Q5NGX/index.html},
  langid = {english},
  number = {2}
}

@article{rissanenOrderEstimationAccumulated1986,
  title = {Order {{Estimation}} by {{Accumulated Prediction Errors}}},
  author = {Rissanen, Jorma},
  date = {1986},
  journaltitle = {Journal of Applied Probability},
  volume = {23},
  pages = {55--61},
  issn = {0021-9002},
  doi = {10.2307/3214342},
  abstract = {This paper presents a new criterion based on prediction error which allows the estimation of the number of parameters as well as structures in statistical models. The criterion is valid for short and long samples alike. Unlike Akaike's earlier criterion, also based on prediction error, the criterion proposed here appears to produce consistent error estimates in ARMA processes.},
  eprint = {3214342},
  eprinttype = {jstor}
}

@article{rissoNormalizationRNAseqData2014,
  title = {Normalization of {{RNA}}-Seq Data Using Factor Analysis of Control Genes or Samples},
  author = {Risso, Davide and Ngai, John and Speed, Terence P. and Dudoit, Sandrine},
  date = {2014-09},
  journaltitle = {Nature Biotechnology},
  volume = {32},
  pages = {896--902},
  issn = {1546-1696},
  doi = {10.1038/nbt.2931},
  url = {https://www.nature.com/articles/nbt.2931},
  urldate = {2019-04-16},
  abstract = {Normalization of RNA-sequencing (RNA-seq) data has proven essential to ensure accurate inference of expression levels. Here, we show that usual normalization approaches mostly account for sequencing depth and fail to correct for library preparation and other more complex unwanted technical effects. We evaluate the performance of the External RNA Control Consortium (ERCC) spike-in controls and investigate the possibility of using them directly for normalization. We show that the spike-ins are not reliable enough to be used in standard global-scaling or regression-based normalization procedures. We propose a normalization strategy, called remove unwanted variation (RUV), that adjusts for nuisance technical effects by performing factor analysis on suitable sets of control genes (e.g., ERCC spike-ins) or samples (e.g., replicate libraries). Our approach leads to more accurate estimates of expression fold-changes and tests of differential expression compared to state-of-the-art normalization methods. In particular, RUV promises to be valuable for large collaborative projects involving multiple laboratories, technicians, and/or sequencing platforms.},
  file = {/mnt/data/Google Drive/Zotero/storage/DBFSSZWX/Risso et al. - 2014 - Normalization of RNA-seq data using factor analysi.pdf;/mnt/data/Google Drive/Zotero/storage/YMGZHS9D/nbt.html},
  langid = {english},
  number = {9}
}

@article{rissoRUVSeqRemoveUnwanted,
  title = {{{RUVSeq}}: {{Remove Unwanted Variation}} from {{RNA}}-{{Seq Data}}},
  author = {Risso, Davide},
  pages = {9},
  file = {/mnt/data/Google Drive/Zotero/storage/MU7WPLGA/Risso - RUVSeq Remove Unwanted Variation from RNA-Seq Dat.pdf},
  langid = {english}
}

@article{rissoZINBWaVEGeneralFlexible2017,
  title = {{{ZINB}}-{{WaVE}}: {{A}} General and Flexible Method for Signal Extraction from Single-Cell {{RNA}}-Seq Data},
  shorttitle = {{{ZINB}}-{{WaVE}}},
  author = {Risso, Davide and Perraudeau, Fanny and Gribkova, Svetlana and Dudoit, Sandrine and Vert, Jean-Philippe},
  date = {2017-04-06},
  journaltitle = {bioRxiv},
  pages = {125112},
  doi = {10.1101/125112},
  url = {https://www.biorxiv.org/content/10.1101/125112v1},
  urldate = {2019-04-09},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Single-cell RNA sequencing (scRNA-seq) is a powerful technique that enables researchers to measure gene expression at the resolution of single cells. Because of the low amount of RNA present in a single cell, many genes fail to be detected even though they are expressed; these genes are usually referred to as dropouts. Here, we present a general and flexible zero-inflated negative binomial model (ZINB-WaVE), which leads to low-dimensional representations of the data that account for zero inflation (dropouts), over-dispersion, and the count nature of the data. We demonstrate, with simulations and real data, that the model and its associated estimation procedure are able to give a more stable and accurate low-dimensional representation of the data than principal component analysis (PCA) and zero-inflated factor analysis (ZIFA), without the need for a preliminary normalization step.{$<$}/p{$>$}},
  file = {/mnt/data/Google Drive/Zotero/storage/5GXVPPWT/Risso et al. - 2017 - ZINB-WaVE A general and flexible method for signa.pdf;/mnt/data/Google Drive/Zotero/storage/4Z26YX54/125112v1.html},
  langid = {english}
}

@online{RKICoronavirusSARSCoV2,
  title = {{{RKI}} - {{Coronavirus SARS}}-{{CoV}}-2 - {{Archiv}} Der {{Situationsberichte}} Des {{Robert Koch}}-{{Instituts}} Zu {{COVID}}-19 (Ab 4.3.2020)},
  url = {https://www.rki.de/DE/Content/InfAZ/N/Neuartiges_Coronavirus/Situationsberichte/Archiv.html},
  urldate = {2021-05-30},
  file = {/mnt/data/Google Drive/Zotero/storage/9YXYCWXU/Archiv.html}
}

@online{RKICoronavirusSARSCoV2a,
  title = {{{RKI}} - {{Coronavirus SARS}}-{{CoV}}-2 - {{Aktueller Lage}}-/{{Situationsbericht}} Des {{RKI}} Zu {{COVID}}-19},
  url = {https://www.rki.de/DE/Content/InfAZ/N/Neuartiges_Coronavirus/Situationsberichte/Gesamt.html},
  urldate = {2021-05-30}
}

@online{RKICoronavirusSARSCoV2b,
  title = {{{RKI}} - {{Coronavirus SARS}}-{{CoV}}-2 - {{November}} 2020: {{Archiv}} Der {{Situationsberichte}} Des {{Robert Koch}}-{{Instituts}} Zu {{COVID}}-19},
  url = {https://www.rki.de/DE/Content/InfAZ/N/Neuartiges_Coronavirus/Situationsberichte/Nov_2020/Archiv_November.html},
  urldate = {2021-05-30}
}

@article{robertsGaussianProcessesTimeseries2013,
  title = {Gaussian Processes for Time-Series Modelling},
  author = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson, N. and Aigrain, S.},
  date = {2013-02-13},
  journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  shortjournal = {Proc. R. Soc. A},
  volume = {371},
  pages = {20110550},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2011.0550},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0550},
  urldate = {2020-07-10},
  abstract = {In this paper we offer a gentle introduction to Gaussian processes for timeseries data analysis. The conceptual framework of Bayesian modelling for timeseries data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches.},
  file = {/mnt/data/Google Drive/Zotero/storage/7G2A6VD8/Roberts et al. - 2013 - Gaussian processes for time-series modelling.pdf},
  langid = {english},
  number = {1984}
}

@article{robertsGaussianProcessesTimeseries2013a,
  title = {Gaussian Processes for Time-Series Modelling},
  author = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson, N. and Aigrain, S.},
  date = {2013-02-13},
  journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  shortjournal = {Proc. R. Soc. A},
  volume = {371},
  pages = {20110550},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2011.0550},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0550},
  urldate = {2020-07-12},
  abstract = {In this paper we offer a gentle introduction to Gaussian processes for timeseries data analysis. The conceptual framework of Bayesian modelling for timeseries data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches.},
  file = {/mnt/data/Google Drive/Zotero/storage/KDQXM7AN/Roberts et al. - 2013 - Gaussian processes for time-series modelling.pdf},
  langid = {english},
  number = {1984}
}

@article{robinsonEdgeRBioconductorPackage2010,
  title = {{{edgeR}}: A {{Bioconductor}} Package for Differential Expression Analysis of Digital Gene Expression Data},
  shorttitle = {{{edgeR}}},
  author = {Robinson, Mark D. and McCarthy, Davis J. and Smyth, Gordon K.},
  date = {2010-01-01},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {26},
  pages = {139--140},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btp616},
  url = {https://academic.oup.com/bioinformatics/article/26/1/139/182458},
  urldate = {2018-12-01},
  abstract = {Abstract.  Summary: It is expected that emerging digital gene expression (DGE) technologies will overtake microarray technologies in the near future for many fu},
  file = {/mnt/data/Google Drive/Zotero/storage/CKS7MEGJ/Robinson et al. - 2010 - edgeR a Bioconductor package for differential exp.pdf;/mnt/data/Google Drive/Zotero/storage/8CQEB75X/182458.html},
  langid = {english},
  number = {1}
}

@article{robinsonScalingNormalizationMethod2010,
  title = {A Scaling Normalization Method for Differential Expression Analysis of {{RNA}}-Seq Data},
  author = {Robinson, Mark D. and Oshlack, Alicia},
  date = {2010-03-02},
  journaltitle = {Genome Biology},
  shortjournal = {Genome Biology},
  volume = {11},
  pages = {R25},
  issn = {1474-760X},
  doi = {10.1186/gb-2010-11-3-r25},
  url = {https://doi.org/10.1186/gb-2010-11-3-r25},
  urldate = {2019-03-20},
  abstract = {The fine detail provided by sequencing-based transcriptome surveys suggests that RNA-seq is likely to become the platform of choice for interrogating steady state RNA. In order to discover biologically important changes in expression, we show that normalization continues to be an essential step in the analysis. We outline a simple and effective method for performing normalization and show dramatically improved results for inferring differential expression in simulated and publicly available data sets.},
  file = {/mnt/data/Google Drive/Zotero/storage/XB33QA35/Robinson and Oshlack - 2010 - A scaling normalization method for differential ex.pdf;/mnt/data/Google Drive/Zotero/storage/GSACFJHZ/gb-2010-11-3-r25.html},
  number = {3}
}

@article{robinsonSmallsampleEstimationNegative2008,
  title = {Small-Sample Estimation of Negative Binomial Dispersion, with Applications to {{SAGE}} Data},
  author = {Robinson, Mark D. and Smyth, Gordon K.},
  date = {2008-04},
  journaltitle = {Biostatistics (Oxford, England)},
  shortjournal = {Biostatistics},
  volume = {9},
  pages = {321--332},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxm030},
  abstract = {We derive a quantile-adjusted conditional maximum likelihood estimator for the dispersion parameter of the negative binomial distribution and compare its performance, in terms of bias, to various other methods. Our estimation scheme outperforms all other methods in very small samples, typical of those from serial analysis of gene expression studies, the motivating data for this study. The impact of dispersion estimation on hypothesis testing is studied. We derive an "exact" test that outperforms the standard approximate asymptotic tests.},
  eprint = {17728317},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/EFNXKJMY/Robinson und Smyth - 2008 - Small-sample estimation of negative binomial dispe.pdf},
  keywords = {Bias,Binomial Distribution,Biometry,Expressed Sequence Tags,Gene Expression Profiling,Gene Library,Humans,Information Storage and Retrieval,Likelihood Functions,Regression Analysis,Research Design,RNA; Messenger,Sample Size,Stochastic Processes,Weights and Measures},
  langid = {english},
  number = {2}
}

@article{roblesEfficientExperimentalDesign2012,
  title = {Efficient Experimental Design and Analysis Strategies for the Detection of Differential Expression Using {{RNA}}-{{Sequencing}}},
  author = {Robles, José A. and Qureshi, Sumaira E. and Stephen, Stuart J. and Wilson, Susan R. and Burden, Conrad J. and Taylor, Jennifer M.},
  date = {2012-09-17},
  journaltitle = {BMC genomics},
  shortjournal = {BMC Genomics},
  volume = {13},
  pages = {484},
  issn = {1471-2164},
  doi = {10.1186/1471-2164-13-484},
  abstract = {BACKGROUND: RNA sequencing (RNA-Seq) has emerged as a powerful approach for the detection of differential gene expression with both high-throughput and high resolution capabilities possible depending upon the experimental design chosen. Multiplex experimental designs are now readily available, these can be utilised to increase the numbers of samples or replicates profiled at the cost of decreased sequencing depth generated per sample. These strategies impact on the power of the approach to accurately identify differential expression. This study presents a detailed analysis of the power to detect differential expression in a range of scenarios including simulated null and differential expression distributions with varying numbers of biological or technical replicates, sequencing depths and analysis methods. RESULTS: Differential and non-differential expression datasets were simulated using a combination of negative binomial and exponential distributions derived from real RNA-Seq data. These datasets were used to evaluate the performance of three commonly used differential expression analysis algorithms and to quantify the changes in power with respect to true and false positive rates when simulating variations in sequencing depth, biological replication and multiplex experimental design choices. CONCLUSIONS: This work quantitatively explores comparisons between contemporary analysis tools and experimental design choices for the detection of differential expression using RNA-Seq. We found that the DESeq algorithm performs more conservatively than edgeR and NBPSeq. With regard to testing of various experimental designs, this work strongly suggests that greater power is gained through the use of biological replicates relative to library (technical) replicates and sequencing depth. Strikingly, sequencing depth could be reduced as low as 15\% without substantial impacts on false positive or true positive rates.},
  eprint = {22985019},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/T9JRMUA9/Robles et al. - 2012 - Efficient experimental design and analysis strateg.pdf},
  keywords = {Algorithms,Gene Expression Profiling,Sequence Analysis; RNA,Statistics as Topic},
  langid = {english},
  pmcid = {PMC3560154}
}

@software{roGulfaMscEbola2019,
  title = {Gulfa/Msc\_ebola},
  author = {Ro, Gunnar},
  date = {2019-10-15T12:12:05Z},
  origdate = {2019-04-14T11:20:17Z},
  url = {https://github.com/Gulfa/msc_ebola},
  urldate = {2020-01-13},
  abstract = {Ebola modelling MSC project. Contribute to Gulfa/msc\_ebola development by creating an account on GitHub.}
}

@report{rosensteelCharacterizingEpidemiologicalGeography2021,
  title = {Characterizing an Epidemiological Geography of the {{United States}}: Influenza as a Case Study},
  shorttitle = {Characterizing an Epidemiological Geography of the {{United States}}},
  author = {Rosensteel, Grant E. and Lee, Elizabeth C. and Colizza, Vittoria and Bansal, Shweta},
  date = {2021-03-01},
  institution = {{Epidemiology}},
  doi = {10.1101/2021.02.24.21252361},
  url = {http://medrxiv.org/lookup/doi/10.1101/2021.02.24.21252361},
  urldate = {2021-04-01},
  abstract = {The prediction, prevention, and management of infectious diseases in the United States is either geographically homogeneous or is coordinated through ad-hoc administrative regions, ignoring the intense spatio-temporal heterogeneity displayed by most outbreaks. Using influenza as a case study, we characterize a regionalization of the United States. Based on influenza time series constructed from fine-scale insurance claims data from 2002-2009, we apply a complex network approach to characterize regions of the U.S. which experience comparable influenza dynamics. Our results identify three to five epidemiologically distinct regions for each flu season, with all locations within each region experiencing synchronous epidemics, and with an average of a two week delay in peak timing between regions. We find that there is significant heterogeneity across seasons in the identity of the regions and the relative timing across regions, making predictability from one season to the next challenging. Within a given season, however, our approach shows the potential to inform on the shaping of regions over time, to improve resources mobilization and targeted communication. Our epidemiologically-driven regionalization approach could allow for disease monitoring and control based on epidemiological risk rather than geopolitical boundaries, and provides a tractable public health approach to account for vast heterogeneity that exists in respiratory disease dynamics.},
  file = {/mnt/data/Google Drive/Zotero/storage/6PNIT86W/Rosensteel et al. - 2021 - Characterizing an epidemiological geography of the.pdf},
  langid = {english},
  type = {preprint}
}

@incollection{roweExpertOpinionsForecasting2001,
  title = {Expert {{Opinions}} in {{Forecasting}}: {{The Role}} of the {{Delphi Technique}}},
  shorttitle = {Expert {{Opinions}} in {{Forecasting}}},
  booktitle = {Principles of {{Forecasting}}: {{A Handbook}} for {{Researchers}} and {{Practitioners}}},
  author = {Rowe, Gene and Wright, George},
  editor = {Armstrong, J. Scott},
  date = {2001},
  pages = {125--144},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-0-306-47630-3_7},
  url = {https://doi.org/10.1007/978-0-306-47630-3_7},
  urldate = {2020-09-29},
  abstract = {Expert opinion is often necessary in forecasting tasks because of a lack of appropriate or available information for using statistical procedures. But how does one get the best forecast from experts? One solution is to use a structured group technique, such as Delphi, for eliciting and combining expert judgments. In using the Delphi technique, one controls the exchange of information between anonymous panelists over a number of rounds (iterations), taking the average of the estimates on the final round as the group judgment. A number of principles are developed here to indicate how to conduct structured groups to obtain good expert judgments. These principles, applied to the conduct of Delphi groups, indicate how many and what type of experts to use (five to 20 experts with disparate domain knowledge); how many rounds to use (generally two or three); what type of feedback to employ (average estimates plus justifications from each expert); how to summarize the final forecast (weight all experts’ estimates equally); how to word questions (in a balanced way with succinct definitions free of emotive terms and irrelevant information); and what response modes to use (frequencies rather than probabilities or odds, with coherence checks when feasible). Delphi groups are substantially more accurate than individual experts and traditional groups and somewhat more accurate than statistical groups (which are made up of noninteracting individuals whose judgments are aggregated). Studies support the advantage of Delphi groups over traditional groups by five to one with one tie, and their advantage over statistical groups by 12 to two with two ties. We anticipate that by following these principles, forecasters may be able to use structured groups to harness effectively expert opinion.},
  isbn = {978-0-306-47630-3},
  keywords = {Delphi,expertise,interacting groups,statistical groups},
  langid = {english},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}}
}

@online{RozbieznosciStatystykachKoronawirusa0100,
  title = {Rozbieżności w statystykach koronawirusa. 22 tys. przypadków będą doliczone do ogólnej liczby wyników},
  year = {16:07:56+0100},
  url = {https://forsal.pl/lifestyle/zdrowie/artykuly/8017628,rozbieznosci-w-statystykach-koronawirusa-22-tys-przypadkow-beda-doliczone-do-ogolnej-liczby-wynikow.html},
  urldate = {2021-05-30},
  abstract = {Od wtorku narzędzia rejestracji wyników testów na koronawirusa zostaną ujednolicone. Brakujące ok. 22 tys. przypadków, które nie zostały wcześniej podane, trafią we wtorek do ogólnej liczby zakażeń – podał GIS.},
  file = {/mnt/data/Google Drive/Zotero/storage/ZXP27ZRQ/8017628,rozbieznosci-w-statystykach-koronawirusa-22-tys-przypadkow-beda-doliczone-do-ogolnej-li.html},
  langid = {polish}
}

@article{rubinBayesianBootstrap1981,
  title = {The {{Bayesian Bootstrap}}},
  author = {Rubin, Donald B.},
  date = {1981-01},
  journaltitle = {Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {9},
  pages = {130--134},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176345338},
  url = {https://projecteuclid.org/euclid.aos/1176345338},
  urldate = {2020-06-02},
  abstract = {The Bayesian bootstrap is the Bayesian analogue of the bootstrap. Instead of simulating the sampling distribution of a statistic estimating a parameter, the Bayesian bootstrap simulates the posterior distribution of the parameter; operationally and inferentially the methods are quite similar. Because both methods of drawing inferences are based on somewhat peculiar model assumptions and the resulting inferences are generally sensitive to these assumptions, neither method should be applied without some consideration of the reasonableness of these model assumptions. In this sense, neither method is a true bootstrap procedure yielding inferences unaided by external assumptions.},
  file = {/mnt/data/Google Drive/Zotero/storage/EIYG5S5U/Rubin - 1981 - The Bayesian Bootstrap.pdf;/mnt/data/Google Drive/Zotero/storage/3HHY4A4F/1176345338.html},
  keywords = {Dirichlet,jackknife,Model-free inference},
  langid = {english},
  mrnumber = {MR600538},
  number = {1}
}

@article{ruedaDifferentialExpressionAnalysis,
  title = {Differential {{Expression Analysis}} Using {{edgeR}}},
  author = {Rueda, Oscar and Pereira, Bernard},
  pages = {4},
  file = {/mnt/data/Google Drive/Zotero/storage/ZXP86Y7T/Rueda and Pereira - Differential Expression Analysis using edgeR.pdf},
  langid = {english}
}

@online{ryantibshiraniQuantileStacking2020,
  title = {Quantile {{Stacking}}},
  author = {{Ryan Tibshirani}},
  date = {2020},
  url = {https://ryantibs.github.io/quantgen/stacking_example.html},
  urldate = {2020-08-23},
  file = {/mnt/data/Google Drive/Zotero/storage/LYC38F6E/stacking_example.html}
}

@article{sadhanalaAdditiveModelsTrend2019,
  title = {Additive Models with Trend Filtering},
  author = {Sadhanala, Veeranjaneyulu and Tibshirani, Ryan J.},
  date = {2019-12},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {47},
  pages = {3032--3068},
  issn = {0090-5364},
  doi = {10.1214/19-AOS1833},
  url = {https://projecteuclid.org/euclid.aos/1572487382},
  urldate = {2020-03-29},
  abstract = {We study additive models built with trend filtering, i.e., additive models whose components are each regularized by the (discrete) total variation of their kth (discrete) derivative, for a chosen integer k ≥ 0. This results in kth degree piecewise polynomial components, (e.g., k = 0 gives piecewise constant components, k = 1 gives piecewise linear, k = 2 gives piecewise quadratic, etc.). Analogous to its advantages in the univariate case, additive trend filtering has favorable theoretical and computational properties, thanks in large part to the localized nature of the (discrete) total variation regularizer that it uses. On the theory side, we derive fast error rates for additive trend filtering estimates, and show these rates are minimax optimal when the underlying function is additive and has component functions whose derivatives are of bounded variation. We also show that these rates are unattainable by additive smoothing splines (and by additive models built from linear smoothers, in general). On the computational side, we use backfitting, to leverage fast univariate trend filtering solvers; we also describe a new backfitting algorithm whose iterations can be run in parallel, which (as far as we can tell) is the first of its kind. Lastly, we present a number of experiments to examine the empirical performance of trend filtering.},
  file = {/mnt/data/Google Drive/Zotero/storage/2Y3KRKHQ/Sadhanala and Tibshirani - 2019 - Additive models with trend filtering.pdf},
  langid = {english},
  number = {6}
}

@article{saidTestingUnitRoots1984,
  title = {Testing for Unit Roots in Autoregressive-Moving Average Models of Unknown Order},
  author = {Said, Said E. and Dickey, David A.},
  date = {1984-12-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {71},
  pages = {599--607},
  issn = {0006-3444},
  doi = {10.1093/biomet/71.3.599},
  url = {https://academic.oup.com/biomet/article/71/3/599/258758},
  urldate = {2019-09-12},
  abstract = {AbstractSUMMARY.  Recently, methods for detecting unit roots in autoregressive and autoregressive-moving average time series have been proposed. The presence of},
  file = {/mnt/data/Google Drive/Zotero/storage/BC4ZW997/Said and Dickey - 1984 - Testing for unit roots in autoregressive-moving av.pdf;/mnt/data/Google Drive/Zotero/storage/IN2NT2WC/258758.html},
  langid = {english},
  number = {3}
}

@software{samabbottCRANPackageIdmodelr,
  title = {{{CRAN}} - {{Package}} Idmodelr},
  author = {{Sam Abbott}},
  url = {https://cran.r-project.org/web/packages/idmodelr/index.html},
  urldate = {2020-04-12},
  file = {/mnt/data/Google Drive/Zotero/storage/PCKAL6ZT/index.html}
}

@software{samabbottEpiforecastsEpiSoonStable2020,
  title = {Epiforecasts/{{EpiSoon}}: {{Stable}} Forecasting Release},
  shorttitle = {Epiforecasts/{{EpiSoon}}},
  author = {Sam Abbott and Nikos Bosse and Michael DeWitt and Andrea Rau and Aurelien Chateigner and Sylvain Mareschal and Joel Hellewell},
  date = {2020-05-19},
  doi = {10.5281/zenodo.3833807},
  url = {https://zenodo.org/record/3833807},
  urldate = {2020-08-12},
  abstract = {Forecasting the effective reproduction number over short timescales},
  file = {/mnt/data/Google Drive/Zotero/storage/WNU6Z9R3/3833807.html},
  organization = {{Zenodo}}
}

@article{schaeybroeckEnsemblePostprocessingUsing2015,
  title = {Ensemble Post-Processing Using Member-by-Member Approaches: Theoretical Aspects},
  shorttitle = {Ensemble Post-Processing Using Member-by-Member Approaches},
  author = {Schaeybroeck, Bert Van and Vannitsem, Stéphane},
  date = {2015},
  journaltitle = {Quarterly Journal of the Royal Meteorological Society},
  volume = {141},
  pages = {807--818},
  issn = {1477-870X},
  doi = {10.1002/qj.2397},
  url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.2397},
  urldate = {2020-04-06},
  abstract = {Linear post-processing approaches are proposed and fundamental mechanisms are analyzed by which the probabilistic skill of an ensemble forecast can be improved. The ensemble mean of the corrected forecast is a linear function of the ensemble mean(s) of the predictor(s). Likewise, the ensemble spread of the corrected forecast depends linearly on that of the uncorrected forecast. The regression coefficients are obtained by maximizing the likelihood function for the error distribution. Comparing different calibration approaches on simple systems that exhibit chaotic features (the Kuramoto–Sivashinsky equation, the spatially extended Lorenz system), four correction mechanisms are identified: the ensemble-mean scaling and nudging using the predictor(s), and the ensemble-spread scaling and nudging. Ensemble-spread corrections turn out to yield improvement only when ‘reliability’ constraints are imposed on the corrected forecast. First of all climatological reliability is enforced and is satisfied when the total variability of the forecast is equal to the variability of the observations. Second, ensemble reliability or calibration of the ensembles is enforced such that the squared error of the ensemble mean coincides with the ensemble variance. In terms of continuous ranked probability skill score, spread calibration provides much more gain in skill than the traditional ensemble-mean calibration and extends for lead times far beyond the error-doubling time. The skill performance is better than or as good as the benchmark calibration method which derives from statistical assumptions –non-homogeneous Gaussian regression. In addition to the member-by-member nature of the approach, benefits compared with the benchmark method can be pinpointed. In particular, although the post-processing methods are performed for each lead time, location and variable independently, they preserve the rank correlations and thus take dependencies across space, time, and different variables into account. In addition, higher-order ensemble moments like kurtosis and skewness correspond to those of the uncorrected forecasts.},
  file = {/mnt/data/Google Drive/Zotero/storage/6YNK3RMJ/Schaeybroeck and Vannitsem - 2015 - Ensemble post-processing using member-by-member ap.pdf;/mnt/data/Google Drive/Zotero/storage/C5IM6W5S/qj.html},
  keywords = {member-by-member approach,model output statistics,statistical post-processing},
  langid = {english},
  number = {688}
}

@article{schobelStatisticalInferenceforPropagation,
  title = {Statistical {{Inferencefor Propagation Processeson Complex Networks}}},
  author = {Schöbel, Dr Anita and Göttingen, Georg-August-Universität and Kneib, Dr Thomas},
  pages = {200},
  file = {/mnt/data/Google Drive/Zotero/storage/L2YVK83S/Schöbel et al. - Mitglieder der Prüfungskommision.pdf},
  langid = {english}
}

@article{serbanCombiningMeanReversion2010,
  title = {Combining Mean Reversion and Momentum Trading Strategies in Foreign Exchange Markets},
  author = {Serban, Alina F.},
  date = {2010-11-01},
  journaltitle = {Journal of Banking \& Finance},
  shortjournal = {Journal of Banking \& Finance},
  volume = {34},
  pages = {2720--2727},
  issn = {0378-4266},
  doi = {10.1016/j.jbankfin.2010.05.011},
  url = {http://www.sciencedirect.com/science/article/pii/S0378426610001883},
  urldate = {2019-08-26},
  abstract = {The literature on equity markets documents the existence of mean reversion and momentum phenomena. Researchers in foreign exchange markets find that foreign exchange rates also display behaviors akin to momentum and mean reversion. This paper implements a trading strategy combining mean reversion and momentum in foreign exchange markets. The strategy was originally designed for equity markets, but it also generates abnormal returns when applied to uncovered interest parity deviations for five countries. I find that the pattern for the positions thus created in the foreign exchange markets is qualitatively similar to that found in the equity markets. Quantitatively, this strategy performs better in foreign exchange markets than in equity markets. Also, it outperforms traditional foreign exchange trading strategies, such as carry trades and moving average rules.},
  file = {/mnt/data/Google Drive/Zotero/storage/52H6S9H7/S0378426610001883.html},
  keywords = {Foreign exchange,Mean reversion,Momentum,Trading strategies,Uncovered interest parity},
  number = {11}
}

@article{serbanCombiningMeanReversion2010a,
  title = {Combining Mean Reversion and Momentum Trading Strategies in Foreign Exchange Markets},
  author = {Serban, Alina F.},
  date = {2010-11-01},
  journaltitle = {Journal of Banking \& Finance},
  shortjournal = {Journal of Banking \& Finance},
  volume = {34},
  pages = {2720--2727},
  issn = {0378-4266},
  doi = {10.1016/j.jbankfin.2010.05.011},
  url = {http://www.sciencedirect.com/science/article/pii/S0378426610001883},
  urldate = {2019-08-26},
  abstract = {The literature on equity markets documents the existence of mean reversion and momentum phenomena. Researchers in foreign exchange markets find that foreign exchange rates also display behaviors akin to momentum and mean reversion. This paper implements a trading strategy combining mean reversion and momentum in foreign exchange markets. The strategy was originally designed for equity markets, but it also generates abnormal returns when applied to uncovered interest parity deviations for five countries. I find that the pattern for the positions thus created in the foreign exchange markets is qualitatively similar to that found in the equity markets. Quantitatively, this strategy performs better in foreign exchange markets than in equity markets. Also, it outperforms traditional foreign exchange trading strategies, such as carry trades and moving average rules.},
  file = {/mnt/data/Google Drive/Zotero/storage/F3DV92YS/Serban - 2010 - Combining mean reversion and momentum trading stra.pdf;/mnt/data/Google Drive/Zotero/storage/QVJ5UTZH/S0378426610001883.html},
  keywords = {Foreign exchange,Mean reversion,Momentum,Trading strategies,Uncovered interest parity},
  number = {11}
}

@article{shannonMathematicalTheoryCommunication1948,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, C E},
  date = {1948},
  pages = {55},
  file = {/mnt/data/Google Drive/Zotero/storage/TMT7GQ6U/Shannon - A Mathematical Theory of Communication.pdf},
  langid = {english}
}

@article{sherrattEvaluatingUseReproduction2020,
  title = {Evaluating the Use of the Reproduction Number as an Epidemiological Tool, Using Spatio-Temporal Trends of the {{Covid}}-19 Outbreak in {{England}}},
  author = {Sherratt, Katharine and Abbott, Sam and Meakin, Sophie R. and Hellewell, Joel and Munday, James D. and Bosse, Nikos and working Group, CMMID Covid-19 and Jit, Mark and Funk, Sebastian},
  date = {2020-10-20},
  journaltitle = {medRxiv},
  pages = {2020.10.18.20214585},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2020.10.18.20214585},
  url = {https://www.medrxiv.org/content/10.1101/2020.10.18.20214585v1},
  urldate = {2021-05-30},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}The time-varying reproduction number (R\textsubscript{t}: the average number secondary infections caused by each infected person) may be used to assess changes in transmission potential during an epidemic. Since new infections usually are not observed directly, it can only be estimated from delayed and potentially biased data. We estimated R\textsubscript{t} using a model that mapped unobserved infections to observed test-positive cases, hospital admissions, and deaths with confirmed Covid-19, in seven regions of England over March through August 2020. We explored the sensitivity of R\textsubscript{t} estimates of Covid-19 in England to different data sources, and investigated the potential of using differences in the estimates to track epidemic dynamics in population sub-groups.{$<$}/p{$><$}p{$>$}Our estimates of transmission potential varied for each data source. The divergence between estimates from each source was not consistent within or across regions over time, although estimates based on hospital admissions and deaths were more spatio-temporally synchronous than compared to estimates from all test-positives. We compared differences in R\textsubscript{t} with the demographic and social context of transmission, and found the differences between R\textsubscript{t} may be linked to biased representations of sub-populations in each data source: from uneven testing rates, or increasing severity of disease with age, seen via outbreaks in care home populations and changing age distributions of cases.{$<$}/p{$><$}p{$>$}We highlight that policy makers should consider the source populations of R\textsubscript{t} estimates. Further work should clarify the best way to combine and interpret R\textsubscript{t} estimates from different data sources based on the desired use.{$<$}/p{$>$}},
  file = {/mnt/data/Google Drive/Zotero/storage/9Z8LVWSS/Sherratt et al. - 2020 - Evaluating the use of the reproduction number as a.pdf},
  langid = {english}
}

@article{shiComprehensiveTimeSeriesRegression2014,
  title = {Comprehensive {{Time}}-{{Series Regression Models Using Gretl U}}.{{S}}. {{GDP}} and {{Government Consumption Expenditures}} \& {{Gross Investment}} from 1980 to 2013},
  author = {Shi, Juehui},
  date = {2014},
  journaltitle = {SSRN Electronic Journal},
  shortjournal = {SSRN Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.2540535},
  url = {http://www.ssrn.com/abstract=2540535},
  urldate = {2019-07-10},
  abstract = {Using Gretl, I apply ARMA, Vector ARMA, VAR, state-space model with a Kalman filter, transfer-function and intervention models, unit root tests, cointegration test, volatility models (ARCH, GARCH, ARCH-M, GARCH-M, Taylor-Schwert GARCH, GJR, TARCH, NARCH, APARCH, EGARCH) to analyze quarterly time series of GDP and Government Consumption Expenditures \& Gross Investment (GCEGI) from 1980 to 2013. The article is organized in three sections: (I) Definition; (II) Regression Models; (III) Discussion [Summary of Major Findings and Their Managerial Implications, Comparison of Empirical Results, Contributions to Literature, Limitations and Future Research, Gretl Scripts]. Additionally, I discovered a unique interaction between GDP and GCEGI in both the short-run and the long-run and provided policy makers with some suggestions. For example in the short run, GDP responded positively and very significantly (0.00248) to GCEGI, while GCEGI reacted positively but not too significantly (0.08051) to GDP. In the long run, current GDP responded negatively and permanently (0.09229) to a shock in past GCEGI, while current GCEGI reacted negatively yet temporarily (0.29821) to a shock in past GDP. Therefore, policy makers should not adjust current GCEGI based merely on the condition of current and past GDP. Although increasing GCEGI does help GDP in the short-term, significantly abrupt increase in GCEGI might not be good to the long-term health of GDP. Instead, a balanced, sustainable, and economically viable solution is recommended, so that the short-term benefits to the current economy from increasing GCEGI often largely secured by the long-term loan outweigh or at least equal to the negative effect to the future economy from the long-term debt incurred by the loan. Finally, I found that non-normally distributed volatility models generally perform better than normally distributed ones. More specifically, TARCH-GED performs the best in the group of non-normally distributed, while GARCH-M does the best in the group of normally distributed.},
  file = {/mnt/data/Google Drive/Zotero/storage/U2FRLU2I/Shi - 2014 - Comprehensive Time-Series Regression Models Using .pdf},
  langid = {english}
}

@article{shmueliExplainPredict2010,
  title = {To {{Explain}} or to {{Predict}}?},
  author = {Shmueli, Galit},
  date = {2010-08},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {25},
  pages = {289--310},
  issn = {0883-4237},
  doi = {10.1214/10-STS330},
  url = {http://projecteuclid.org/euclid.ss/1294167961},
  urldate = {2019-09-16},
  abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
  file = {/mnt/data/Google Drive/Zotero/storage/5G26ZWRE/Shmueli - 2010 - To Explain or to Predict.pdf},
  langid = {english},
  number = {3}
}

@book{shumwayTimeSeriesAnalysis2011,
  title = {Time Series Analysis and Its Applications: With {{R}} Examples},
  shorttitle = {Time Series Analysis and Its Applications},
  author = {Shumway, Robert H. and Stoffer, David S.},
  date = {2011},
  edition = {3rd ed},
  publisher = {{Springer}},
  location = {{New York}},
  file = {/mnt/data/Google Drive/Zotero/storage/9XNWKECM/Shumway and Stoffer - 2011 - Time series analysis and its applications with R .pdf;/mnt/data/Google Drive/Zotero/storage/LE5GIQU9/tsa4.pdf},
  isbn = {978-1-4419-7864-6},
  keywords = {Time-series analysis},
  langid = {english},
  pagetotal = {596},
  series = {Springer Texts in Statistics}
}

@book{shumwayTimeSeriesAnalysis2011a,
  title = {Time Series Analysis and Its Applications: With {{R}} Examples},
  shorttitle = {Time Series Analysis and Its Applications},
  author = {Shumway, Robert H. and Stoffer, David S.},
  date = {2011},
  edition = {3rd ed},
  publisher = {{Springer}},
  location = {{New York}},
  isbn = {978-1-4419-7864-6},
  keywords = {Time-series analysis},
  langid = {english},
  pagetotal = {596},
  series = {Springer Texts in Statistics}
}

@article{smithDiagnosticChecksNonstandard1985,
  title = {Diagnostic Checks of Non-Standard Time Series Models},
  author = {Smith, J. Q.},
  date = {1985-01-01},
  journaltitle = {Journal of Forecasting},
  shortjournal = {Journal of Forecasting},
  volume = {4},
  pages = {283--291},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0277-6693},
  doi = {10.1002/for.3980040305},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980040305},
  urldate = {2020-08-13},
  abstract = {Abstract Diagnostic checks have become a standard tool for helping to assess the adequacy of a forecasting system since Box and Jenkins' (1970) ARIMA modelling technique became popular. However, most of the research has developed checks for normal or second-order stationary models. This paper gives various diagnostic checks that can be performed simply on nonnormal, non-standard models such as the class of multiprocess models (Harrison and Stevens, 1976), where residuals are definitely not normal. The performance to date of these models can then be objectively scrutinized on-line. Examples, including a generalized cusum technique, are given to illustrate the effectiveness of the techniques on specific series.},
  file = {/mnt/data/Google Drive/Zotero/storage/Y2MDS4DR/Smith - 1985 - Diagnostic checks of non-standard time series mode.pdf;/mnt/data/Google Drive/Zotero/storage/EJF98Y4J/for.html},
  keywords = {Cusum,Diagnostic checks,Non-normal models,Time-series analysis},
  number = {3}
}

@online{statistischesbundesamtBevoelkerungNachNationalitaet2020,
  title = {Bevölkerung nach Nationalität und Bundesländern},
  author = {{Statistisches Bundesamt}},
  date = {2020-06-19},
  url = {https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Bevoelkerung/Bevoelkerungsstand/Tabellen/bevoelkerung-nichtdeutsch-laender.html},
  urldate = {2021-06-10},
  abstract = {Diese Tabelle enthält: Bevölkerung am 31.12.2019 nach Nationalität und Bundesländern.},
  file = {/mnt/data/Google Drive/Zotero/storage/BVRMKNET/bevoelkerung-nichtdeutsch-laender.html},
  langid = {german},
  organization = {{Statistisches Bundesamt}}
}

@online{StatsModelsStatisticsPython,
  title = {{{StatsModels}}: {{Statistics}} in {{Python}} — Statsmodels v0.10.1 Documentation},
  url = {https://www.statsmodels.org/stable/index.html},
  urldate = {2019-09-13},
  file = {/mnt/data/Google Drive/Zotero/storage/5FQPJPM2/index.html}
}

@article{stephensFalseDiscoveryRates2017,
  title = {False Discovery Rates: A New Deal},
  shorttitle = {False Discovery Rates},
  author = {Stephens, Matthew},
  date = {2017-04-01},
  journaltitle = {Biostatistics},
  shortjournal = {Biostatistics},
  volume = {18},
  pages = {275--294},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxw041},
  url = {https://academic.oup.com/biostatistics/article/18/2/275/2557030},
  urldate = {2019-04-16},
  abstract = {Summary.  We introduce a new Empirical Bayes approach for large-scale hypothesis testing, including estimating false discovery rates (FDRs), and effect sizes. T},
  file = {/mnt/data/Google Drive/Zotero/storage/RJKBMUJZ/Stephens - 2017 - False discovery rates a new deal.pdf;/mnt/data/Google Drive/Zotero/storage/F7IGBZJL/2557030.html},
  langid = {english},
  number = {2}
}

@article{subramanianGeneSetEnrichment2005,
  title = {Gene Set Enrichment Analysis: {{A}} Knowledge-Based Approach for Interpreting Genome-Wide Expression Profiles},
  shorttitle = {Gene Set Enrichment Analysis},
  author = {Subramanian, Aravind and Tamayo, Pablo and Mootha, Vamsi K. and Mukherjee, Sayan and Ebert, Benjamin L. and Gillette, Michael A. and Paulovich, Amanda and Pomeroy, Scott L. and Golub, Todd R. and Lander, Eric S. and Mesirov, Jill P.},
  date = {2005-10-25},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {102},
  pages = {15545--15550},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0506580102},
  url = {http://www.pnas.org/content/102/43/15545},
  urldate = {2018-12-02},
  abstract = {Although genomewide RNA expression analysis has become a routine tool in biomedical research, extracting biological insight from such information remains a major challenge. Here, we describe a powerful analytical method called Gene Set Enrichment Analysis (GSEA) for interpreting gene expression data. The method derives its power by focusing on gene sets, that is, groups of genes that share common biological function, chromosomal location, or regulation. We demonstrate how GSEA yields insights into several cancer-related data sets, including leukemia and lung cancer. Notably, where single-gene analysis finds little similarity between two independent studies of patient survival in lung cancer, GSEA reveals many biological pathways in common. The GSEA method is embodied in a freely available software package, together with an initial database of 1,325 biologically defined gene sets.},
  eprint = {16199517},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/8AG9AWE4/Subramanian et al. - 2005 - Gene set enrichment analysis A knowledge-based ap.pdf;/mnt/data/Google Drive/Zotero/storage/PC93AVKK/Subramanian et al. - 2005 - Gene set enrichment analysis A knowledge-based ap.pdf;/mnt/data/Google Drive/Zotero/storage/TJ2AG9FT/15545.html;/mnt/data/Google Drive/Zotero/storage/U5SU4RBT/15545.html},
  keywords = {microarray},
  langid = {english},
  number = {43}
}

@article{susmelHourlyVolatilitySpillovers1994,
  title = {Hourly Volatility Spillovers between International Equity Markets},
  author = {Susmel, Raul and Engle, Robert F.},
  date = {1994-02-01},
  journaltitle = {Journal of International Money and Finance},
  shortjournal = {Journal of International Money and Finance},
  volume = {13},
  pages = {3--25},
  issn = {0261-5606},
  doi = {10.1016/0261-5606(94)90021-3},
  url = {http://www.sciencedirect.com/science/article/pii/0261560694900213},
  urldate = {2019-06-19},
  abstract = {This paper examines the timing of mean and volatility spillovers between New York and London equity markets. Using an ARCH model it is found that the evidence of volatility spillovers between these markets is minimal and have a duration which lasts only an hour or so. The most significant effects surround the movement of share prices around the New York opening, but these results are not strong. Several new ARCH models are estimated including an asymmetric or ‘leverage’ model and a non-linear model which allows big shocks to have a different impact from small shocks.},
  file = {/mnt/data/Google Drive/Zotero/storage/TTLMA9SS/0261560694900213.html},
  number = {1}
}

@article{svenssonNoteGenerationTimes2007,
  title = {A Note on Generation Times in Epidemic Models},
  author = {Svensson, Åke},
  date = {2007-07-01},
  journaltitle = {Mathematical Biosciences},
  shortjournal = {Mathematical Biosciences},
  volume = {208},
  pages = {300--311},
  issn = {0025-5564},
  doi = {10.1016/j.mbs.2006.10.010},
  url = {http://www.sciencedirect.com/science/article/pii/S0025556406002094},
  urldate = {2020-06-05},
  abstract = {The time between the infection of a primary case and one of its secondary cases is called a generation time. The distribution (and mean) of the generation times is derived for a rather general class of epidemic models. The relation to assumptions on distributions of latency times and infectious times or more generally on random time varying infectiousness, is investigated. Serial times, defined as the times between occurrence of observable events in the progress of an infectious disease (e.g., the onset of clinical symptoms), are also considered.},
  file = {/mnt/data/Google Drive/Zotero/storage/UDUF9FLS/S0025556406002094.html},
  keywords = {Epidemic models,Generation times,Serial times,Transmission intervals},
  langid = {english},
  number = {1}
}

@article{taiebHierarchicalProbabilisticForecasting2020,
  title = {Hierarchical {{Probabilistic Forecasting}} of {{Electricity Demand With Smart Meter Data}}},
  author = {Taieb, Souhaib Ben and Taylor, James W. and Hyndman, Rob J.},
  date = {2020-03-30},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  pages = {1--17},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2020.1736081},
  url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1736081},
  urldate = {2020-09-15},
  abstract = {Electricity smart meters record consumption, on a near real-time basis, at the level of individual commercial and residential properties. From this, a hierarchy can be constructed consisting of time series of demand at the smart meter level, and at various levels of aggregation, such as substations, cities and regions. Forecasts are needed at each level to support the efficient and reliable management of consumption. A limitation of previous research in this area is that it considered only deterministic prediction. To enable improved decision-making, we introduce an algorithm for producing a probability density forecast for each series within a large-scale hierarchy. The resulting forecasts are coherent in the sense that the forecast distribution of each aggregate series is equal to the convolution of the forecast distributions of the corresponding disaggregate series. Our algorithm has the advantage of synthesizing information from different levels in the hierarchy through forecast combination. Distributional assumptions are not required, and dependencies between forecast distributions are imposed through the use of empirical copulas. Scalability to large hierarchies is enabled by decomposing the problem into multiple lower-dimension sub-problems. Results for UK electricity smart meter data show performance gains for our method when compared to benchmarks.},
  file = {/mnt/data/Google Drive/Zotero/storage/7YR9ZJAQ/Taieb et al. - 2020 - Hierarchical Probabilistic Forecasting of Electric.pdf},
  langid = {english}
}

@online{tangSTLSTMDeepLearning2019,
  title = {{{ST}}-{{LSTM}}: {{A Deep Learning Approach Combined Spatio}}-{{Temporal Features}} for {{Short}}-{{Term Forecast}} in {{Rail Transit}}},
  shorttitle = {{{ST}}-{{LSTM}}},
  author = {Tang, Qicheng and Yang, Mengning and Yang, Ying},
  date = {2019},
  doi = {10.1155/2019/8392592},
  url = {https://www.hindawi.com/journals/jat/2019/8392592/},
  urldate = {2019-09-19},
  abstract = {The short-term forecast of rail transit is one of the most essential issues in urban intelligent transportation system (ITS). Accurate forecast result can provide support for the forewarning of flow outburst and enables passengers to make an appropriate travel plan. Therefore, it is significant to develop a more accurate forecast model. Long short-term memory (LSTM) network has been proved to be effective on data with temporal features. However, it cannot process the correlation between time and space in rail transit. As a result, a novel forecast model combining spatio-temporal features based on LSTM network (ST-LSTM) is proposed. Different from other forecast methods, ST-LSTM network uses a new method to extract spatio-temporal features from the data and combines them together as the input. Compared with other conventional models, ST-LSTM network can achieve a better performance in experiments.},
  file = {/mnt/data/Google Drive/Zotero/storage/B28NWTUQ/Tang et al. - 2019 - ST-LSTM A Deep Learning Approach Combined Spatio-.pdf;/mnt/data/Google Drive/Zotero/storage/PFQZ6RKG/8392592.html},
  langid = {english},
  organization = {{Journal of Advanced Transportation}},
  type = {Research article}
}

@article{taylorComparisonAggregationMethods,
  title = {A {{Comparison}} of {{Aggregation Methods}} for {{Probabilistic Forecasts}} of {{COVID}}-19 {{Mortality}} in the {{United States}}},
  author = {Taylor, Kathryn S and Taylor, James W},
  pages = {32},
  abstract = {The COVID-19 pandemic has placed forecasting models at the forefront of health policy making. Predictions of mortality and hospitalization help governments meet planning and resource allocation challenges. In this paper, we consider the weekly forecasting of the cumulative mortality due to COVID19 at the national and state level in the U.S. Optimal decision-making requires a forecast of a probability distribution, rather than just a single point forecast. Interval forecasts are also important, as they can support decision making and provide situational awareness. We consider the case where probabilistic forecasts have been provided by multiple forecasting teams, and we aggregate the forecasts to extract the wisdom of the crowd. With only limited information available regarding the historical accuracy of the forecasting teams, we consider aggregation methods that do not rely on a record of past accuracy. In this empirical paper, we evaluate the accuracy of aggregation methods that have been previously proposed for interval forecasts and predictions of probability distributions. These include the use of the simple average, the median, and trimming methods, which enable robust estimation and allow the aggregate forecast to reduce the impact of a tendency for the forecasting teams to be under- or overconfident. We use data that has been made publicly available from the COVID-19 Forecast Hub. Our finding is that, while the simple average provides a reasonable benchmark, greater forecast accuracy can be achieved using the median, and some forms of trimming.},
  file = {/mnt/data/Google Drive/Zotero/storage/7FWJYPEV/Taylor and Taylor - A Comparison of Aggregation Methods for Probabilis.pdf},
  langid = {english}
}

@article{taylorComparisonAggregationMethodsa,
  title = {A {{Comparison}} of {{Aggregation Methods}} for {{Probabilistic Forecasts}} of {{COVID}}-19 {{Mortality}} in the {{United States}}},
  author = {Taylor, Kathryn S and Taylor, James W},
  pages = {32},
  abstract = {The COVID-19 pandemic has placed forecasting models at the forefront of health policy making. Predictions of mortality and hospitalization help governments meet planning and resource allocation challenges. In this paper, we consider the weekly forecasting of the cumulative mortality due to COVID19 at the national and state level in the U.S. Optimal decision-making requires a forecast of a probability distribution, rather than just a single point forecast. Interval forecasts are also important, as they can support decision making and provide situational awareness. We consider the case where probabilistic forecasts have been provided by multiple forecasting teams, and we aggregate the forecasts to extract the wisdom of the crowd. With only limited information available regarding the historical accuracy of the forecasting teams, we consider aggregation methods that do not rely on a record of past accuracy. In this empirical paper, we evaluate the accuracy of aggregation methods that have been previously proposed for interval forecasts and predictions of probability distributions. These include the use of the simple average, the median, and trimming methods, which enable robust estimation and allow the aggregate forecast to reduce the impact of a tendency for the forecasting teams to be under- or overconfident. We use data that has been made publicly available from the COVID-19 Forecast Hub. Our finding is that, while the simple average provides a reasonable benchmark, greater forecast accuracy can be achieved using the median, and some forms of trimming.},
  file = {/mnt/data/Google Drive/Zotero/storage/4EKVPIND/Taylor and Taylor - A Comparison of Aggregation Methods for Probabilis.pdf},
  langid = {english}
}

@article{taylorComparisonAggregationMethodsb,
  title = {A {{Comparison}} of {{Aggregation Methods}} for {{Probabilistic Forecasts}} of {{COVID}}-19 {{Mortality}} in the {{United States}}},
  author = {Taylor, Kathryn S and Taylor, James W},
  pages = {32},
  abstract = {The COVID-19 pandemic has placed forecasting models at the forefront of health policy making. Predictions of mortality and hospitalization help governments meet planning and resource allocation challenges. In this paper, we consider the weekly forecasting of the cumulative mortality due to COVID19 at the national and state level in the U.S. Optimal decision-making requires a forecast of a probability distribution, rather than just a single point forecast. Interval forecasts are also important, as they can support decision making and provide situational awareness. We consider the case where probabilistic forecasts have been provided by multiple forecasting teams, and we aggregate the forecasts to extract the wisdom of the crowd. With only limited information available regarding the historical accuracy of the forecasting teams, we consider aggregation (i.e. combining) methods that do not rely on a record of past accuracy. In this empirical paper, we evaluate the accuracy of aggregation methods that have been previously proposed for interval forecasts and predictions of probability distributions. These include the use of the simple average, the median, and trimming methods, which enable robust estimation and allow the aggregate forecast to reduce the impact of a tendency for the forecasting teams to be under- or overconfident. We use data that has been made publicly available from the COVID-19 Forecast Hub. While the simple average performed well for the high mortality series, we obtained greater accuracy using the median and certain trimming methods for the low and medium mortality series. It will be interesting to see if this remains the case as the pandemic evolves.},
  file = {/mnt/data/Google Drive/Zotero/storage/8YGC7EYJ/Taylor and Taylor - A Comparison of Aggregation Methods for Probabilis.pdf},
  langid = {english}
}

@article{teamForecastingCOVID19Impact2020,
  title = {Forecasting {{COVID}}-19 Impact on Hospital Bed-Days, {{ICU}}-Days, Ventilator-Days and Deaths by {{US}} State in the next 4 Months},
  author = {health service utilization forecasting Team, IHME COVID-19 and Murray, Christopher JL},
  date = {2020-03-30},
  journaltitle = {medRxiv},
  pages = {2020.03.27.20043752},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2020.03.27.20043752},
  url = {https://www.medrxiv.org/content/10.1101/2020.03.27.20043752v1},
  urldate = {2021-05-29},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}h3{$>$}Importance{$<$}/h3{$>$} {$<$}p{$>$}This study presents the first set of estimates of predicted health service utilization and deaths due to COVID-19 by day for the next 4 months for each state in the US.{$<$}/p{$><$}h3{$>$}Objective{$<$}/h3{$>$} {$<$}p{$>$}To determine the extent and timing of deaths and excess demand for hospital services due to COVID-19 in the US.{$<$}/p{$><$}h3{$>$}Design, Setting, and Participants{$<$}/h3{$>$} {$<$}p{$>$}This study used data on confirmed COVID-19 deaths by day from WHO websites and local and national governments; data on hospital capacity and utilization for US states; and observed COVID-19 utilization data from select locations to develop a statistical model forecasting deaths and hospital utilization against capacity by state for the US over the next 4 months.{$<$}/p{$><$}h3{$>$}Exposure(s){$<$}/h3{$>$} {$<$}p{$>$}COVID-19.{$<$}/p{$><$}h3{$>$}Main outcome(s) and measure(s){$<$}/h3{$>$} {$<$}p{$>$}Deaths, bed and ICU occupancy, and ventilator use.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$>$} {$<$}p{$>$}Compared to licensed capacity and average annual occupancy rates, excess demand from COVID-19 at the peak of the pandemic in the second week of April is predicted to be 64,175 (95\% UI 7,977 to 251,059) total beds and 17,380 (95\% UI 2,432 to 57,955) ICU beds. At the peak of the pandemic, ventilator use is predicted to be 19,481 (95\% UI 9,767 to 39,674). The date of peak excess demand by state varies from the second week of April through May. We estimate that there will be a total of 81,114 (95\% UI 38,242 to 162,106) deaths from COVID-19 over the next 4 months in the US. Deaths from COVID-19 are estimated to drop below 10 deaths per day between May 31 and June 6.{$<$}/p{$><$}h3{$>$}Conclusions and Relevance{$<$}/h3{$>$} {$<$}p{$>$}In addition to a large number of deaths from COVID-19, the epidemic in the US will place a load well beyond the current capacity of hospitals to manage, especially for ICU care. These estimates can help inform the development and implementation of strategies to mitigate this gap, including reducing non-COVID-19 demand for services and temporarily increasing system capacity. These are urgently needed given that peak volumes are estimated to be only three weeks away. The estimated excess demand on hospital systems is predicated on the enactment of social distancing measures in all states that have not done so already within the next week and maintenance of these measures throughout the epidemic, emphasizing the importance of implementing, enforcing, and maintaining these measures to mitigate hospital system overload and prevent deaths.{$<$}/p{$><$}h3{$>$}Data availability statement{$<$}/h3{$>$} {$<$}p{$>$}A full list of data citations are available by contacting the corresponding author.{$<$}/p{$><$}h3{$>$}Funding Statement{$<$}/h3{$>$} {$<$}p{$>$}Bill \&amp; Melinda Gates Foundation and the State of Washington{$<$}/p{$><$}h3{$>$}Key Points{$<$}/h3{$>$} {$<$}h3{$>$}Question{$<$}/h3{$>$} {$<$}p{$>$}Assuming social distancing measures are maintained, what are the forecasted gaps in available health service resources and number of deaths from the COVID-19 pandemic for each state in the United States?{$<$}/p{$><$}h3{$>$}Findings{$<$}/h3{$>$} {$<$}p{$>$}Using a statistical model, we predict excess demand will be 64,175 (95\% UI 7,977 to 251,059) total beds and 17,380 (95\% UI 2,432 to 57,955) ICU beds at the peak of COVID-19. Peak ventilator use is predicted to be 19,481 (95\% UI 9,767 to 39,674) ventilators. Peak demand will be in the second week of April. We estimate 81,114 (95\% UI 38,242 to 162,106) deaths in the United States from COVID-19 over the next 4 months.{$<$}/p{$><$}h3{$>$}Meaning{$<$}/h3{$>$} {$<$}p{$>$}Even with social distancing measures enacted and sustained, the peak demand for hospital services due to the COVID-19 pandemic is likely going to exceed capacity substantially. Alongside the implementation and enforcement of social distancing measures, there is an urgent need to develop and implement plans to reduce non-COVID-19 demand for and temporarily increase capacity of health facilities.{$<$}/p{$>$}},
  file = {/mnt/data/Google Drive/Zotero/storage/HP5YTXXP/Team and Murray - 2020 - Forecasting COVID-19 impact on hospital bed-days, .pdf;/mnt/data/Google Drive/Zotero/storage/QMBBN8NS/2020.03.27.html},
  langid = {english}
}

@article{tetlockForecastingTournamentsTools2014,
  title = {Forecasting {{Tournaments}}: {{Tools}} for {{Increasing Transparency}} and {{Improving}} the {{Quality}} of {{Debate}}},
  shorttitle = {Forecasting {{Tournaments}}},
  author = {Tetlock, Philip E. and Mellers, Barbara A. and Rohrbaugh, Nick and Chen, Eva},
  date = {2014-08-01},
  journaltitle = {Current Directions in Psychological Science},
  shortjournal = {Curr Dir Psychol Sci},
  volume = {23},
  pages = {290--295},
  publisher = {{SAGE Publications Inc}},
  issn = {0963-7214},
  doi = {10.1177/0963721414534257},
  url = {https://doi.org/10.1177/0963721414534257},
  urldate = {2021-05-30},
  abstract = {Forecasting tournaments are level-playing-field competitions that reveal which individuals, teams, or algorithms generate more accurate probability estimates on which topics. This article describes a massive geopolitical tournament that tested clashing views on the feasibility of improving judgmental accuracy and on the best methods of doing so. The tournament’s winner, the Good Judgment Project, outperformed the simple average of the crowd by (a) designing new forms of cognitive-debiasing training, (b) incentivizing rigorous thinking in teams and prediction markets, (c) skimming top talent into elite collaborative teams of “super forecasters,” and (d) fine-tuning aggregation algorithms for distilling greater wisdom from crowds. Tournaments have the potential to open closed minds and increase assertion-to-evidence ratios in polarized scientific and policy debates.},
  keywords = {accuracy,forecasting,probability,tournaments},
  langid = {english},
  number = {4}
}

@online{thibautjombartemilysnightingalemarkjitolivierlepolaindewarouxgwenknightstefanflascherosalindeggoadamjkucharskicarla.b.pearsonsimonrproctercmmidncovworkinggroupwjohnedmunds.ForecastingCriticalCare2020,
  title = {Forecasting Critical Care Bed Requirements for {{COVID}}-19 Patients in {{England}}},
  author = {Thibaut Jombart, Emily S Nightingale, Mark Jit, Olivier le Polain de Waroux, Gwen Knight, Stefan Flasche, Rosalind Eggo, Adam J Kucharski, Carl A.B. Pearson, Simon R Procter, CMMID nCov working group \& W John Edmunds.},
  date = {2020-03-22T00:00:00+00:00},
  url = {https://cmmid.github.io/topics/covid19/current-patterns-transmission/ICU-projections.html},
  urldate = {2020-04-12},
  abstract = {We estimate critical care bed demand for COVID-19 cases in England for the next two weeks. Results suggest that current capacity might be reached or exceeded by the end of March 2020.},
  file = {/mnt/data/Google Drive/Zotero/storage/R38ZM99F/ICU-projections.html},
  langid = {english},
  organization = {{CMMID Repository}}
}

@article{thompsonImprovedInferenceTimevarying2019,
  title = {Improved Inference of Time-Varying Reproduction Numbers during Infectious Disease Outbreaks},
  author = {Thompson, R. N. and Stockwin, J. E. and van Gaalen, R. D. and Polonsky, J. A. and Kamvar, Z. N. and Demarsh, P. A. and Dahlqwist, E. and Li, S. and Miguel, E. and Jombart, T. and Lessler, J. and Cauchemez, S. and Cori, A.},
  date = {2019-08-26},
  journaltitle = {Epidemics},
  shortjournal = {Epidemics},
  pages = {100356},
  issn = {1755-4365},
  doi = {10.1016/j.epidem.2019.100356},
  url = {http://www.sciencedirect.com/science/article/pii/S1755436519300350},
  urldate = {2019-10-30},
  abstract = {Accurate estimation of the parameters characterising infectious disease transmission is vital for optimising control interventions during epidemics. A valuable metric for assessing the current threat posed by an outbreak is the time-dependent reproduction number, i.e. the expected number of secondary cases caused by each infected individual. This quantity can be estimated using data on the numbers of observed new cases at successive times during an epidemic and the distribution of the serial interval (the time between symptomatic cases in a transmission chain). Some methods for estimating the reproduction number rely on pre-existing estimates of the serial interval distribution and assume that the entire outbreak is driven by local transmission. Here we show that accurate inference of current transmissibility, and the uncertainty associated with this estimate, requires: (i) up-to-date observations of the serial interval to be included, and; (ii) cases arising from local transmission to be distinguished from those imported from elsewhere. We demonstrate how pathogen transmissibility can be inferred appropriately using datasets from outbreaks of H1N1 influenza, Ebola virus disease and Middle-East Respiratory Syndrome. We present a tool for estimating the reproduction number in real-time during infectious disease outbreaks accurately, which is available as an R software package (EpiEstim 2.2). It is also accessible as an interactive, user-friendly online interface (EpiEstim App), permitting its use by non-specialists. Our tool is easy to apply for assessing the transmission potential, and hence informing control, during future outbreaks of a wide range of invading pathogens.},
  file = {/mnt/data/Google Drive/Zotero/storage/QLQPSF74/Thompson et al. - 2019 - Improved inference of time-varying reproduction nu.pdf;/mnt/data/Google Drive/Zotero/storage/Y8GML2T9/S1755436519300350.html},
  keywords = {Disease control,Infectious disease epidemiology,Mathematical modelling,Parameter inference,Reproduction number,Serial interval},
  langid = {english},
  options = {useprefix=true}
}

@article{thoreyOnlineLearningContinuous2017,
  title = {Online Learning with the {{Continuous Ranked Probability Score}} for Ensemble Forecasting: {{Ensemble Online Learning}}},
  shorttitle = {Online Learning with the {{Continuous Ranked Probability Score}} for Ensemble Forecasting},
  author = {Thorey, J. and Mallet, V. and Baudin, P.},
  date = {2017-01},
  journaltitle = {Quarterly Journal of the Royal Meteorological Society},
  shortjournal = {Q.J.R. Meteorol. Soc.},
  volume = {143},
  pages = {521--529},
  issn = {00359009},
  doi = {10.1002/qj.2940},
  url = {http://doi.wiley.com/10.1002/qj.2940},
  urldate = {2020-05-14},
  abstract = {Ensemble forecasting resorts to multiple individual forecasts to produce a discrete probability distribution which accurately represents the uncertainties. Before every forecast, a weighted empirical distribution function is derived from the ensemble, so as to minimize the Continuous Ranked Probability Score (CRPS). We apply online learning techniques, which have previously been used for deterministic forecasting, and we adapt them for the minimization of the CRPS. The proposed method theoretically guarantees that the aggregated forecast competes, in terms of CRPS, against the best weighted empirical distribution function with weights constant in time. This is illustrated on synthetic data. Besides, our study improves the knowledge of the CRPS expectation for model mixtures. We generalize results on the bias of the CRPS computed with ensemble forecasts, and propose a new scheme to achieve fair CRPS minimization, without any assumption on the distributions.},
  file = {/mnt/data/Google Drive/Zotero/storage/CR9EWRQ8/Thorey et al. - 2017 - Online learning with the Continuous Ranked Probabi.pdf},
  langid = {english},
  number = {702}
}

@article{tibshiraniElementsStatisticalLearning,
  title = {Elements of {{Statistical Learning}}},
  author = {Tibshirani, Sami and Friedman, Harry},
  pages = {764},
  file = {/mnt/data/Google Drive/Zotero/storage/XDEHIRQM/Tibshirani and Friedman - Valerie and Patrick Hastie.pdf},
  langid = {english}
}

@article{tibshiraniValeriePatrickHastie,
  title = {Valerie and {{Patrick Hastie}}},
  author = {Tibshirani, Sami and Friedman, Harry},
  pages = {764},
  file = {/mnt/data/Google Drive/Zotero/storage/UZCINHYZ/Tibshirani and Friedman - Valerie and Patrick Hastie.pdf},
  langid = {english}
}

@article{timmermannForecastingMethodsFinance2018,
  title = {Forecasting {{Methods}} in {{Finance}}},
  author = {Timmermann, Allan},
  date = {2018},
  journaltitle = {Annual Review of Financial Economics},
  volume = {10},
  pages = {449--479},
  doi = {10.1146/annurev-financial-110217-022713},
  url = {https://doi.org/10.1146/annurev-financial-110217-022713},
  urldate = {2020-12-14},
  abstract = {Our review highlights some of the key challenges in financial forecasting problems and opportunities arising from the unique features of financial data. We analyze the difficulty of establishing predictability in an environment with a low signal-to-noise ratio, persistent predictors, and instability in predictive relations arising from competitive pressures and investors’ learning. We discuss approaches for forecasting the mean, variance, and probability distribution of asset returns. Finally, we discuss how to evaluate financial forecasts while accounting for the possibility that numerous forecasting models may have been considered, leading to concerns of data mining.},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-financial-110217-022713},
  number = {1}
}

@online{timothywrusselljoelhellewell1samabbott1nickgoldinghamishgibbschristopherijarviskevinvanzandvoortcmmidncovworkinggroupstefanflascherosalindeggowjohnedmundsadamjkucharski.UsingDelayadjustedCase2020,
  title = {Using a Delay-Adjusted Case Fatality Ratio to Estimate under-Reporting},
  author = {Timothy W Russell, Joel Hellewell1, Sam Abbott1, Nick Golding, Hamish Gibbs, Christopher I Jarvis, Kevin van Zandvoort, CMMID nCov working group, Stefan Flasche, Rosalind Eggo, W John Edmunds \& Adam J Kucharski.},
  date = {2020-03-22T00:00:00+00:00},
  url = {https://cmmid.github.io/topics/covid19/severity/global_cfr_estimates.html},
  urldate = {2020-04-12},
  abstract = {Using a corrected case fatality ratio, we calculate estimates of the level of under-reporting for any country with greater than ten deaths},
  file = {/mnt/data/Google Drive/Zotero/storage/BK7WAVI3/global_cfr_estimates.html},
  langid = {english},
  organization = {{CMMID Repository}}
}

@online{timothywrusselljoelhellewellsamabbottnickgoldinghamishgibbschristopherijarviskevinvanzandvoortcmmidncovworkinggroupstefanflascherosalindmeggowjohnedmundsadamjkucharskiUsingDelayadjustedCase2020,
  title = {Using a Delay-Adjusted Case Fatality Ratio to Estimate under-Reporting},
  author = {{Timothy W Russell, Joel Hellewell, Sam Abbott, Nick Golding, Hamish Gibbs, Christopher I Jarvis, Kevin van Zandvoort, CMMID nCov working group, Stefan Flasche, Rosalind M Eggo, W John Edmunds \& Adam J Kucharski}},
  date = {2020-03-22T00:00:00+00:00},
  url = {https://cmmid.github.io/topics/covid19/global_cfr_estimates.html},
  urldate = {2020-08-12},
  abstract = {Using a corrected case fatality ratio, we calculate estimates of the level of under-reporting for any country with greater than ten deaths},
  file = {/mnt/data/Google Drive/Zotero/storage/KJ34HWBU/global_cfr_estimates.html},
  langid = {english},
  organization = {{CMMID Repository}}
}

@online{umass-amherstinfluenzaforecastingcenterofexcellenceCovid19forecasthubOrg2020,
  title = {Covid19forecasthub.Org},
  author = {{UMass-Amherst Influenza Forecasting Center of Excellence}},
  date = {2020},
  url = {https://covid19forecasthub.org/},
  urldate = {2020-08-07},
  file = {/mnt/data/Google Drive/Zotero/storage/J5JLPCXS/covid19forecasthub.org.html},
  organization = {{https://github.com/reichlab/covid19-forecast-hub}}
}

@online{UtilizingGeneralHuman,
  title = {Utilizing General Human Movement Models to Predict the Spread of Emerging Infectious Diseases in Resource Poor Settings | {{Scientific Reports}}},
  url = {https://www.nature.com/articles/s41598-019-41192-3},
  urldate = {2019-09-16},
  file = {/mnt/data/Google Drive/Zotero/storage/I9FTYTFG/s41598-019-41192-3.html}
}

@article{vandenbergeObservationWeightsUnlock2018,
  title = {Observation Weights Unlock Bulk {{RNA}}-Seq Tools for Zero Inflation and Single-Cell Applications},
  author = {Van den Berge, Koen and Perraudeau, Fanny and Soneson, Charlotte and Love, Michael I. and Risso, Davide and Vert, Jean-Philippe and Robinson, Mark D. and Dudoit, Sandrine and Clement, Lieven},
  date = {2018-02-26},
  journaltitle = {Genome Biology},
  shortjournal = {Genome Biology},
  volume = {19},
  pages = {24},
  issn = {1474-760X},
  doi = {10.1186/s13059-018-1406-4},
  url = {https://doi.org/10.1186/s13059-018-1406-4},
  urldate = {2019-04-11},
  abstract = {Dropout events in single-cell RNA sequencing (scRNA-seq) cause many transcripts to go undetected and induce an excess of zero read counts, leading to power issues in differential expression (DE) analysis. This has triggered the development of bespoke scRNA-seq DE methods to cope with zero inflation. Recent evaluations, however, have shown that dedicated scRNA-seq tools provide no advantage compared to traditional bulk RNA-seq tools. We introduce a weighting strategy, based on a zero-inflated negative binomial model, that identifies excess zero counts and generates gene- and cell-specific weights to unlock bulk RNA-seq DE pipelines for zero-inflated data, boosting performance for scRNA-seq.},
  file = {/mnt/data/Google Drive/Zotero/storage/SMUBSS9S/Van den Berge et al. - 2018 - Observation weights unlock bulk RNA-seq tools for .pdf;/mnt/data/Google Drive/Zotero/storage/NX28TJNP/s13059-018-1406-4.html},
  number = {1}
}

@article{vandewielBayesianAnalysisRNA2013,
  title = {Bayesian Analysis of {{RNA}} Sequencing Data by Estimating Multiple Shrinkage Priors},
  author = {Van De Wiel, Mark A. and Leday, Gwenaël G. R. and Pardo, Luba and Rue, Håvard and Van Der Vaart, Aad W. and Van Wieringen, Wessel N.},
  date = {2013-01},
  journaltitle = {Biostatistics (Oxford, England)},
  shortjournal = {Biostatistics},
  volume = {14},
  pages = {113--128},
  issn = {1468-4357},
  doi = {10.1093/biostatistics/kxs031},
  abstract = {Next generation sequencing is quickly replacing microarrays as a technique to probe different molecular levels of the cell, such as DNA or RNA. The technology provides higher resolution, while reducing bias. RNA sequencing results in counts of RNA strands. This type of data imposes new statistical challenges. We present a novel, generic approach to model and analyze such data. Our approach aims at large flexibility of the likelihood (count) model and the regression model alike. Hence, a variety of count models is supported, such as the popular NB model, which accounts for overdispersion. In addition, complex, non-balanced designs and random effects are accommodated. Like some other methods, our method provides shrinkage of dispersion-related parameters. However, we extend it by enabling joint shrinkage of parameters, including those for which inference is desired. We argue that this is essential for Bayesian multiplicity correction. Shrinkage is effectuated by empirically estimating priors. We discuss several parametric (mixture) and non-parametric priors and develop procedures to estimate (parameters of) those. Inference is provided by means of local and Bayesian false discovery rates. We illustrate our method on several simulations and two data sets, also to compare it with other methods. Model- and data-based simulations show substantial improvements in the sensitivity at the given specificity. The data motivate the use of the ZI-NB as a powerful alternative to the NB, which results in higher detection rates for low-count data. Finally, compared with other methods, the results on small sample subsets are more reproducible when validated on their large sample complements, illustrating the importance of the type of shrinkage.},
  eprint = {22988280},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/7N4XVQ6Y/Van De Wiel et al. - 2013 - Bayesian analysis of RNA sequencing data by estima.pdf},
  keywords = {Base Sequence,Bayes Theorem,Computer Simulation,Data Interpretation; Statistical,Models; Statistical,Molecular Sequence Data,RNA,Sequence Analysis; RNA},
  langid = {english},
  number = {1}
}

@article{viboudFutureInfluenzaForecasts2019,
  title = {The Future of Influenza Forecasts},
  author = {Viboud, Cécile and Vespignani, Alessandro},
  date = {2019-02-19},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {116},
  pages = {2802--2804},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1822167116},
  url = {https://www.pnas.org/content/116/8/2802},
  urldate = {2020-04-12},
  abstract = {Recent years have seen a growing interest in generating real-time epidemic forecasts to help control infectious diseases, prompted by a succession of global and regional outbreaks. Increased availability of epidemiological data and novel digital data streams such as search engine queries and social media (1, 2), together with the rise of machine learning and sophisticated statistical approaches, have injected new blood into the science of outbreak forecasts (3, 4). In parallel, mechanistic transmission models have benefited from computational advances and extensive data on the mobility and sociodemographic structure of human populations (5, 6). In this rapidly advancing research landscape, modeling consortiums have generated systematic model comparisons of the impact of new interventions and ensemble predictions of outbreak trajectory, for use by decision makers (7⇓⇓⇓⇓–12). Despite the rapid development of disease forecasting as a discipline, however, and the interest of public health policy makers in making better use of analytics tools to control outbreaks, forecasts are rarely operational in the same way that weather forecasts, extreme events, and climate predictions are. The influenza study by Reich et al. (13) in PNAS is a unique example of multiyear infectious disease forecasts featuring a variety of modeling approaches, with consistent model formulations and forecasting targets throughout the 7-y study period (13). This is a major improvement over previous model comparison studies that used different targets and time horizons and sometimes different epidemiological datasets. While there is considerable interest among modelers in advancing the science of disease forecasts, the level of confidence of the public health community in exploiting these predictions in real-world situations remains unclear. The disconnect is in part due to poor understanding of modeling concepts by policy experts, which is compounded by a lack of a well-established operational framework for using and …  [↵][1]1To whom correspondence should be addressed. Email: viboudc\{at\}mail.nih.gov.  [1]: \#xref-corresp-1-1},
  eprint = {30737293},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/JFE2J84D/Viboud and Vespignani - 2019 - The future of influenza forecasts.pdf;/mnt/data/Google Drive/Zotero/storage/67WHC8X4/2802.html},
  langid = {english},
  number = {8}
}

@article{viboudRAPIDDEbolaForecasting2018,
  title = {The {{RAPIDD}} Ebola Forecasting Challenge: {{Synthesis}} and Lessons Learnt},
  shorttitle = {The {{RAPIDD}} Ebola Forecasting Challenge},
  author = {Viboud, Cécile and Sun, Kaiyuan and Gaffey, Robert and Ajelli, Marco and Fumanelli, Laura and Merler, Stefano and Zhang, Qian and Chowell, Gerardo and Simonsen, Lone and Vespignani, Alessandro},
  date = {2018-03-01},
  journaltitle = {Epidemics},
  shortjournal = {Epidemics},
  volume = {22},
  pages = {13--21},
  issn = {1755-4365},
  doi = {10.1016/j.epidem.2017.08.002},
  url = {https://www.sciencedirect.com/science/article/pii/S1755436517301275},
  urldate = {2021-05-30},
  abstract = {Infectious disease forecasting is gaining traction in the public health community; however, limited systematic comparisons of model performance exist. Here we present the results of a synthetic forecasting challenge inspired by the West African Ebola crisis in 2014–2015 and involving 16 international academic teams and US government agencies, and compare the predictive performance of 8 independent modeling approaches. Challenge participants were invited to predict 140 epidemiological targets across 5 different time points of 4 synthetic Ebola outbreaks, each involving different levels of interventions and “fog of war” in outbreak data made available for predictions. Prediction targets included 1–4 week-ahead case incidences, outbreak size, peak timing, and several natural history parameters. With respect to weekly case incidence targets, ensemble predictions based on a Bayesian average of the 8 participating models outperformed any individual model and did substantially better than a null auto-regressive model. There was no relationship between model complexity and prediction accuracy; however, the top performing models for short-term weekly incidence were reactive models with few parameters, fitted to a short and recent part of the outbreak. Individual model outputs and ensemble predictions improved with data accuracy and availability; by the second time point, just before the peak of the epidemic, estimates of final size were within 20\% of the target. The 4th challenge scenario − mirroring an uncontrolled Ebola outbreak with substantial data reporting noise − was poorly predicted by all modeling teams. Overall, this synthetic forecasting challenge provided a deep understanding of model performance under controlled data and epidemiological conditions. We recommend such “peace time” forecasting challenges as key elements to improve coordination and inspire collaboration between modeling groups ahead of the next pandemic threat, and to assess model forecasting accuracy for a variety of known and hypothetical pathogens.},
  file = {/mnt/data/Google Drive/Zotero/storage/FIZA5NMQ/Viboud et al. - 2018 - The RAPIDD ebola forecasting challenge Synthesis .pdf;/mnt/data/Google Drive/Zotero/storage/KA34URD6/S1755436517301275.html},
  keywords = {Data accuracy,Ebola epidemic,Forecasting challenge,Mathematical modeling,Model comparison,Prediction horizon,Prediction performance,Synthetic data},
  langid = {english},
  series = {The {{RAPIDD Ebola Forecasting Challenge}}}
}

@article{wahidMicroRNAsSynthesisMechanism2010,
  title = {{{MicroRNAs}}: {{Synthesis}}, Mechanism, Function, and Recent Clinical Trials},
  shorttitle = {{{MicroRNAs}}},
  author = {Wahid, Fazli and Shehzad, Adeeb and Khan, Taous and Kim, You Young},
  date = {2010-11-01},
  journaltitle = {Biochimica et Biophysica Acta (BBA) - Molecular Cell Research},
  shortjournal = {Biochimica et Biophysica Acta (BBA) - Molecular Cell Research},
  volume = {1803},
  pages = {1231--1243},
  issn = {0167-4889},
  doi = {10.1016/j.bbamcr.2010.06.013},
  url = {http://www.sciencedirect.com/science/article/pii/S0167488910001837},
  urldate = {2019-05-22},
  abstract = {MicroRNAs (miRNAs) are a class of small, endogenous RNAs of 21–25 nucleotides (nts) in length. They play an important regulatory role in animals and plants by targeting specific mRNAs for degradation or translation repression. Recent scientific advances have revealed the synthesis pathways and the regulatory mechanisms of miRNAs in animals and plants. miRNA-based regulation is implicated in disease etiology and has been studied for treatment. Furthermore, several preclinical and clinical trials have been initiated for miRNA-based therapeutics. In this review, the existing knowledge about miRNAs synthesis, mechanisms for regulation of the genome, and their widespread functions in animals and plants is summarized. The current status of preclinical and clinical trials regarding miRNA therapeutics is also reviewed. The recent findings in miRNA studies, summarized in this review, may add new dimensions to small RNA biology and miRNA therapeutics.},
  file = {/mnt/data/Google Drive/Zotero/storage/9VVBJ4HA/Wahid et al. - 2010 - MicroRNAs Synthesis, mechanism, function, and rec.pdf;/mnt/data/Google Drive/Zotero/storage/DP52PSGJ/S0167488910001837.html},
  keywords = {miRNA,miRNA based gene regulations,miRNA therapeutics,mRNA degradation,Small RNAs},
  number = {11}
}

@article{wangDeepLearningSpatioTemporal2019,
  title = {Deep {{Learning}} for {{Spatio}}-{{Temporal Data Mining}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} for {{Spatio}}-{{Temporal Data Mining}}},
  author = {Wang, Senzhang and Cao, Jiannong and Yu, Philip S.},
  date = {2019-06-11},
  url = {https://arxiv.org/abs/1906.04928v2},
  urldate = {2019-09-19},
  abstract = {With the fast development of various positioning techniques such as Global Position System (GPS), mobile devices and remote sensing, spatio-temporal data has become increasingly available nowadays. Mining valuable knowledge from spatio-temporal data is critically important to many real world applications including human mobility understanding, smart transportation, urban planning, public safety, health care and environmental management. As the number, volume and resolution of spatio-temporal datasets increase rapidly, traditional data mining methods, especially statistics based methods for dealing with such data are becoming overwhelmed. Recently, with the advances of deep learning techniques, deep leaning models such as convolutional neural network (CNN) and recurrent neural network (RNN) have enjoyed considerable success in various machine learning tasks due to their powerful hierarchical feature learning ability in both spatial and temporal domains, and have been widely applied in various spatio-temporal data mining (STDM) tasks such as predictive learning, representation learning, anomaly detection and classification. In this paper, we provide a comprehensive survey on recent progress in applying deep learning techniques for STDM. We first categorize the types of spatio-temporal data and briefly introduce the popular deep learning models that are used in STDM. Then a framework is introduced to show a general pipeline of the utilization of deep learning models for STDM. Next we classify existing literatures based on the types of ST data, the data mining tasks, and the deep learning models, followed by the applications of deep learning for STDM in different domains including transportation, climate science, human mobility, location based social network, crime analysis, and neuroscience. Finally, we conclude the limitations of current research and point out future research directions.},
  file = {/mnt/data/Google Drive/Zotero/storage/D7VKGB9B/Wang et al. - 2019 - Deep Learning for Spatio-Temporal Data Mining A S.pdf;/mnt/data/Google Drive/Zotero/storage/KQZYU92I/1906.html},
  langid = {english}
}

@online{wangDeepLearningSpatioTemporal2019a,
  title = {Deep {{Learning}} for {{Spatio}}-{{Temporal Data Mining}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} for {{Spatio}}-{{Temporal Data Mining}}},
  author = {Wang, Senzhang and Cao, Jiannong and Yu, Philip S.},
  date = {2019-06-11},
  url = {http://arxiv.org/abs/1906.04928},
  urldate = {2019-09-19},
  abstract = {With the fast development of various positioning techniques such as Global Position System (GPS), mobile devices and remote sensing, spatio-temporal data has become increasingly available nowadays. Mining valuable knowledge from spatio-temporal data is critically important to many real world applications including human mobility understanding, smart transportation, urban planning, public safety, health care and environmental management. As the number, volume and resolution of spatio-temporal datasets increase rapidly, traditional data mining methods, especially statistics based methods for dealing with such data are becoming overwhelmed. Recently, with the advances of deep learning techniques, deep leaning models such as convolutional neural network (CNN) and recurrent neural network (RNN) have enjoyed considerable success in various machine learning tasks due to their powerful hierarchical feature learning ability in both spatial and temporal domains, and have been widely applied in various spatio-temporal data mining (STDM) tasks such as predictive learning, representation learning, anomaly detection and classification. In this paper, we provide a comprehensive survey on recent progress in applying deep learning techniques for STDM. We first categorize the types of spatio-temporal data and briefly introduce the popular deep learning models that are used in STDM. Then a framework is introduced to show a general pipeline of the utilization of deep learning models for STDM. Next we classify existing literatures based on the types of ST data, the data mining tasks, and the deep learning models, followed by the applications of deep learning for STDM in different domains including transportation, climate science, human mobility, location based social network, crime analysis, and neuroscience. Finally, we conclude the limitations of current research and point out future research directions.},
  archiveprefix = {arXiv},
  eprint = {1906.04928},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/NG8DKUBP/Wang et al. - 2019 - Deep Learning for Spatio-Temporal Data Mining A S.pdf},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryclass = {cs, stat}
}

@article{wangRNASeqRevolutionaryTool2009,
  title = {{{RNA}}-{{Seq}}: A Revolutionary Tool for Transcriptomics},
  shorttitle = {{{RNA}}-{{Seq}}},
  author = {Wang, Zhong and Gerstein, Mark and Snyder, Michael},
  date = {2009-01},
  journaltitle = {Nature reviews. Genetics},
  shortjournal = {Nat Rev Genet},
  volume = {10},
  pages = {57--63},
  issn = {1471-0056},
  doi = {10.1038/nrg2484},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2949280/},
  urldate = {2018-10-04},
  abstract = {RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.},
  eprint = {19015660},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/HEC6RJVI/Wang et al. - 2009 - RNA-Seq a revolutionary tool for transcriptomics.pdf},
  number = {1},
  pmcid = {PMC2949280}
}

@article{wellsExacerbationEbolaOutbreaks2019,
  title = {The Exacerbation of {{Ebola}} Outbreaks by Conflict in the {{Democratic Republic}} of the {{Congo}}},
  author = {Wells, Chad R. and Pandey, Abhishek and Mbah, Martial L. Ndeffo and Gaüzère, Bernard-A. and Malvy, Denis and Singer, Burton H. and Galvani, Alison P.},
  date = {2019-10-16},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  pages = {201913980},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1913980116},
  url = {https://www.pnas.org/content/early/2019/10/15/1913980116},
  urldate = {2019-10-30},
  abstract = {The interplay between civil unrest and disease transmission is not well understood. Violence targeting healthcare workers and Ebola treatment centers in the Democratic Republic of the Congo (DRC) has been thwarting the case isolation, treatment, and vaccination efforts. The extent to which conflict impedes public health response and contributes to incidence has not previously been evaluated. We construct a timeline of conflict events throughout the course of the epidemic and provide an ethnographic appraisal of the local conditions that preceded and followed conflict events. Informed by temporal incidence and conflict data as well as the ethnographic evidence, we developed a model of Ebola transmission and control to assess the impact of conflict on the epidemic in the eastern DRC from April 30, 2018, to June 23, 2019. We found that both the rapidity of case isolation and the population-level effectiveness of vaccination varied notably as a result of preceding unrest and subsequent impact of conflict events. Furthermore, conflict events were found to reverse an otherwise declining phase of the epidemic trajectory. Our model framework can be extended to other infectious diseases in the same and other regions of the world experiencing conflict and violence.},
  eprint = {31636188},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/ULSEZQQ3/Wells et al. - 2019 - The exacerbation of Ebola outbreaks by conflict in.pdf;/mnt/data/Google Drive/Zotero/storage/EH3MGU3D/1913980116.html},
  keywords = {epidemiology,healthcare workers,humanitarian crisis,insecurity},
  langid = {english}
}

@article{wellsExacerbationEbolaOutbreaks2019a,
  title = {The Exacerbation of {{Ebola}} Outbreaks by Conflict in the {{Democratic Republic}} of the {{Congo}}},
  author = {Wells, Chad R. and Pandey, Abhishek and Ndeffo Mbah, Martial L. and Gaüzère, Bernard-A. and Malvy, Denis and Singer, Burton H. and Galvani, Alison P.},
  date = {2019-11-26},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc Natl Acad Sci USA},
  volume = {116},
  pages = {24366--24372},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1913980116},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1913980116},
  urldate = {2020-01-16},
  abstract = {The interplay between civil unrest and disease transmission is not well understood. Violence targeting healthcare workers and Ebola treatment centers in the Democratic Republic of the Congo (DRC) has been thwarting the case isolation, treatment, and vaccination efforts. The extent to which conflict impedes public health response and contributes to incidence has not previously been evaluated. We construct a timeline of conflict events throughout the course of the epidemic and provide an ethnographic appraisal of the local conditions that preceded and followed conflict events. Informed by temporal incidence and conflict data as well as the ethnographic evidence, we developed a model of Ebola transmission and control to assess the impact of conflict on the epidemic in the eastern DRC from April 30, 2018, to June 23, 2019. We found that both the rapidity of case isolation and the population-level effectiveness of vaccination varied notably as a result of preceding unrest and subsequent impact of conflict events. Furthermore, conflict events were found to reverse an otherwise declining phase of the epidemic trajectory. Our model framework can be extended to other infectious diseases in the same and other regions of the world experiencing conflict and violence.},
  file = {/mnt/data/Google Drive/Zotero/storage/4GPMKJMW/Wells et al. - 2019 - The exacerbation of Ebola outbreaks by conflict in.pdf},
  langid = {english},
  number = {48}
}

@online{WhyOptimOut2016,
  title = {Why Optim() Is out of Date},
  date = {2016-11-10T14:33:38+00:00},
  url = {https://www.r-bloggers.com/why-optim-is-out-of-date/},
  urldate = {2020-04-07},
  abstract = {Why optim() is out of date And perhaps you should be careful using it Once upon a time Once upon a time, there was a young Oxford D.Phil. graduate with a thesis on quantum mechanics who — by virtue of a mixup in identities — got hired as an Agricultural Economist. He was actually better…},
  file = {/mnt/data/Google Drive/Zotero/storage/ANNBD4ZN/why-optim-is-out-of-date.html},
  langid = {american},
  organization = {{R-bloggers}}
}

@article{wickramasuriyaOptimalForecastReconciliation2019,
  title = {Optimal {{Forecast Reconciliation}} for {{Hierarchical}} and {{Grouped Time Series Through Trace Minimization}}},
  author = {Wickramasuriya, Shanika L. and Athanasopoulos, George and Hyndman, Rob J.},
  date = {2019-04-03},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {114},
  pages = {804--819},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2018.1448825},
  url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2018.1448825},
  urldate = {2020-05-15},
  abstract = {Large collections of time series often have aggregation constraints due to product or geographical groupings. The forecasts for the most disaggregated series are usually required to add-up exactly to the forecasts of the aggregated series, a constraint we refer to as “coherence”. Forecast reconciliation is the process of adjusting forecasts to make them coherent.},
  file = {/mnt/data/Google Drive/Zotero/storage/TIDP2UWF/Wickramasuriya et al. - 2019 - Optimal Forecast Reconciliation for Hierarchical a.pdf},
  langid = {english},
  number = {526}
}

@article{wilbanksEvaluationAlgorithmPerformance2010,
  title = {Evaluation of {{Algorithm Performance}} in {{ChIP}}-{{Seq Peak Detection}}},
  author = {Wilbanks, Elizabeth G. and Facciotti, Marc T.},
  date = {2010-07-08},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS One},
  volume = {5},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0011471},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2900203/},
  urldate = {2018-12-03},
  abstract = {Next-generation DNA sequencing coupled with chromatin immunoprecipitation (ChIP-seq) is revolutionizing our ability to interrogate whole genome protein-DNA interactions. Identification of protein binding sites from ChIP-seq data has required novel computational tools, distinct from those used for the analysis of ChIP-Chip experiments. The growing popularity of ChIP-seq spurred the development of many different analytical programs (at last count, we noted 31 open source methods), each with some purported advantage. Given that the literature is dense and empirical benchmarking challenging, selecting an appropriate method for ChIP-seq analysis has become a daunting task. Herein we compare the performance of eleven different peak calling programs on common empirical, transcription factor datasets and measure their sensitivity, accuracy and usability. Our analysis provides an unbiased critical assessment of available technologies, and should assist researchers in choosing a suitable tool for handling ChIP-seq data.},
  eprint = {20628599},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/L8H9MCR3/Wilbanks and Facciotti - 2010 - Evaluation of Algorithm Performance in ChIP-Seq Pe.pdf},
  number = {7},
  pmcid = {PMC2900203}
}

@article{wilkinsonModellingApproachABC1977,
  title = {A Modelling Approach to {{ABC}}},
  author = {Wilkinson, Richard},
  date = {1977},
  pages = {88},
  file = {/mnt/data/Google Drive/Zotero/storage/95L75H5X/Wilkinson - 1977 - A modelling approach to ABC.pdf},
  langid = {english}
}

@article{wilksEnforcingCalibrationEnsemble2018,
  title = {Enforcing Calibration in Ensemble Postprocessing: {{Enforcing Calibration}} in {{Ensemble Postprocessing}}},
  shorttitle = {Enforcing Calibration in Ensemble Postprocessing},
  author = {Wilks, Daniel S.},
  date = {2018-01},
  journaltitle = {Quarterly Journal of the Royal Meteorological Society},
  shortjournal = {Q.J.R. Meteorol. Soc.},
  volume = {144},
  pages = {76--84},
  issn = {00359009},
  doi = {10.1002/qj.3185},
  url = {http://doi.wiley.com/10.1002/qj.3185},
  urldate = {2020-04-06},
  file = {/mnt/data/Google Drive/Zotero/storage/KDGXTZRJ/Wilks - 2018 - Enforcing calibration in ensemble postprocessing .pdf},
  langid = {english},
  number = {710}
}

@incollection{wilksUnivariateEnsemblePostprocessing2018,
  title = {Univariate {{Ensemble Postprocessing}}},
  booktitle = {Statistical {{Postprocessing}} of {{Ensemble Forecasts}}},
  author = {Wilks, Daniel S.},
  date = {2018},
  pages = {49--89},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-812372-0.00003-0},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128123720000030},
  urldate = {2020-05-14},
  file = {/mnt/data/Google Drive/Zotero/storage/T9LR4PUI/Wilks - 2018 - Univariate Ensemble Postprocessing.pdf},
  isbn = {978-0-12-812372-0},
  langid = {english}
}

@article{wilsonGoodEnoughPractices2017,
  title = {Good Enough Practices in Scientific Computing},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  date = {2017-06-22},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {13},
  pages = {e1005510},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005510},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510},
  urldate = {2021-04-30},
  abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
  file = {/mnt/data/Google Drive/Zotero/storage/83P6YWIC/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf;/mnt/data/Google Drive/Zotero/storage/57E4YB9G/article.html},
  keywords = {Computer software,Control systems,Data management,Metadata,Programming languages,Reproducibility,Software tools,Source code},
  langid = {english},
  number = {6}
}

@article{winklerScoringRulesEvaluation1996,
  title = {Scoring Rules and the Evaluation of Probabilities},
  author = {Winkler, R. L. and Muñoz, Javier and Cervera, José L. and Bernardo, José M. and Blattenberger, Gail and Kadane, Joseph B. and Lindley, Dennis V. and Murphy, Allan H. and Oliver, Robert M. and Ríos-Insua, David},
  date = {1996-06-01},
  journaltitle = {Test},
  shortjournal = {Test},
  volume = {5},
  pages = {1--60},
  issn = {1863-8260},
  doi = {10.1007/BF02562681},
  url = {https://doi.org/10.1007/BF02562681},
  urldate = {2021-03-04},
  abstract = {In Bayesian inference and decision analysis, inferences and predictions are inherently probabilistic in nature. Scoring rules, which involve the computation of a score based on probability forecasts and what actually occurs, can be used to evaluate probabilities and to provide appropriate incentives for “good” probabilities. This paper review scoring rules and some related measures for evaluating probabilities, including decompositions of scoring rules and attributes of “goodness” of probabilites, comparability of scores, and the design of scoring rules for specific inferential and decision-making problems},
  file = {/mnt/data/Google Drive/Zotero/storage/VHTQR6BK/Winkler et al. - 1996 - Scoring rules and the evaluation of probabilities.pdf},
  langid = {english},
  number = {1}
}

@report{woodyProjectionsFirstwaveCOVID192020,
  title = {Projections for First-Wave {{COVID}}-19 Deaths across the {{US}} Using Social-Distancing Measures Derived from Mobile Phones},
  author = {Woody, Spencer and Garcia Tec, Mauricio and Dahan, Maytal and Gaither, Kelly and Lachmann, Michael and Fox, Spencer and Meyers, Lauren Ancel and Scott, James G},
  date = {2020-04-22},
  institution = {{Infectious Diseases (except HIV/AIDS)}},
  doi = {10.1101/2020.04.16.20068163},
  url = {http://medrxiv.org/lookup/doi/10.1101/2020.04.16.20068163},
  urldate = {2020-08-21},
  abstract = {We propose a Bayesian model for projecting first-wave COVID-19 deaths in all 50 U.S. states. Our model's projections are based on data derived from mobile-phone GPS traces, which allows us to estimate how social-distancing behavior is "flattening the curve" in each state. In a two-week look-ahead test of out-of-sample forecasting accuracy, our model significantly outperforms the widely used model from the Institute for Health Metrics and Evaluation (IHME), achieving 42\% lower prediction error: 13.2 deaths per day average error across all U.S. states, versus 22.8 deaths per day average error for the IHME model. Our model also provides an accurate, if slightly conservative, assessment of forecasting accuracy: in the same look-ahead test, 98\% of data points fell within the model's 95\% credible intervals. Our model's projections are updated daily at https://covid-19. tacc.utexas.edu/projections/},
  file = {/mnt/data/Google Drive/Zotero/storage/ADLHTZZN/Woody et al. - 2020 - Projections for first-wave COVID-19 deaths across .pdf},
  langid = {english},
  type = {preprint}
}

@article{wuNewShrinkageEstimator2013,
  title = {A New Shrinkage Estimator for Dispersion Improves Differential Expression Detection in {{RNA}}-Seq Data},
  author = {Wu, Hao and Wang, Chi and Wu, Zhijin},
  date = {2013-04},
  journaltitle = {Biostatistics (Oxford, England)},
  shortjournal = {Biostatistics},
  volume = {14},
  pages = {232--243},
  issn = {1468-4357},
  doi = {10.1093/biostatistics/kxs033},
  abstract = {Recent developments in RNA-sequencing (RNA-seq) technology have led to a rapid increase in gene expression data in the form of counts. RNA-seq can be used for a variety of applications, however, identifying differential expression (DE) remains a key task in functional genomics. There have been a number of statistical methods for DE detection for RNA-seq data. One common feature of several leading methods is the use of the negative binomial (Gamma-Poisson mixture) model. That is, the unobserved gene expression is modeled by a gamma random variable and, given the expression, the sequencing read counts are modeled as Poisson. The distinct feature in various methods is how the variance, or dispersion, in the Gamma distribution is modeled and estimated. We evaluate several large public RNA-seq datasets and find that the estimated dispersion in existing methods does not adequately capture the heterogeneity of biological variance among samples. We present a new empirical Bayes shrinkage estimate of the dispersion parameters and demonstrate improved DE detection.},
  eprint = {23001152},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/DG7W28SY/Wu et al. - 2013 - A new shrinkage estimator for dispersion improves .pdf},
  keywords = {Bayes Theorem,Binomial Distribution,Biostatistics,Databases; Nucleic Acid,Gene Expression Profiling,Humans,Models; Statistical,Poisson Distribution,Sequence Analysis; RNA},
  langid = {english},
  number = {2},
  pmcid = {PMC3590927}
}

@article{wuNewShrinkageEstimator2013a,
  title = {A New Shrinkage Estimator for Dispersion Improves Differential Expression Detection in {{RNA}}-Seq Data},
  author = {Wu, Hao and Wang, Chi and Wu, Zhijin},
  date = {2013-04-01},
  journaltitle = {Biostatistics},
  shortjournal = {Biostatistics},
  volume = {14},
  pages = {232--243},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxs033},
  url = {https://academic.oup.com/biostatistics/article/14/2/232/376433},
  urldate = {2019-05-14},
  abstract = {Abstract.  Recent developments in RNA-sequencing (RNA-seq) technology have led to a rapid increase in gene expression data in the form of counts. RNA-seq can be},
  file = {/mnt/data/Google Drive/Zotero/storage/HQ3G25GD/Wu et al. - 2013 - A new shrinkage estimator for dispersion improves .pdf;/mnt/data/Google Drive/Zotero/storage/J3FQHLX9/376433.html},
  langid = {english},
  number = {2}
}

@article{yamanaSuperensembleForecastsDengue2016,
  title = {Superensemble Forecasts of Dengue Outbreaks},
  author = {Yamana, Teresa K. and Kandula, Sasikiran and Shaman, Jeffrey},
  date = {2016-10-31},
  journaltitle = {Journal of The Royal Society Interface},
  shortjournal = {Journal of The Royal Society Interface},
  volume = {13},
  pages = {20160410},
  publisher = {{Royal Society}},
  doi = {10.1098/rsif.2016.0410},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsif.2016.0410},
  urldate = {2021-05-30},
  abstract = {In recent years, a number of systems capable of predicting future infectious disease incidence have been developed. As more of these systems are operationalized, it is important that the forecasts generated by these different approaches be formally reconciled so that individual forecast error and bias are reduced. Here we present a first example of such multi-system, or superensemble, forecast. We develop three distinct systems for predicting dengue, which are applied retrospectively to forecast outbreak characteristics in San Juan, Puerto Rico. We then use Bayesian averaging methods to combine the predictions from these systems and create superensemble forecasts. We demonstrate that on average, the superensemble approach produces more accurate forecasts than those made from any of the individual forecasting systems.},
  file = {/mnt/data/Google Drive/Zotero/storage/E5APZHIN/Yamana et al. - 2016 - Superensemble forecasts of dengue outbreaks.pdf;/mnt/data/Google Drive/Zotero/storage/2RH9MFEI/rsif.2016.html},
  number = {123}
}

@article{yangDurationUrinationDoes2014,
  title = {Duration of Urination Does Not Change with Body Size},
  author = {Yang, Patricia J. and Pham, Jonathan and Choo, Jerome and Hu, David L.},
  date = {2014-08-19},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {111},
  pages = {11932--11937},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1402289111},
  url = {https://www.pnas.org/content/111/33/11932},
  urldate = {2020-01-27},
  abstract = {Many urological studies rely on models of animals, such as rats and pigs, but their relation to the human urinary system is poorly understood. Here, we elucidate the hydrodynamics of urination across five orders of magnitude in body mass. Using high-speed videography and flow-rate measurement obtained at Zoo Atlanta, we discover that all mammals above 3 kg in weight empty their bladders over nearly constant duration of 21 ± 13 s. This feat is possible, because larger animals have longer urethras and thus, higher gravitational force and higher flow speed. Smaller mammals are challenged during urination by high viscous and capillary forces that limit their urine to single drops. Our findings reveal that the urethra is a flow-enhancing device, enabling the urinary system to be scaled up by a factor of 3,600 in volume without compromising its function. This study may help to diagnose urinary problems in animals as well as inspire the design of scalable hydrodynamic systems based on those in nature.},
  eprint = {24969420},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/XSC7XIRG/Yang et al. - 2014 - Duration of urination does not change with body si.pdf;/mnt/data/Google Drive/Zotero/storage/R58JV3WZ/11932.html},
  keywords = {allometry,Bernoulli's principle,scaling,urology},
  langid = {english},
  number = {33}
}

@online{yaoBayesianAggregation2019,
  title = {Bayesian {{Aggregation}}},
  author = {Yao, Yuling},
  date = {2019-12-24},
  url = {http://arxiv.org/abs/1912.11218},
  urldate = {2020-05-25},
  abstract = {A general challenge in statistics is prediction in the presence of multiple candidate models or learning algorithms. Model aggregation tries to combine all predictive distributions from individual models, which is more stable and flexible than single model selection. In this article we describe when and how to aggregate models under the lens of Bayesian decision theory. Among two widely used methods, Bayesian model averaging (BMA) and Bayesian stacking, we compare their predictive performance, and review their theoretical optimality, probabilistic interpretation, practical implementation, and extensions in complex models.},
  archiveprefix = {arXiv},
  eprint = {1912.11218},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/U3WDTIZ5/Yao - 2019 - Bayesian Aggregation.pdf;/mnt/data/Google Drive/Zotero/storage/Y2K797FW/1912.html},
  keywords = {Statistics - Methodology},
  primaryclass = {stat}
}

@article{yaoUsingStackingAverage2018,
  title = {Using Stacking to Average {{Bayesian}} Predictive Distributions},
  author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
  date = {2018-09},
  journaltitle = {Bayesian Analysis},
  shortjournal = {Bayesian Anal.},
  volume = {13},
  pages = {917--1007},
  issn = {1936-0975},
  doi = {10.1214/17-BA1091},
  url = {http://arxiv.org/abs/1704.02030},
  urldate = {2020-03-10},
  abstract = {The widely recommended procedure of Bayesian model averaging is flawed in the M-open setting in which the true data-generating process is not one of the candidate models being fit. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions, extending the utility function to any proper scoring rule, using Pareto smoothed importance sampling to efficiently compute the required leave-one-out posterior distributions and regularization to get more stability. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging (BMA), pseudo-BMA using AIC-type weighting, and a variant of pseudo-BMA that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with BB-pseudo-BMA as an approximate alternative when computation cost is an issue.},
  archiveprefix = {arXiv},
  eprint = {1704.02030},
  eprinttype = {arxiv},
  file = {/mnt/data/Google Drive/Zotero/storage/4J6RFHS6/Yao et al. - 2018 - Using stacking to average Bayesian predictive dist.pdf},
  keywords = {Statistics - Computation,Statistics - Methodology},
  langid = {english},
  number = {3}
}

@article{yaoUsingStackingAverage2018a,
  title = {Using {{Stacking}} to {{Average Bayesian Predictive Distributions}} (with {{Discussion}})},
  author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
  date = {2018-09},
  journaltitle = {Bayesian Analysis},
  shortjournal = {Bayesian Anal.},
  volume = {13},
  pages = {917--1007},
  issn = {1936-0975},
  doi = {10.1214/17-BA1091},
  url = {https://projecteuclid.org/euclid.ba/1516093227},
  urldate = {2020-05-16},
  abstract = {Bayesian model averaging is flawed in the M-open setting in which the true data-generating process is not one of the candidate models being fit. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions. We extend the utility function to any proper scoring rule and use Pareto smoothed importance sampling to efficiently compute the required leave-one-out posterior distributions. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging (BMA), Pseudo-BMA, and a variant of Pseudo-BMA that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with bootstrapped-Pseudo-BMA as an approximate alternative when computation cost is an issue.},
  file = {/mnt/data/Google Drive/Zotero/storage/CAU9DYUN/Yao et al. - 2018 - Using Stacking to Average Bayesian Predictive Dist.pdf},
  langid = {english},
  number = {3}
}

@article{yuDynamicM6AModification2018,
  title = {Dynamic {{m6A}} Modification Regulates Local Translation of {{mRNA}} in Axons},
  author = {Yu, Jun and Chen, Mengxian and Huang, Haijiao and Zhu, Junda and Song, Huixue and Zhu, Jian and Park, Jaewon and Ji, Sheng-Jian},
  date = {2018-02-16},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Res},
  volume = {46},
  pages = {1412--1423},
  issn = {0305-1048},
  doi = {10.1093/nar/gkx1182},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5815124/},
  urldate = {2018-12-11},
  abstract = {N 6-methyladenosine (m6A) is a reversible modification in mRNA and has been shown to regulate processing, translation and decay of mRNA. However, the roles of m6A modification in neuronal development are still not known. Here, we found that the m6A eraser FTO is enriched in axons and can be locally translated. Axon-specific inhibition of FTO by rhein, or compartmentalized siRNA knockdown of Fto in axons led to increases of m6A levels. GAP-43 mRNA is modified by m6A and is a substrate of FTO in axons. Loss-of-function of this non-nuclear pool of FTO resulted in increased m6A modification and decreased local translation of axonal GAP-43 mRNA, which eventually repressed axon elongation. Mutation of a predicted m6A site in GAP-43 mRNA eliminated its m6A modification and exempted regulation of its local translation by axonal FTO. This work showed an example of dynamic internal m6A demethylation of non-nuclear localized mRNA by the demethylase FTO. Regulation of m6A modification of axonal mRNA by axonal FTO might be a general mechanism to control their local translation in neuronal development.},
  eprint = {29186567},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/NCLBUAYT/Yu et al. - 2018 - Dynamic m6A modification regulates local translati.pdf},
  number = {3},
  pmcid = {PMC5815124}
}

@article{yueRNAN6methyladenosineMethylation2015,
  title = {{{RNA N6}}-Methyladenosine Methylation in Post-Transcriptional Gene Expression Regulation},
  author = {Yue, Yanan and Liu, Jianzhao and He, Chuan},
  date = {2015-07-01},
  journaltitle = {Genes \& Development},
  shortjournal = {Genes Dev},
  volume = {29},
  pages = {1343--1355},
  issn = {0890-9369},
  doi = {10.1101/gad.262766.115},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4511210/},
  urldate = {2019-01-07},
  abstract = {N6-methyladenosine (m6A) is the most prevalent internal modification that occurs in the messenger RNA (mRNA) of most eukaryotes. In this review, Yue et al. summarize recent progress in the study of the m6A mRNA methylation machineries across eukaryotes and discuss their newly uncovered roles in post-transcriptional gene expression regulation., N6-methyladenosine (m6A) is the most prevalent and internal modification that occurs in the messenger RNAs (mRNA) of most eukaryotes, although its functional relevance remained a mystery for decades. This modification is installed by the m6A methylation “writers” and can be reversed by demethylases that serve as “erasers.” In this review, we mainly summarize recent progress in the study of the m6A mRNA methylation machineries across eukaryotes and discuss their newly uncovered biological functions. The broad roles of m6A in regulating cell fates and embryonic development highlight the existence of another layer of epigenetic regulation at the RNA level, where mRNA is subjected to chemical modifications that affect protein expression.},
  eprint = {26159994},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/BGZXIBZL/Yue et al. - 2015 - RNA N6-methyladenosine methylation in post-transcr.pdf},
  number = {13},
  pmcid = {PMC4511210}
}

@article{yuShrinkageEstimationDispersion2013,
  title = {Shrinkage Estimation of Dispersion in {{Negative Binomial}} Models for {{RNA}}-Seq Experiments with Small Sample Size},
  author = {Yu, Danni and Huber, Wolfgang and Vitek, Olga},
  date = {2013-05-15},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {29},
  pages = {1275--1282},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btt143},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3654711/},
  urldate = {2019-03-27},
  abstract = {Motivation: RNA-seq experiments produce digital counts of reads that are affected by both biological and technical variation. To distinguish the systematic changes in expression between conditions from noise, the counts are frequently modeled by the Negative Binomial distribution. However, in experiments with small sample size, the per-gene estimates of the dispersion parameter are unreliable., Method: We propose a simple and effective approach for estimating the dispersions. First, we obtain the initial estimates for each gene using the method of moments. Second, the estimates are regularized, i.e. shrunk towards a common value that minimizes the average squared difference between the initial estimates and the shrinkage estimates. The approach does not require extra modeling assumptions, is easy to compute and is compatible with the exact test of differential expression., Results: We evaluated the proposed approach using 10 simulated and experimental datasets and compared its performance with that of currently popular packages edgeR, DESeq, baySeq, BBSeq and SAMseq. For these datasets, sSeq performed favorably for experiments with small sample size in sensitivity, specificity and computational time., Availability: http://www.stat.purdue.edu/∼ovitek/Software.html and Bioconductor., Contact: ovitek@purdue.edu, Supplementary information: Supplementary data are available at Bioinformatics online.},
  eprint = {23589650},
  eprinttype = {pmid},
  file = {/mnt/data/Google Drive/Zotero/storage/FB7Z6CIH/Yu et al. - 2013 - Shrinkage estimation of dispersion in Negative Bin.pdf},
  number = {10},
  pmcid = {PMC3654711}
}

@article{zamoEstimationContinuousRanked2018,
  title = {Estimation of the {{Continuous Ranked Probability Score}} with {{Limited Information}} and {{Applications}} to {{Ensemble Weather Forecasts}}},
  author = {Zamo, Michaël and Naveau, Philippe},
  date = {2018-02-01},
  journaltitle = {Mathematical Geosciences},
  shortjournal = {Math Geosci},
  volume = {50},
  pages = {209--234},
  issn = {1874-8953},
  doi = {10.1007/s11004-017-9709-7},
  url = {https://doi.org/10.1007/s11004-017-9709-7},
  urldate = {2020-04-13},
  abstract = {The continuous ranked probability score (CRPS) is a much used measure of performance for probabilistic forecasts of a scalar observation. It is a quadratic measure of the difference between the forecast cumulative distribution function (CDF) and the empirical CDF of the observation. Analytic formulations of the CRPS can be derived for most classical parametric distributions, and be used to assess the efficiency of different CRPS estimators. When the true forecast CDF is not fully known, but represented as an ensemble of values, the CRPS is estimated with some error. Thus, using the CRPS to compare parametric probabilistic forecasts with ensemble forecasts may be misleading due to the unknown error of the estimated CRPS for the ensemble. With simulated data, the impact of the type of the verified ensemble (a random sample or a set of quantiles) on the CRPS estimation is studied. Based on these simulations, recommendations are issued to choose the most accurate CRPS estimator according to the type of ensemble. The interest of these recommendations is illustrated with real ensemble weather forecasts. Also, relationships between several estimators of the CRPS are demonstrated and used to explain the differences of accuracy between the estimators.},
  file = {/mnt/data/Google Drive/Zotero/storage/IRN58QLH/Zamo and Naveau - 2018 - Estimation of the Continuous Ranked Probability Sc.pdf},
  langid = {english},
  number = {2}
}

@article{zamoEstimationContinuousRanked2018a,
  title = {Estimation of the {{Continuous Ranked Probability Score}} with {{Limited Information}} and {{Applications}} to {{Ensemble Weather Forecasts}}},
  author = {Zamo, Michaël and Naveau, Philippe},
  date = {2018-02-01},
  journaltitle = {Mathematical Geosciences},
  shortjournal = {Math Geosci},
  volume = {50},
  pages = {209--234},
  issn = {1874-8953},
  doi = {10.1007/s11004-017-9709-7},
  url = {https://doi.org/10.1007/s11004-017-9709-7},
  urldate = {2020-05-12},
  abstract = {The continuous ranked probability score (CRPS) is a much used measure of performance for probabilistic forecasts of a scalar observation. It is a quadratic measure of the difference between the forecast cumulative distribution function (CDF) and the empirical CDF of the observation. Analytic formulations of the CRPS can be derived for most classical parametric distributions, and be used to assess the efficiency of different CRPS estimators. When the true forecast CDF is not fully known, but represented as an ensemble of values, the CRPS is estimated with some error. Thus, using the CRPS to compare parametric probabilistic forecasts with ensemble forecasts may be misleading due to the unknown error of the estimated CRPS for the ensemble. With simulated data, the impact of the type of the verified ensemble (a random sample or a set of quantiles) on the CRPS estimation is studied. Based on these simulations, recommendations are issued to choose the most accurate CRPS estimator according to the type of ensemble. The interest of these recommendations is illustrated with real ensemble weather forecasts. Also, relationships between several estimators of the CRPS are demonstrated and used to explain the differences of accuracy between the estimators.},
  file = {/mnt/data/Google Drive/Zotero/storage/JKHD2EQY/Zamo and Naveau - 2018 - Estimation of the Continuous Ranked Probability Sc.pdf},
  langid = {english},
  number = {2}
}

@article{zhangBayesianHierarchicalModel2018,
  title = {A {{Bayesian}} Hierarchical Model for Analyzing Methylated {{RNA}} Immunoprecipitation Sequencing Data},
  author = {Zhang, Minzhe and Li, Qiwei and Xie, Yang},
  date = {2018-09-01},
  journaltitle = {Quantitative Biology},
  shortjournal = {Quant Biol},
  volume = {6},
  pages = {275--286},
  issn = {2095-4697},
  doi = {10.1007/s40484-018-0149-2},
  url = {https://doi.org/10.1007/s40484-018-0149-2},
  urldate = {2019-03-05},
  abstract = {BackgroundThe recently emerged technology of methylated RNA immunoprecipitation sequencing (MeRIP-seq) sheds light on the study of RNA epigenetics. This new bioinformatics question calls for effective and robust peaking calling algorithms to detect mRNA methylation sites from MeRIP-seq data.MethodsWe propose a Bayesian hierarchical model to detect methylation sites from MeRIP-seq data. Our modeling approach includes several important characteristics. First, it models the zero-inflated and over-dispersed counts by deploying a zero-inflated negative binomial model. Second, it incorporates a hidden Markov model (HMM) to account for the spatial dependency of neighboring read enrichment. Third, our Bayesian inference allows the proposed model to borrow strength in parameter estimation, which greatly improves the model stability when dealing with MeRIP-seq data with a small number of replicates. We use Markov chain Monte Carlo (MCMC) algorithms to simultaneously infer the model parameters in a de novo fashion. The R Shiny demo is available at https://qiwei.shinyapps.io/BaySeqPeak and the R/C ++ code is available at https://github.com/liqiwei2000/BaySeqPeak.ResultsIn simulation studies, the proposed method outperformed the competing methods exomePeak and MeTPeak, especially when an excess of zeros were present in the data. In real MeRIP-seq data analysis, the proposed method identified methylation sites that were more consistent with biological knowledge, and had better spatial resolution compared to the other methods.ConclusionsIn this study, we develop a Bayesian hierarchical model to identify methylation peaks in MeRIP-seq data. The proposed method has a competitive edge over existing methods in terms of accuracy, robustness and spatial resolution.Open image in new window},
  file = {/mnt/data/Google Drive/Zotero/storage/9BGA345F/Zhang et al. - 2018 - A Bayesian hierarchical model for analyzing methyl.pdf},
  keywords = {Bayesian inference,hidden Markov model,MeRIP-seq data,RNA epigenomics,zero-inflated negative binomial},
  langid = {english},
  number = {3}
}

@article{zhangVariableSelectionProcedures2019,
  title = {Variable Selection Procedures from Multiple Testing},
  author = {Zhang, Baoxue and Cheng, Guanghui and Zhang, Chunming and Zheng, Shurong},
  date = {2019-04},
  journaltitle = {Science China Mathematics},
  shortjournal = {Sci. China Math.},
  volume = {62},
  pages = {771--782},
  issn = {1674-7283, 1869-1862},
  doi = {10.1007/s11425-016-9186-x},
  url = {http://link.springer.com/10.1007/s11425-016-9186-x},
  urldate = {2020-01-13},
  abstract = {Variable selection has played an important role in statistical learning and scientific discoveries during the past ten years, and multiple testing is a fundamental problem in statistical inference and also has wide applications in many scientific fields. Significant advances have been achieved in both areas. This study attempts to find a connection between the adaptive LASSO (least absolute shrinkage and selection operator) and multiple testing procedures in linear regression models. We also propose procedures based on multiple testing methods to select variables and control the selection error rate, i.e., the false discovery rate. Simulation studies demonstrate that the proposed methods show good performance relative to controlling the selection error rate under a wide range of settings.},
  file = {/mnt/data/Google Drive/Zotero/storage/5VXXSYUD/Zhang et al. - 2019 - Variable selection procedures from multiple testin.pdf},
  langid = {english},
  number = {4}
}

@article{zhuHeavytailedPriorDistributions,
  title = {Heavy-Tailed Prior Distributions for Sequence Count Data: Removing the Noise and Preserving Large Differences},
  shorttitle = {Heavy-Tailed Prior Distributions for Sequence Count Data},
  author = {Zhu, Anqi and Ibrahim, Joseph G. and Love, Michael I.},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  doi = {10.1093/bioinformatics/bty895},
  url = {https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty895/5159452},
  urldate = {2019-01-04},
  abstract = {AbstractMotivation.  In RNA-seq differential expression analysis, investigators aim to detect those genes with changes in expression level across conditions, de},
  file = {/mnt/data/Google Drive/Zotero/storage/4GI2Z6NK/Zhu et al. - Heavy-tailed prior distributions for sequence coun.pdf;/mnt/data/Google Drive/Zotero/storage/53QRL4SC/Zhu et al. - Heavy-tailed prior distributions for sequence coun.pdf;/mnt/data/Google Drive/Zotero/storage/46HPAP6J/5159452.html;/mnt/data/Google Drive/Zotero/storage/QXATEWWE/5159452.html},
  langid = {english}
}

@book{zotero-447,
  type = {book}
}


